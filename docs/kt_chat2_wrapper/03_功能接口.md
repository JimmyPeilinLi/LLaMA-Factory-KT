# KTMoEWrapper SFT 集成功能接口

## 1. KTMoEWrapper 工厂接口

### 1.1 构造函数

```python
class KTMoEWrapper:
    def __new__(
        cls,
        # ========== 必需参数 ==========
        layer_idx: int,                              # 层索引
        num_experts: int,                            # 专家总数
        num_experts_per_tok: int,                    # 每 token 激活的专家数 (top-k)
        hidden_size: int,                            # 隐藏层维度
        moe_intermediate_size: int,                  # MoE 中间层维度
        num_gpu_experts: int,                        # GPU 上的专家数（当前为 0）
        cpuinfer_threads: int,                       # CPU 推理线程数
        threadpool_count: int,                       # NUMA 子池数量（TP 控制）
        weight_path: str,                            # 权重路径
        chunked_prefill_size: int,                   # 分块预填充大小

        # ========== 推理特有参数 ==========
        cpu_save: bool = False,                      # 是否保存到 CPU 内存
        max_deferred_experts_per_token: Optional[int] = None,  # 延迟专家数

        # ========== 模式和方法选择 ==========
        method: str = "AMXINT4",                     # 后端方法
        mode: str = "inference",                     # 操作模式

        # ========== SFT 特有参数 ==========
        lora_rank: int = 16,                         # LoRA 秩
        lora_alpha: float = 32.0,                    # LoRA 缩放因子
        max_cache_depth: int = 1,                    # 最大前向缓存深度

        # ========== 量化配置（K-Group 方法）==========
        group_size: int = 128,                       # 量化组大小
        zero_point: bool = True,                     # 是否使用零点量化
    ) -> Union[BaseMoEWrapper, BaseSFTMoEWrapper]:
        ...
```

### 1.2 参数详解

#### 必需参数

| 参数 | 类型 | 说明 | 示例值 |
|------|------|------|--------|
| `layer_idx` | int | 层索引，从 0 开始 | 0, 1, 2, ... |
| `num_experts` | int | 专家总数 | 256 (DeepSeek-V3) |
| `num_experts_per_tok` | int | 每 token 激活的专家数 | 8 |
| `hidden_size` | int | 隐藏层维度 | 7168 |
| `moe_intermediate_size` | int | MoE 中间层维度 | 2048 |
| `num_gpu_experts` | int | GPU 上的专家数 | 0（当前固定） |
| `cpuinfer_threads` | int | CPU 推理线程数 | 60 |
| `threadpool_count` | int | NUMA 子池数量 | 1 (no-TP) 或 4 (TP) |
| `weight_path` | str | 权重路径 | "/path/to/weights" |
| `chunked_prefill_size` | int | 分块预填充大小 | 25600 |

#### TP 相关参数

| 参数 | 类型 | 说明 | TP 模式 | no-TP 模式 |
|------|------|------|---------|-----------|
| `cpuinfer_threads` | int | 总 CPU 线程数 | 按 NUMA 节点分配 | 全部在单节点 |
| `threadpool_count` | int | NUMA 子池数量 | > 1 | = 1 |

**线程分配示例**：
```python
# cpuinfer_threads=60, threadpool_count=4
# 分配结果: [15, 15, 15, 15]

# cpuinfer_threads=61, threadpool_count=4
# 分配结果: [16, 16, 15, 14]
```

#### SFT 特有参数

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `lora_rank` | int | 16 | LoRA 低秩矩阵的秩 (r) |
| `lora_alpha` | float | 32.0 | LoRA 缩放因子 (α) |
| `max_cache_depth` | int | 1 | 最大前向缓存深度，用于梯度检查点 |

**LoRA 缩放公式**：
```
lora_output = (input @ A.T @ B.T) * (lora_alpha / lora_rank)
output = base_output + lora_output
```

#### 预留参数

| 参数 | 类型 | 当前值 | 未来用途 |
|------|------|--------|---------|
| `num_gpu_experts` | int | 0 | 控制在 GPU 上运行的专家数量 |

#### 量化配置参数（K-Group 方法）

| 参数 | 类型 | 默认值 | 适用方法 |
|------|------|--------|---------|
| `group_size` | int | 128 | `AMXINT4_KGroup_SFT`, `AMXINT4_1KGroup_SFT` |
| `zero_point` | bool | True | `AMXINT4_KGroup_SFT`, `AMXINT4_1KGroup_SFT` |

### 1.3 返回类型

| mode | 返回类型 |
|------|---------|
| `"inference"` | `BaseMoEWrapper` 子类实例 |
| `"sft"` | `BaseSFTMoEWrapper` 子类实例 |

### 1.4 支持的方法

#### SFT 模式 (`mode="sft"`)

| method | 说明 | 精度 |
|--------|------|------|
| `AMXBF16_SFT` | AMX BF16 精度训练 | 最高（推荐） |
| `AMXINT8_SFT` | AMX INT8 量化训练 | 中等 |
| `AMXINT4_SFT` | AMX INT4 量化训练 | 较低 |
| `AMXINT4_1_SFT` | AMX INT4_1 量化训练 | 较低 |
| `AMXINT4_KGroup_SFT` | AMX INT4 K-Group 量化 | 较低 |
| `AMXINT4_1KGroup_SFT` | AMX INT4_1 K-Group 量化 | 较低 |

### 1.5 使用示例

```python
from kt_kernel.experts import KTMoEWrapper

# 创建 SFT Wrapper（no-TP 模式）
wrapper = KTMoEWrapper(
    layer_idx=0,
    num_experts=256,
    num_experts_per_tok=8,
    hidden_size=7168,
    moe_intermediate_size=2048,
    num_gpu_experts=0,
    cpuinfer_threads=60,
    threadpool_count=1,  # no-TP
    weight_path="/path/to/weights",
    chunked_prefill_size=25600,
    method="AMXBF16_SFT",
    mode="sft",
    lora_rank=16,
    lora_alpha=32.0,
)

# 创建 SFT Wrapper（TP 模式）
wrapper = KTMoEWrapper(
    layer_idx=0,
    num_experts=256,
    num_experts_per_tok=8,
    hidden_size=7168,
    moe_intermediate_size=2048,
    num_gpu_experts=0,
    cpuinfer_threads=60,
    threadpool_count=4,  # TP=4
    weight_path="/path/to/weights",
    chunked_prefill_size=25600,
    method="AMXBF16_SFT",
    mode="sft",
    lora_rank=16,
    lora_alpha=32.0,
)
```

---

## 2. BaseSFTMoEWrapper 接口

### 2.1 load_weights

**功能**：从预量化权重文件加载基础权重

```python
def load_weights(self, physical_to_logical_map_cpu: torch.Tensor) -> None:
    """
    从预量化权重文件加载基础权重。

    Args:
        physical_to_logical_map_cpu: 物理专家 ID 到逻辑专家 ID 的映射
            形状: [num_experts]
            类型: torch.int64
            设备: CPU

    Raises:
        RuntimeError: 如果权重路径无效或权重加载失败
    """
```

**使用示例**：
```python
physical_map = torch.arange(256, dtype=torch.int64, device="cpu")
wrapper.load_weights(physical_map)
```

### 2.2 load_weights_from_tensors

**功能**：从 PyTorch 张量加载基础权重（在线量化）

```python
def load_weights_from_tensors(
    self,
    gate_proj: torch.Tensor,
    up_proj: torch.Tensor,
    down_proj: torch.Tensor,
    physical_to_logical_map_cpu: torch.Tensor,
) -> None:
    """
    从 PyTorch 张量加载基础权重。

    Args:
        gate_proj: Gate 投影权重
            形状: [num_experts, intermediate_size, hidden_size]
            类型: torch.bfloat16
            设备: CPU
        up_proj: Up 投影权重
            形状: [num_experts, intermediate_size, hidden_size]
        down_proj: Down 投影权重
            形状: [num_experts, hidden_size, intermediate_size]
        physical_to_logical_map_cpu: 专家 ID 映射

    Note:
        调用此方法后，内部会自动执行权重量化（如果 method 不是 AMXBF16_SFT）
    """
```

**使用示例**：
```python
# 从模型提取权重
gate_proj = torch.stack([expert.gate_proj.weight for expert in experts], dim=0)
up_proj = torch.stack([expert.up_proj.weight for expert in experts], dim=0)
down_proj = torch.stack([expert.down_proj.weight for expert in experts], dim=0)

# 转换为 CPU BF16
gate_proj = gate_proj.cpu().to(torch.bfloat16).contiguous()
up_proj = up_proj.cpu().to(torch.bfloat16).contiguous()
down_proj = down_proj.cpu().to(torch.bfloat16).contiguous()

# 加载权重
physical_map = torch.arange(256, dtype=torch.int64, device="cpu")
wrapper.load_weights_from_tensors(gate_proj, up_proj, down_proj, physical_map)
```

### 2.3 init_lora_weights

**功能**：初始化 6 个 LoRA 权重矩阵

```python
def init_lora_weights(
    self,
    gate_lora_a: torch.Tensor,
    gate_lora_b: torch.Tensor,
    up_lora_a: torch.Tensor,
    up_lora_b: torch.Tensor,
    down_lora_a: torch.Tensor,
    down_lora_b: torch.Tensor,
) -> None:
    """
    初始化 LoRA 权重。

    Args:
        gate_lora_a: Gate LoRA A 矩阵
            形状: [num_experts, lora_rank, hidden_size]
            类型: torch.bfloat16
            设备: CPU
        gate_lora_b: Gate LoRA B 矩阵
            形状: [num_experts, intermediate_size, lora_rank]
        up_lora_a: Up LoRA A 矩阵
            形状: [num_experts, lora_rank, hidden_size]
        up_lora_b: Up LoRA B 矩阵
            形状: [num_experts, intermediate_size, lora_rank]
        down_lora_a: Down LoRA A 矩阵
            形状: [num_experts, lora_rank, intermediate_size]
        down_lora_b: Down LoRA B 矩阵
            形状: [num_experts, hidden_size, lora_rank]

    Note:
        - 所有张量必须在 CPU 上且为 BF16 类型
        - A 矩阵通常使用 kaiming_uniform 初始化
        - B 矩阵通常初始化为零
    """
```

**LoRA 权重形状总结**：

| 矩阵 | 形状 | 说明 |
|------|------|------|
| `gate_lora_a` | [E, r, H] | E=专家数, r=rank, H=hidden |
| `gate_lora_b` | [E, I, r] | I=intermediate |
| `up_lora_a` | [E, r, H] | |
| `up_lora_b` | [E, I, r] | |
| `down_lora_a` | [E, r, I] | |
| `down_lora_b` | [E, H, r] | |

**使用示例**：
```python
import math

num_experts = 256
hidden_size = 7168
intermediate_size = 2048
lora_rank = 16

# 初始化 LoRA 权重
gate_lora_a = torch.zeros(num_experts, lora_rank, hidden_size, dtype=torch.bfloat16, device="cpu")
gate_lora_b = torch.zeros(num_experts, intermediate_size, lora_rank, dtype=torch.bfloat16, device="cpu")
up_lora_a = torch.zeros(num_experts, lora_rank, hidden_size, dtype=torch.bfloat16, device="cpu")
up_lora_b = torch.zeros(num_experts, intermediate_size, lora_rank, dtype=torch.bfloat16, device="cpu")
down_lora_a = torch.zeros(num_experts, lora_rank, intermediate_size, dtype=torch.bfloat16, device="cpu")
down_lora_b = torch.zeros(num_experts, hidden_size, lora_rank, dtype=torch.bfloat16, device="cpu")

# A 矩阵使用 kaiming_uniform 初始化
for tensor in [gate_lora_a, up_lora_a, down_lora_a]:
    torch.nn.init.kaiming_uniform_(tensor, a=math.sqrt(5))

# 初始化到 Wrapper
wrapper.init_lora_weights(
    gate_lora_a, gate_lora_b,
    up_lora_a, up_lora_b,
    down_lora_a, down_lora_b
)
```

### 2.4 forward_sft

**功能**：SFT 前向传播

```python
def forward_sft(
    self,
    hidden_states: torch.Tensor,
    expert_ids: torch.Tensor,
    weights: torch.Tensor,
    save_for_backward: bool = True,
) -> torch.Tensor:
    """
    SFT 前向传播。

    Args:
        hidden_states: 输入隐藏状态
            形状: [qlen, hidden_size]
            类型: torch.bfloat16 或 torch.float32
            设备: CPU 或 CUDA
        expert_ids: 每个 token 选择的专家 ID
            形状: [qlen, num_experts_per_tok]
            类型: torch.int64
        weights: 专家权重（归一化后）
            形状: [qlen, num_experts_per_tok]
            类型: torch.float32
        save_for_backward: 是否保存激活值用于反向传播
            True: 训练模式，保存激活值
            False: 仅前向模式，不保存

    Returns:
        output: 输出隐藏状态
            形状: [qlen, hidden_size]
            类型: 与输入相同
            设备: 与输入相同

    Raises:
        RuntimeError: 如果权重未加载或 LoRA 未初始化
        RuntimeError: 如果 save_for_backward=True 但缓存已满
    """
```

**使用示例**：
```python
# 输入数据
hidden_states = torch.randn(128, 7168, dtype=torch.bfloat16, device="cuda")
expert_ids = torch.randint(0, 256, (128, 8), dtype=torch.int64)
weights = torch.softmax(torch.randn(128, 8), dim=-1).float()

# 前向传播（训练模式）
output = wrapper.forward_sft(hidden_states, expert_ids, weights, save_for_backward=True)

# 前向传播（推理模式）
output = wrapper.forward_sft(hidden_states, expert_ids, weights, save_for_backward=False)
```

### 2.5 backward

**功能**：反向传播，计算输入梯度和 LoRA 梯度

```python
def backward(
    self,
    grad_output: torch.Tensor,
) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]:
    """
    反向传播，计算梯度。

    Args:
        grad_output: 上游梯度
            形状: [qlen, hidden_size]
            类型: torch.bfloat16 或 torch.float32
            设备: CPU 或 CUDA

    Returns:
        grad_input: 输入梯度
            形状: [qlen, hidden_size]
            类型: 与 grad_output 相同
        grad_loras: LoRA 梯度字典
            {
                "grad_gate_lora_a": [num_experts, lora_rank, hidden_size],
                "grad_gate_lora_b": [num_experts, intermediate_size, lora_rank],
                "grad_up_lora_a": [num_experts, lora_rank, hidden_size],
                "grad_up_lora_b": [num_experts, intermediate_size, lora_rank],
                "grad_down_lora_a": [num_experts, lora_rank, intermediate_size],
                "grad_down_lora_b": [num_experts, hidden_size, lora_rank],
            }

    Raises:
        RuntimeError: 如果没有前向缓存（未调用 forward_sft 或 save_for_backward=False）

    Note:
        必须在 forward_sft(save_for_backward=True) 之后调用
    """
```

**使用示例**：
```python
# 前向传播
output = wrapper.forward_sft(hidden_states, expert_ids, weights, save_for_backward=True)

# 计算损失的梯度
grad_output = torch.randn_like(output)

# 反向传播
grad_input, grad_loras = wrapper.backward(grad_output)

# grad_loras 包含 6 个 LoRA 梯度
print(grad_loras.keys())
# dict_keys(['grad_gate_lora_a', 'grad_gate_lora_b', 'grad_up_lora_a',
#            'grad_up_lora_b', 'grad_down_lora_a', 'grad_down_lora_b'])
```

### 2.6 update_lora_weights

**功能**：同步 LoRA 权重到 C++ 后端

```python
def update_lora_weights(self) -> None:
    """
    同步 LoRA 权重到 C++ 后端。

    在使用外部优化器更新 LoRA 权重后调用此方法，
    将更新后的权重同步到 C++ 内核。

    Note:
        - 这是零拷贝操作，只传递 Python 张量指针
        - 必须在优化器 step() 之后调用
        - 调用前确保 Python 端的 LoRA 张量已更新

    典型使用流程：
        1. forward_sft() - 前向传播
        2. backward() - 反向传播
        3. optimizer.step() - 优化器更新 LoRA 权重
        4. update_lora_weights() - 同步到 C++
        5. optimizer.zero_grad() - 清零梯度
    """
```

**使用示例**：
```python
# 完整训练循环
optimizer = torch.optim.AdamW([
    wrapper.gate_lora_a, wrapper.gate_lora_b,
    wrapper.up_lora_a, wrapper.up_lora_b,
    wrapper.down_lora_a, wrapper.down_lora_b,
], lr=1e-4)

for batch in dataloader:
    # 1. 前向传播
    output = wrapper.forward_sft(hidden_states, expert_ids, weights, save_for_backward=True)

    # 2. 计算损失
    loss = criterion(output, target)

    # 3. 反向传播
    grad_output = compute_grad(loss, output)
    grad_input, grad_loras = wrapper.backward(grad_output)

    # 4. 更新 LoRA 权重的梯度
    wrapper.gate_lora_a.grad = grad_loras["grad_gate_lora_a"]
    wrapper.gate_lora_b.grad = grad_loras["grad_gate_lora_b"]
    # ... 其他 LoRA 梯度 ...

    # 5. 优化器步骤
    optimizer.step()

    # 6. 同步到 C++ 后端
    wrapper.update_lora_weights()

    # 7. 清零梯度
    optimizer.zero_grad()
```

---

## 3. TP/no-TP 配置接口

### 3.1 no-TP 模式

```python
# no-TP 模式：单 NUMA 节点
wrapper = KTMoEWrapper(
    ...,
    cpuinfer_threads=60,
    threadpool_count=1,  # 关键：设为 1
    ...
)
```

**特点**：
- 所有 60 个线程在单个 NUMA 节点运行
- 无权重分割
- 适合单 NUMA 系统或小规模测试

### 3.2 TP 模式

```python
# TP 模式：多 NUMA 节点并行
wrapper = KTMoEWrapper(
    ...,
    cpuinfer_threads=60,
    threadpool_count=4,  # 关键：设为 > 1
    ...
)
```

**特点**：
- 60 个线程分配到 4 个 NUMA 节点（每节点 15 线程）
- 权重按 intermediate_size 维度分割
- 计算自动并行化

### 3.3 约束条件

```python
# 约束：intermediate_size 必须能被 threadpool_count 整除
assert moe_intermediate_size % threadpool_count == 0, (
    f"intermediate_size ({moe_intermediate_size}) must be "
    f"divisible by threadpool_count ({threadpool_count})"
)
```

### 3.4 与 LLaMA-Factory-KT 参数映射

| LLaMA-Factory 参数 | KTMoEWrapper 参数 | 说明 |
|-------------------|------------------|------|
| `kt_tp_enabled` | `threadpool_count > 1` | TP 开关 |
| `kt_tp_count` | `threadpool_count` | TP 并行度 |
| `kt_num_threads` | `cpuinfer_threads` | 总线程数 |

---

## 4. 扩展适配接口

### 4.1 添加新 SFT 方法

**步骤 1**：在 `experts.py` 中注册方法名

```python
# kt_kernel/experts.py
SFT_METHODS = frozenset([
    "AMXBF16_SFT",
    "AMXINT8_SFT",
    "AMXINT4_SFT",
    "AMXINT4_1_SFT",
    "AMXINT4_KGroup_SFT",
    "AMXINT4_1KGroup_SFT",
    "NEW_METHOD_SFT",  # 新增方法
])
```

**步骤 2**：在 `amx_sft.py` 中注册 C++ 类映射

```python
# kt_kernel/utils/amx_sft.py
_SFT_METHOD_TO_CLASS = {
    "AMXBF16_SFT": AMXBF16_SFT_MOE,
    "AMXINT8_SFT": AMXInt8_SFT_MOE,
    "AMXINT4_SFT": AMXInt4_SFT_MOE,
    "AMXINT4_1_SFT": AMXInt4_1_SFT_MOE,
    "AMXINT4_KGroup_SFT": AMXInt4_KGroup_SFT_MOE,
    "AMXINT4_1KGroup_SFT": AMXInt4_1KGroup_SFT_MOE,
    "NEW_METHOD_SFT": NewMethod_SFT_MOE,  # 新增映射
}
```

**步骤 3**：无需修改 KTMoEWrapper 或 BaseSFTMoEWrapper

### 4.2 条件字段处理

```python
# 在 AMXSFTMoEWrapper 中根据 method 添加条件配置
if self.method in ("AMXINT4_KGroup_SFT", "AMXINT4_1KGroup_SFT"):
    config.quant_config.group_size = self.group_size
    config.quant_config.zero_point = self.zero_point
```

### 4.3 未来 GPU experts 兼容预案

```python
# 当前实现（num_gpu_experts=0）
wrapper = KTMoEWrapper(
    ...,
    num_gpu_experts=0,  # 全部专家在 CPU
    ...
)

# 未来扩展（num_gpu_experts>0）
wrapper = KTMoEWrapper(
    ...,
    num_gpu_experts=64,  # 64 个专家在 GPU
    ...
)
# 预计需要的额外配置：
# - gpu_expert_ids: 指定哪些专家在 GPU
# - gpu_weight_path: GPU 专家权重路径
```

---

## 5. 与 LLaMA-Factory-KT 的集成接口

### 5.1 wrap_moe_layers_with_kt_wrapper

**功能**：替换模型中的 MoE 层为 KTMoEWrapper

```python
def wrap_moe_layers_with_kt_wrapper(
    model: PreTrainedModel,
    model_args: ModelArguments,
    finetuning_args: FinetuningArguments,
) -> List[KTMoELayerWrapper]:
    """
    使用 KTMoEWrapper 包装模型中的 MoE 层。

    Args:
        model: HuggingFace 预训练模型
        model_args: 模型参数（包含 KT 配置）
        finetuning_args: 微调参数（包含 LoRA 配置）

    Returns:
        wrappers: KTMoELayerWrapper 实例列表

    Note:
        此函数替换 wrap_moe_layers_with_amx()
    """
```

### 5.2 KTMoELayerWrapper

**功能**：nn.Module 封装，适配 PyTorch 训练

```python
class KTMoELayerWrapper(nn.Module):
    """
    包装 KTMoEWrapper 为 PyTorch nn.Module。

    属性：
        kt_wrapper: BaseSFTMoEWrapper 实例
        router: Router 模块
        shared_experts: 共享专家模块（可选）
        lora_params: LoRA 参数字典（nn.ParameterDict）
        _is_kt_moe_wrapper: PEFT 识别标记

    方法：
        forward(hidden_states): 前向传播
        update_lora_pointers(): 同步 LoRA 权重到 C++
    """

    def __init__(
        self,
        kt_wrapper: BaseSFTMoEWrapper,
        router: nn.Module,
        shared_experts: Optional[nn.Module],
        moe_config: MOEArchConfig,
        lora_params: Dict[str, nn.Parameter],
        hidden_size: int,
        layer_idx: int,
    ):
        ...

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        ...

    def update_lora_pointers(self) -> None:
        ...
```

### 5.3 KTMoEFunction

**功能**：PyTorch autograd Function，桥接 KTMoEWrapper 和自动微分

```python
class KTMoEFunction(torch.autograd.Function):
    """
    PyTorch autograd Function 实现。

    静态方法：
        forward(ctx, hidden_states, topk_ids, topk_weights, kt_wrapper, training)
        backward(ctx, grad_output)
    """

    @staticmethod
    def forward(ctx, hidden_states, topk_ids, topk_weights, kt_wrapper, training):
        ...

    @staticmethod
    def backward(ctx, grad_output):
        ...
```

### 5.4 LoRA 参数管理

```python
# LoRA 参数作为 nn.Parameter 管理
lora_params = nn.ParameterDict({
    "gate_lora_a": nn.Parameter(gate_lora_a, requires_grad=True),
    "gate_lora_b": nn.Parameter(gate_lora_b, requires_grad=True),
    "up_lora_a": nn.Parameter(up_lora_a, requires_grad=True),
    "up_lora_b": nn.Parameter(up_lora_b, requires_grad=True),
    "down_lora_a": nn.Parameter(down_lora_a, requires_grad=True),
    "down_lora_b": nn.Parameter(down_lora_b, requires_grad=True),
})

# 优化器可以直接收集这些参数
optimizer = torch.optim.AdamW(lora_params.parameters(), lr=1e-4)
```

### 5.5 配置参数映射

| ModelArguments | KTMoEWrapper 参数 |
|----------------|------------------|
| `kt_backend` | `method` |
| `kt_num_threads` | `cpuinfer_threads` |
| `kt_tp_enabled` | `threadpool_count > 1` |
| `kt_tp_count` | `threadpool_count` |
| `kt_weight_path` | `weight_path` |

| FinetuningArguments | KTMoEWrapper 参数 |
|--------------------|------------------|
| `lora_rank` | `lora_rank` |
| `lora_alpha` | `lora_alpha` |

---

## 6. 错误处理

### 6.1 常见错误

| 错误 | 原因 | 解决方案 |
|------|------|---------|
| `ValueError: Method 'X' not supported for SFT mode` | method 与 mode 不匹配 | 使用 `SFT_METHODS` 中的方法 |
| `RuntimeError: Weights not loaded` | 未调用 load_weights | 先调用 load_weights 或 load_weights_from_tensors |
| `RuntimeError: LoRA not initialized` | 未调用 init_lora_weights | 先调用 init_lora_weights |
| `RuntimeError: Forward cache full` | max_cache_depth 不足 | 增加 max_cache_depth 或及时调用 backward |
| `RuntimeError: No forward cache` | backward 前未调用 forward_sft | 确保 forward_sft 时 save_for_backward=True |
| `RuntimeError: intermediate_size not divisible` | TP 约束不满足 | 调整 threadpool_count |

### 6.2 状态检查

```python
# 检查权重是否加载
assert wrapper._weights_loaded, "Weights not loaded"

# 检查 LoRA 是否初始化
assert wrapper._lora_initialized, "LoRA not initialized"

# 检查缓存深度
assert wrapper._cache_depth > 0, "No forward cache for backward"
```
