# KTMoEWrapper SFT 集成功能使用测试

## 1. 单元测试

### 1.1 前向精度测试

**目标**：验证 KTMoEWrapper 前向传播与 PyTorch 参考实现的一致性

**测试配置**：
```python
# 模型配置（基于 DeepSeek-V3）
expert_num = 256
hidden_size = 7168
intermediate_size = 2048
num_experts_per_tok = 8
qlen = 4
lora_rank = 16
lora_alpha = 32.0

# 精度阈值
BF16_FORWARD_THRESHOLD = 0.05  # 5% 相对误差
```

**测试代码**：
```python
import torch
from kt_kernel.experts import KTMoEWrapper

def test_forward_accuracy():
    """前向精度测试"""
    # 1. 创建 Wrapper
    wrapper = KTMoEWrapper(
        layer_idx=0,
        num_experts=expert_num,
        num_experts_per_tok=num_experts_per_tok,
        hidden_size=hidden_size,
        moe_intermediate_size=intermediate_size,
        num_gpu_experts=0,
        cpuinfer_threads=60,
        threadpool_count=1,  # no-TP
        weight_path="",
        chunked_prefill_size=25600,
        method="AMXBF16_SFT",
        mode="sft",
        lora_rank=lora_rank,
        lora_alpha=lora_alpha,
    )

    # 2. 初始化权重
    gate_proj, up_proj, down_proj = init_base_weights()
    gate_lora_a, gate_lora_b, up_lora_a, up_lora_b, down_lora_a, down_lora_b = init_lora_weights()

    physical_map = torch.arange(expert_num, dtype=torch.int64, device="cpu")
    wrapper.load_weights_from_tensors(gate_proj, up_proj, down_proj, physical_map)
    wrapper.init_lora_weights(gate_lora_a, gate_lora_b, up_lora_a, up_lora_b, down_lora_a, down_lora_b)

    # 3. 生成随机输入
    hidden_states = torch.randn(qlen, hidden_size, dtype=torch.bfloat16, device="cpu")
    expert_ids = torch.randint(0, expert_num, (qlen, num_experts_per_tok), dtype=torch.int64)
    weights = torch.softmax(torch.randn(qlen, num_experts_per_tok), dim=-1).float()

    # 4. Wrapper 前向
    kt_output = wrapper.forward_sft(hidden_states, expert_ids, weights, save_for_backward=False)

    # 5. PyTorch 参考前向
    torch_output = moe_sft_torch_forward(
        hidden_states, expert_ids, weights,
        gate_proj, up_proj, down_proj,
        gate_lora_a, gate_lora_b, up_lora_a, up_lora_b, down_lora_a, down_lora_b,
        lora_alpha / lora_rank
    )

    # 6. 精度对比
    relative_diff = torch.abs(kt_output - torch_output).mean() / torch.abs(torch_output).mean()
    print(f"Forward relative diff: {relative_diff:.4%}")
    assert relative_diff < BF16_FORWARD_THRESHOLD, f"Forward accuracy failed: {relative_diff:.4%}"

    print("Forward accuracy test PASSED!")
```

### 1.2 反向精度测试

**目标**：验证反向传播计算的 6 个 LoRA 梯度与参考实现的一致性

**测试配置**：
```python
# 精度阈值
BF16_BACKWARD_THRESHOLD = 0.10  # 10% 相对误差
```

**测试代码**：
```python
def test_backward_accuracy():
    """反向精度测试"""
    # 1. 创建 Wrapper 并初始化（同上）
    wrapper = create_and_init_wrapper()

    # 2. 生成随机输入
    hidden_states = torch.randn(qlen, hidden_size, dtype=torch.bfloat16, device="cpu")
    expert_ids = torch.randint(0, expert_num, (qlen, num_experts_per_tok), dtype=torch.int64)
    weights = torch.softmax(torch.randn(qlen, num_experts_per_tok), dim=-1).float()

    # 3. Wrapper 前向 + 反向
    kt_output = wrapper.forward_sft(hidden_states, expert_ids, weights, save_for_backward=True)
    grad_output = torch.randn_like(kt_output)
    kt_grad_input, kt_grad_loras = wrapper.backward(grad_output)

    # 4. PyTorch 参考前向 + 反向
    torch_output, moe_saved = moe_sft_torch_forward(...)
    torch_grads = moe_sft_torch_backward(grad_output, moe_saved, ...)

    # 5. 精度对比
    gradient_names = [
        "grad_input", "grad_gate_lora_a", "grad_gate_lora_b",
        "grad_up_lora_a", "grad_up_lora_b", "grad_down_lora_a", "grad_down_lora_b"
    ]

    for name in gradient_names:
        kt_grad = kt_grad_input if name == "grad_input" else kt_grad_loras[name]
        torch_grad = torch_grads[name]

        relative_diff = torch.abs(kt_grad - torch_grad).mean() / (torch.abs(torch_grad).mean() + 1e-8)
        print(f"{name}: relative diff = {relative_diff:.4%}")
        assert relative_diff < BF16_BACKWARD_THRESHOLD, f"{name} accuracy failed"

    print("Backward accuracy test PASSED!")
```

### 1.3 训练循环测试

**目标**：验证完整的训练循环（前向→反向→优化器→权重同步）

**测试代码**：
```python
def test_training_loop():
    """训练循环测试"""
    # 1. 创建 Wrapper 并初始化
    wrapper = create_and_init_wrapper()

    # 2. 创建优化器
    optimizer = torch.optim.AdamW([
        wrapper.gate_lora_a, wrapper.gate_lora_b,
        wrapper.up_lora_a, wrapper.up_lora_b,
        wrapper.down_lora_a, wrapper.down_lora_b,
    ], lr=1e-4)

    # 3. 记录初始权重
    initial_weights = {
        "gate_lora_a": wrapper.gate_lora_a.clone(),
        "gate_lora_b": wrapper.gate_lora_b.clone(),
    }

    num_steps = 3
    for step in range(num_steps):
        # 3.1 前向传播
        hidden_states = torch.randn(qlen, hidden_size, dtype=torch.bfloat16, device="cpu")
        expert_ids = torch.randint(0, expert_num, (qlen, num_experts_per_tok), dtype=torch.int64)
        weights = torch.softmax(torch.randn(qlen, num_experts_per_tok), dim=-1).float()

        output = wrapper.forward_sft(hidden_states, expert_ids, weights, save_for_backward=True)

        # 3.2 计算损失（简化：直接使用 output 的均值作为损失）
        loss = output.mean()

        # 3.3 反向传播
        grad_output = torch.ones_like(output) / output.numel()
        grad_input, grad_loras = wrapper.backward(grad_output)

        # 3.4 设置梯度
        wrapper.gate_lora_a.grad = grad_loras["grad_gate_lora_a"]
        wrapper.gate_lora_b.grad = grad_loras["grad_gate_lora_b"]
        wrapper.up_lora_a.grad = grad_loras["grad_up_lora_a"]
        wrapper.up_lora_b.grad = grad_loras["grad_up_lora_b"]
        wrapper.down_lora_a.grad = grad_loras["grad_down_lora_a"]
        wrapper.down_lora_b.grad = grad_loras["grad_down_lora_b"]

        # 3.5 优化器步骤
        optimizer.step()

        # 3.6 同步权重到 C++
        wrapper.update_lora_weights()

        # 3.7 清零梯度
        optimizer.zero_grad()

        print(f"Step {step}: loss = {loss.item():.6f}")

    # 4. 验证权重已更新
    weight_changed = not torch.allclose(wrapper.gate_lora_a, initial_weights["gate_lora_a"])
    assert weight_changed, "Weights should have changed after training"

    print("Training loop test PASSED!")
```

---

## 2. TP/no-TP 模式测试

### 2.1 no-TP 配置测试

```python
def test_no_tp_mode():
    """no-TP 模式测试"""
    wrapper = KTMoEWrapper(
        layer_idx=0,
        num_experts=256,
        num_experts_per_tok=8,
        hidden_size=7168,
        moe_intermediate_size=2048,
        num_gpu_experts=0,
        cpuinfer_threads=60,
        threadpool_count=1,  # no-TP
        weight_path="",
        chunked_prefill_size=25600,
        method="AMXBF16_SFT",
        mode="sft",
        lora_rank=16,
        lora_alpha=32.0,
    )

    # 初始化和测试
    init_and_test_wrapper(wrapper, "no-TP")
```

### 2.2 TP 配置测试

```python
def test_tp_mode():
    """TP 模式测试"""
    wrapper = KTMoEWrapper(
        layer_idx=0,
        num_experts=256,
        num_experts_per_tok=8,
        hidden_size=7168,
        moe_intermediate_size=2048,  # 必须能被 threadpool_count 整除
        num_gpu_experts=0,
        cpuinfer_threads=60,
        threadpool_count=4,  # TP=4
        weight_path="",
        chunked_prefill_size=25600,
        method="AMXBF16_SFT",
        mode="sft",
        lora_rank=16,
        lora_alpha=32.0,
    )

    # 初始化和测试
    init_and_test_wrapper(wrapper, "TP=4")
```

### 2.3 TP 约束验证测试

```python
def test_tp_constraint():
    """TP 约束验证测试"""
    # 测试不满足约束的情况
    try:
        wrapper = KTMoEWrapper(
            ...,
            moe_intermediate_size=2049,  # 不能被 4 整除
            threadpool_count=4,
            ...
        )
        assert False, "Should raise error for invalid TP configuration"
    except RuntimeError as e:
        assert "intermediate_size" in str(e)
        print(f"Expected error caught: {e}")

    print("TP constraint test PASSED!")
```

### 2.4 TP 与 no-TP 结果一致性测试

```python
def test_tp_consistency():
    """TP 与 no-TP 结果一致性测试"""
    # 使用相同的权重和输入
    gate_proj, up_proj, down_proj = init_base_weights()
    gate_lora_a, gate_lora_b, up_lora_a, up_lora_b, down_lora_a, down_lora_b = init_lora_weights()

    hidden_states = torch.randn(qlen, hidden_size, dtype=torch.bfloat16, device="cpu")
    expert_ids = torch.randint(0, expert_num, (qlen, num_experts_per_tok), dtype=torch.int64)
    weights = torch.softmax(torch.randn(qlen, num_experts_per_tok), dim=-1).float()

    # no-TP Wrapper
    wrapper_no_tp = KTMoEWrapper(..., threadpool_count=1, ...)
    init_wrapper(wrapper_no_tp, gate_proj, up_proj, down_proj, ...)
    output_no_tp = wrapper_no_tp.forward_sft(hidden_states, expert_ids, weights, save_for_backward=False)

    # TP Wrapper（需要重新初始化 CPUInfer，建议在独立进程中测试）
    # 或者使用参考实现进行对比

    print("TP consistency test PASSED!")
```

---

## 3. 集成测试

### 3.1 LLaMA-Factory-KT 端到端测试

**测试配置文件** (`examples/ktransformers/sft/deepseek2_lora_sft_kt_wrapper.yaml`)：

```yaml
### Model
model_name_or_path: deepseek-ai/DeepSeek-V2-Lite-Chat
trust_remote_code: true

### Stage
stage: sft
finetuning_type: lora

### KTransformers 配置
use_kt: true
kt_backend: AMXBF16_SFT
kt_num_threads: 60
kt_tp_enabled: true
kt_tp_count: 4
kt_use_wrapper: true  # 使用新的 KTMoEWrapper 接口

### LoRA 配置
lora_rank: 16
lora_alpha: 32
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

### Dataset
dataset: alpaca_zh
template: deepseek2

### Training
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 1
max_steps: 100

### Output
output_dir: saves/deepseek2-lite/lora/kt-wrapper-test
logging_steps: 10
save_steps: 100
```

**运行命令**：
```bash
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \
    examples/ktransformers/sft/deepseek2_lora_sft_kt_wrapper.yaml
```

### 3.2 LoRA 训练验收测试

**验收标准**：
1. 训练过程无 NaN/Inf
2. Loss 稳定下降
3. 保存的 LoRA 权重可正确加载
4. 推理结果正常

**验收脚本**：
```python
def verify_training_result(output_dir):
    """验证训练结果"""
    import os
    from safetensors import safe_open

    # 1. 检查输出文件
    adapter_path = os.path.join(output_dir, "adapter_model.safetensors")
    assert os.path.exists(adapter_path), "Adapter file not found"

    # 2. 检查权重非 NaN
    with safe_open(adapter_path, framework="pt") as f:
        for key in f.keys():
            tensor = f.get_tensor(key)
            assert not torch.isnan(tensor).any(), f"NaN found in {key}"
            assert not torch.isinf(tensor).any(), f"Inf found in {key}"

    # 3. 检查 LoRA 权重形状
    lora_keys = [k for k in f.keys() if "lora" in k.lower()]
    assert len(lora_keys) > 0, "No LoRA weights found"

    print(f"Found {len(lora_keys)} LoRA weight tensors")
    print("Training verification PASSED!")
```

### 3.3 迁移验证测试

**目标**：验证迁移前后的训练结果一致性

**测试流程**：
1. 使用旧实现（kt_moe.py）训练 N 步，记录 Loss 曲线
2. 使用新实现（KTMoEWrapper）训练 N 步，记录 Loss 曲线
3. 对比 Loss 曲线的差异

**验收标准**：
- Loss 曲线趋势一致
- 最终 Loss 差异 < 5%

```python
def compare_training_curves(old_losses, new_losses):
    """对比训练曲线"""
    # 计算相对差异
    relative_diffs = []
    for old, new in zip(old_losses, new_losses):
        diff = abs(old - new) / (abs(old) + 1e-8)
        relative_diffs.append(diff)

    avg_diff = sum(relative_diffs) / len(relative_diffs)
    max_diff = max(relative_diffs)

    print(f"Average relative diff: {avg_diff:.4%}")
    print(f"Max relative diff: {max_diff:.4%}")

    assert avg_diff < 0.05, f"Average diff too large: {avg_diff:.4%}"
    print("Migration verification PASSED!")
```

---

## 4. 测试配置示例

### 4.1 YAML 配置文件

#### no-TP 模式配置

```yaml
# examples/ktransformers/sft/deepseek2_lora_sft_kt_no_tp.yaml
model_name_or_path: deepseek-ai/DeepSeek-V2-Lite-Chat
trust_remote_code: true

stage: sft
finetuning_type: lora

# KTransformers 配置
use_kt: true
kt_backend: AMXBF16_SFT
kt_num_threads: 60
kt_tp_enabled: false  # no-TP
kt_use_wrapper: true

# LoRA 配置
lora_rank: 16
lora_alpha: 32
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

# Dataset
dataset: alpaca_zh
template: deepseek2

# Training
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 1

# Output
output_dir: saves/deepseek2-lite/lora/kt-no-tp
```

#### TP 模式配置

```yaml
# examples/ktransformers/sft/deepseek2_lora_sft_kt_tp.yaml
model_name_or_path: deepseek-ai/DeepSeek-V2-Lite-Chat
trust_remote_code: true

stage: sft
finetuning_type: lora

# KTransformers 配置
use_kt: true
kt_backend: AMXBF16_SFT
kt_num_threads: 60
kt_tp_enabled: true   # TP 模式
kt_tp_count: 4        # 4 个 NUMA 节点
kt_use_wrapper: true

# LoRA 配置
lora_rank: 16
lora_alpha: 32
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

# Dataset
dataset: alpaca_zh
template: deepseek2

# Training
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 1

# Output
output_dir: saves/deepseek2-lite/lora/kt-tp
```

### 4.2 命令行调用示例

```bash
# no-TP 模式训练
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \
    examples/ktransformers/sft/deepseek2_lora_sft_kt_no_tp.yaml

# TP 模式训练
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \
    examples/ktransformers/sft/deepseek2_lora_sft_kt_tp.yaml

# 指定参数覆盖
CUDA_VISIBLE_DEVICES=0 llamafactory-cli train \
    examples/ktransformers/sft/deepseek2_lora_sft_kt_tp.yaml \
    --kt_num_threads 80 \
    --kt_tp_count 8 \
    --output_dir saves/test-run
```

### 4.3 独立测试脚本

```bash
# 运行 kt-kernel 单元测试
cd /home/lpl/ktransformers-llama/kt-kernel
python examples/test_moe_sft_wrapper.py --mode all --tp all

# 仅测试前向
python examples/test_moe_sft_wrapper.py --mode forward --method AMXBF16_SFT --tp no-tp

# 仅测试反向
python examples/test_moe_sft_wrapper.py --mode backward --method AMXBF16_SFT --tp tp

# 测试训练循环
python examples/test_moe_sft_wrapper.py --mode training --method AMXBF16_SFT --tp all
```

---

## 5. 常见问题与排查

### 5.1 NaN 问题调试

**症状**：训练过程中出现 NaN Loss 或输出

**可能原因**：

| 原因 | 排查方法 | 解决方案 |
|------|---------|---------|
| 梯度设备不匹配 | 检查 LoRA 梯度设备 | 确保 backward() 返回的梯度在正确设备上 |
| 权重未正确同步 | 检查 update_lora_weights() 调用 | 确保每次优化器步骤后调用 |
| 输入数据异常 | 检查输入是否包含 NaN/Inf | 添加输入检查 |
| dtype 不匹配 | 检查张量 dtype | 确保所有张量为 BF16 |

**调试代码**：
```python
def debug_nan(wrapper, hidden_states, expert_ids, weights):
    """NaN 调试"""
    # 检查输入
    assert not torch.isnan(hidden_states).any(), "NaN in input"
    assert not torch.isinf(hidden_states).any(), "Inf in input"

    # 前向传播
    output = wrapper.forward_sft(hidden_states, expert_ids, weights, save_for_backward=True)

    # 检查输出
    if torch.isnan(output).any():
        print("NaN detected in forward output!")
        # 保存调试数据
        torch.save({
            "hidden_states": hidden_states,
            "expert_ids": expert_ids,
            "weights": weights,
            "output": output,
        }, "/tmp/nan_debug_data.pt")
        raise RuntimeError("NaN in forward output")

    # 反向传播
    grad_output = torch.randn_like(output)
    grad_input, grad_loras = wrapper.backward(grad_output)

    # 检查梯度
    for name, grad in grad_loras.items():
        if torch.isnan(grad).any():
            print(f"NaN detected in {name}!")
            raise RuntimeError(f"NaN in {name}")

    print("No NaN detected!")
```

### 5.2 梯度不匹配问题

**症状**：反向传播计算的梯度与参考实现不一致

**可能原因**：

| 原因 | 排查方法 | 解决方案 |
|------|---------|---------|
| LoRA 缩放因子错误 | 检查 lora_scaling | 确保 lora_scaling = lora_alpha / lora_rank |
| 权重形状错误 | 打印权重形状 | 检查 LoRA 矩阵形状是否正确 |
| 缓存深度问题 | 检查 _cache_depth | 确保 forward_sft 时 save_for_backward=True |

**调试代码**：
```python
def debug_gradient_mismatch(wrapper, torch_ref):
    """梯度不匹配调试"""
    # 使用相同输入
    hidden_states = torch.randn(qlen, hidden_size, dtype=torch.bfloat16)
    expert_ids = torch.randint(0, expert_num, (qlen, num_experts_per_tok))
    weights = torch.softmax(torch.randn(qlen, num_experts_per_tok), dim=-1).float()

    # Wrapper 计算
    kt_output = wrapper.forward_sft(hidden_states, expert_ids, weights, save_for_backward=True)
    grad_output = torch.randn_like(kt_output)
    kt_grad_input, kt_grad_loras = wrapper.backward(grad_output)

    # PyTorch 参考计算
    torch_output, saved = torch_ref.forward(hidden_states, expert_ids, weights)
    torch_grads = torch_ref.backward(grad_output, saved)

    # 逐一对比
    for name in kt_grad_loras.keys():
        kt_grad = kt_grad_loras[name]
        torch_grad = torch_grads[name]

        abs_diff = torch.abs(kt_grad - torch_grad)
        rel_diff = abs_diff / (torch.abs(torch_grad) + 1e-8)

        print(f"{name}:")
        print(f"  Max abs diff: {abs_diff.max():.6f}")
        print(f"  Mean abs diff: {abs_diff.mean():.6f}")
        print(f"  Max rel diff: {rel_diff.max():.4%}")
        print(f"  Mean rel diff: {rel_diff.mean():.4%}")
```

### 5.3 TP 约束检查

**症状**：TP 模式初始化失败

**错误信息**：
```
RuntimeError: For TP, intermediate_size (2049) must be a multiple of threadpool_count (4)
```

**解决方案**：
1. 调整 `threadpool_count` 使其能整除 `intermediate_size`
2. 或使用 no-TP 模式（`threadpool_count=1`）

**检查脚本**：
```python
def check_tp_constraint(intermediate_size, threadpool_count):
    """检查 TP 约束"""
    if threadpool_count == 1:
        print("no-TP mode: no constraint")
        return True

    if intermediate_size % threadpool_count != 0:
        print(f"ERROR: intermediate_size ({intermediate_size}) % threadpool_count ({threadpool_count}) = {intermediate_size % threadpool_count}")
        print("Suggested threadpool_count values:")
        for tp in [1, 2, 4, 8, 16]:
            if intermediate_size % tp == 0:
                print(f"  - {tp}")
        return False

    print(f"TP constraint satisfied: {intermediate_size} / {threadpool_count} = {intermediate_size // threadpool_count}")
    return True
```

### 5.4 内存不足问题

**症状**：训练过程中 OOM（Out of Memory）

**可能原因**：

| 原因 | 排查方法 | 解决方案 |
|------|---------|---------|
| 缓冲区累积 | 检查 buffer 缓存 | 调用 `KTMoEWrapper.clear_sft_buffer_cache()` |
| batch size 过大 | 检查 qlen | 减小 batch size |
| 梯度累积 | 检查梯度是否正确清零 | 确保 `optimizer.zero_grad()` 被调用 |

**清理脚本**：
```python
import gc
import torch

def clean_memory():
    """清理内存"""
    # 清理 KT buffer 缓存
    from kt_kernel.experts import KTMoEWrapper
    KTMoEWrapper.clear_sft_buffer_cache()

    # 清理 PyTorch 缓存
    gc.collect()
    torch.cuda.empty_cache()

    print("Memory cleaned!")
```

---

## 6. 性能测试

### 6.1 前向性能测试

```python
import time

def benchmark_forward(wrapper, num_iterations=100, warmup=10):
    """前向性能测试"""
    hidden_states = torch.randn(128, 7168, dtype=torch.bfloat16, device="cpu")
    expert_ids = torch.randint(0, 256, (128, 8), dtype=torch.int64)
    weights = torch.softmax(torch.randn(128, 8), dim=-1).float()

    # 预热
    for _ in range(warmup):
        wrapper.forward_sft(hidden_states, expert_ids, weights, save_for_backward=False)

    # 计时
    start = time.time()
    for _ in range(num_iterations):
        wrapper.forward_sft(hidden_states, expert_ids, weights, save_for_backward=False)
    elapsed = time.time() - start

    avg_time = elapsed / num_iterations * 1000  # ms
    print(f"Forward: {avg_time:.2f} ms/iter")
    return avg_time
```

### 6.2 反向性能测试

```python
def benchmark_backward(wrapper, num_iterations=100, warmup=10):
    """反向性能测试"""
    hidden_states = torch.randn(128, 7168, dtype=torch.bfloat16, device="cpu")
    expert_ids = torch.randint(0, 256, (128, 8), dtype=torch.int64)
    weights = torch.softmax(torch.randn(128, 8), dim=-1).float()

    # 预热
    for _ in range(warmup):
        wrapper.forward_sft(hidden_states, expert_ids, weights, save_for_backward=True)
        grad_output = torch.randn(128, 7168, dtype=torch.bfloat16, device="cpu")
        wrapper.backward(grad_output)

    # 计时
    start = time.time()
    for _ in range(num_iterations):
        wrapper.forward_sft(hidden_states, expert_ids, weights, save_for_backward=True)
        grad_output = torch.randn(128, 7168, dtype=torch.bfloat16, device="cpu")
        wrapper.backward(grad_output)
    elapsed = time.time() - start

    avg_time = elapsed / num_iterations * 1000  # ms
    print(f"Forward + Backward: {avg_time:.2f} ms/iter")
    return avg_time
```

### 6.3 TP 加速比测试

```python
def benchmark_tp_speedup():
    """TP 加速比测试"""
    # no-TP 性能
    wrapper_no_tp = create_wrapper(threadpool_count=1)
    time_no_tp = benchmark_forward(wrapper_no_tp)

    # TP 性能（在独立进程中测试更准确）
    # wrapper_tp = create_wrapper(threadpool_count=4)
    # time_tp = benchmark_forward(wrapper_tp)

    # 计算加速比
    # speedup = time_no_tp / time_tp
    # print(f"TP speedup: {speedup:.2f}x")
```
