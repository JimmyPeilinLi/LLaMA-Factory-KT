# KTMoEWrapper SFT 集成功能详细设计

## 1. 架构设计

### 1.1 类继承关系图

```
┌─────────────────────────────────────────────────────────────────┐
│                    KTMoEWrapper (工厂类)                          │
│         根据 mode 和 method 参数返回具体实现                        │
│  mode="inference" → BaseMoEWrapper    mode="sft" → BaseSFTMoEWrapper │
└─────────────────────────────────────────────────────────────────┘
                              │
          ┌───────────────────┴───────────────────┐
          │                                       │
┌─────────────────────────┐         ┌─────────────────────────┐
│      _MoEBase           │         │      _MoEBase           │
│    (共享基类)            │         │    (共享基类)            │
│ - _cpu_infer_instance   │         │ - _cpu_infer_instance   │
│ - _get_cpu_infer()      │         │ - _get_cpu_infer()      │
│ - _validate_base_config │         │ - _validate_base_config │
└─────────┬───────────────┘         └─────────┬───────────────┘
          │                                   │
┌─────────────────────────┐         ┌─────────────────────────┐
│   BaseMoEWrapper        │         │   BaseSFTMoEWrapper     │
│   (推理基类)             │         │   (SFT 基类)            │
│ - forward()             │         │ - forward_sft()         │
│ - submit_forward()      │         │ - backward()            │
│ - sync_forward()        │         │ - update_lora_weights() │
│ - load_weights()        │         │ - init_lora_weights()   │
└─────────┬───────────────┘         └─────────┬───────────────┘
          │                                   │
┌─────────────────────────┐         ┌─────────────────────────┐
│   AMXMoEWrapper         │         │   AMXSFTMoEWrapper      │
│   NativeMoEWrapper      │         │   (AMX SFT 实现)         │
│   LlamafileMoEWrapper   │         │                         │
│   GeneralMoEWrapper     │         │                         │
└─────────────────────────┘         └─────────────────────────┘
```

### 1.2 与现有 kt_moe.py 的对比

| 组件 | 现有实现 (kt_moe.py) | 新实现 (KTMoEWrapper) |
|------|---------------------|----------------------|
| 工厂入口 | 无统一入口 | `KTMoEWrapper(mode, method)` |
| CPUInfer 管理 | 手动创建 `kt_kernel_ext.CPUInfer` | `_MoEBase._get_cpu_infer()` 自动单例 |
| 配置对象 | 直接使用 `MOESFTConfig` | 内部封装，用户无需感知 |
| 前向传播 | `MOEAMXFunction.forward()` | `wrapper.forward_sft()` |
| 反向传播 | `MOEAMXFunction.backward()` | `wrapper.backward()` |
| LoRA 管理 | `MOELayerWrapper.lora_params` | `wrapper.init_lora_weights()` |
| PyTorch 集成 | 自定义 `torch.autograd.Function` | 需在 LLaMA-Factory 侧集成 |

### 1.3 TP/no-TP 架构

```
                    threadpool_count 参数
                           │
         ┌─────────────────┴─────────────────┐
         │                                   │
    threadpool_count=1                 threadpool_count>1
         │                                   │
    ┌────┴────┐                        ┌─────┴─────┐
    │ no-TP   │                        │    TP     │
    │  模式   │                        │   模式    │
    └────┬────┘                        └─────┬─────┘
         │                                   │
┌────────────────────┐           ┌────────────────────────┐
│ 单 NUMA 子池        │           │ 多 NUMA 子池            │
│ subpool_count = 1  │           │ subpool_count = N      │
│ 所有线程在同一节点   │           │ 权重按 NUMA 节点分割    │
│ 无权重分割          │           │ 自动线程分配            │
└────────────────────┘           └────────────────────────┘
```

**TP 模式线程分配算法**：

```python
# 总线程数: cpuinfer_threads
# NUMA 节点数: threadpool_count

for i in range(threadpool_count):
    threads_for_numa_i = cpuinfer_threads // threadpool_count
    if i < cpuinfer_threads % threadpool_count:
        threads_for_numa_i += 1  # 前 extra 个节点多分配 1 个线程
```

**示例**：`cpuinfer_threads=60, threadpool_count=4`
- NUMA 0: 15 线程
- NUMA 1: 15 线程
- NUMA 2: 15 线程
- NUMA 3: 15 线程

---

## 2. 迁移方案设计

### 2.1 代码迁移对照表

#### CPUInfer 初始化

**迁移前** (kt_moe.py:341-374)：
```python
def init_cpu_infer(model_args):
    from kt_kernel import kt_kernel_ext

    if model_args.kt_tp_enabled:
        cpu_infer = kt_kernel_ext.CPUInfer(model_args.kt_num_threads)
    else:
        pool_config = kt_kernel_ext.WorkerPoolConfig()
        pool_config.subpool_count = 1
        pool_config.subpool_numa_map = [0]
        pool_config.subpool_thread_count = [model_args.kt_num_threads]
        cpu_infer = kt_kernel_ext.CPUInfer(pool_config)

    return cpu_infer
```

**迁移后**：
```python
# CPUInfer 自动管理，无需显式初始化
# 通过 KTMoEWrapper 参数控制：
#   - cpuinfer_threads: 总线程数
#   - threadpool_count: NUMA 子池数量（TP 控制）
```

#### MoE 层包装

**迁移前** (kt_moe.py:839-1010)：
```python
def wrap_moe_layers_with_amx(model, model_args, finetuning_args, cpu_infer):
    for layer_idx, layer in enumerate(model.model.layers):
        # 1. 创建 MOESFTConfig
        config = kt_kernel_ext.moe.MOESFTConfig()
        config.expert_num = moe_config.expert_num
        config.lora_rank = finetuning_args.lora_rank
        # ...

        # 2. 创建 AMX MOE 实例
        moe_amx = kt_kernel_ext.moe.AMXBF16_SFT_MOE(config)

        # 3. 加载权重
        cpu_infer.submit(moe_amx.load_weights_task())
        cpu_infer.sync()

        # 4. 创建自定义 Wrapper
        wrapper = MOELayerWrapper(original_moe, moe_amx, cpu_infer, lora_params, ...)
        setattr(layer, moe_config.moe_layer_attr, wrapper)
```

**迁移后**：
```python
def wrap_moe_layers_with_kt_wrapper(model, model_args, finetuning_args):
    from kt_kernel.experts import KTMoEWrapper

    for layer_idx, layer in enumerate(model.model.layers):
        # 1. 使用统一工厂接口
        wrapper = KTMoEWrapper(
            layer_idx=layer_idx,
            num_experts=moe_config.expert_num,
            num_experts_per_tok=moe_config.num_experts_per_tok,
            hidden_size=model.config.hidden_size,
            moe_intermediate_size=moe_config.intermediate_size,
            num_gpu_experts=0,  # 当前全放 CPU
            cpuinfer_threads=model_args.kt_num_threads,
            threadpool_count=model_args.kt_tp_count if model_args.kt_tp_enabled else 1,
            weight_path=model_args.kt_weight_path or "",
            chunked_prefill_size=25600,
            method="AMXBF16_SFT",
            mode="sft",
            lora_rank=finetuning_args.lora_rank,
            lora_alpha=finetuning_args.lora_alpha,
        )

        # 2. 加载权重
        wrapper.load_weights_from_tensors(gate_proj, up_proj, down_proj, physical_map)

        # 3. 初始化 LoRA
        wrapper.init_lora_weights(gate_lora_a, gate_lora_b, ...)

        # 4. 创建适配层（包装 Wrapper 以兼容 PyTorch autograd）
        kt_wrapper = KTMoELayerWrapper(wrapper, router, shared_experts, ...)
        setattr(layer, moe_config.moe_layer_attr, kt_wrapper)
```

#### 前向/反向传播

**迁移前** (kt_moe.py:389-638)：
```python
class MOEAMXFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, hidden_states, topk_ids, topk_weights, moe_amx, cpu_infer, ...):
        # 手动管理 CPU 张量转换
        input_data = hidden_states.view(qlen, hidden_size).to(torch.bfloat16).cpu().contiguous()
        output = torch.zeros((qlen, hidden_size), dtype=torch.bfloat16, device="cpu")

        # 提交 C++ task
        cpu_infer.submit(moe_amx.forward_sft_task(...))
        cpu_infer.sync()

        return output.to(device=original_device, dtype=original_dtype)

    @staticmethod
    def backward(ctx, grad_output):
        # 手动管理梯度缓冲区
        grad_input = torch.zeros((qlen, hidden_size), dtype=torch.bfloat16, device="cpu")
        grad_gate_lora_a = torch.zeros_like(...)

        ctx.cpu_infer.submit(ctx.moe_amx.backward_task(...))
        ctx.cpu_infer.sync()

        return grad_input, None, None, ...
```

**迁移后**：
```python
class KTMoEFunction(torch.autograd.Function):
    @staticmethod
    def forward(ctx, hidden_states, topk_ids, topk_weights, kt_wrapper, training):
        # 直接调用 Wrapper 方法
        output = kt_wrapper.forward_sft(
            hidden_states, topk_ids, topk_weights,
            save_for_backward=training
        )
        ctx.kt_wrapper = kt_wrapper
        ctx.training = training
        return output

    @staticmethod
    def backward(ctx, grad_output):
        # 直接调用 Wrapper 方法
        grad_input, grad_loras = ctx.kt_wrapper.backward(grad_output)

        # 累积 LoRA 梯度到参数
        # ...

        return grad_input, None, None, None, None
```

### 2.2 迁移步骤

1. **引入 KTMoEWrapper 依赖**
   - 确保 kt-kernel 版本支持 `KTMoEWrapper` 工厂接口
   - 添加 `from kt_kernel.experts import KTMoEWrapper`

2. **重构 MoE 层包装逻辑**
   - 删除 `init_cpu_infer()` 函数
   - 修改 `wrap_moe_layers_with_amx()` → `wrap_moe_layers_with_kt_wrapper()`

3. **适配 PyTorch autograd**
   - 修改 `MOEAMXFunction` → `KTMoEFunction`
   - 简化前向/反向逻辑，委托给 Wrapper

4. **更新配置参数映射**
   - `kt_tp_enabled` → `threadpool_count > 1`
   - `kt_num_threads` → `cpuinfer_threads`
   - `kt_backend` → `method`

---

## 3. 核心组件

### 3.1 KTMoEWrapper 工厂类

**位置**：`kt_kernel/experts.py`

**职责**：
- 验证 `mode` 和 `method` 参数兼容性
- 根据参数组合创建具体的 Wrapper 实例
- 提供静态工具方法（如 `clear_buffer_cache()`）

**模式验证规则**：
```python
# 推理模式支持的方法
INFERENCE_METHODS = {"AMXINT4", "AMXINT8", "RAWINT4", "FP8", "LLAMAFILE", "MOE_INT4", "MOE_INT8"}

# SFT 模式支持的方法
SFT_METHODS = {"AMXBF16_SFT", "AMXINT8_SFT", "AMXINT4_SFT", "AMXINT4_1_SFT",
               "AMXINT4_KGroup_SFT", "AMXINT4_1KGroup_SFT"}
```

### 3.2 BaseSFTMoEWrapper SFT 基类

**位置**：`kt_kernel/experts_sft.py`

**职责**：
- 定义 SFT 模式的抽象方法接口
- 管理 LoRA 权重占位符
- 状态跟踪（`_weights_loaded`, `_lora_initialized`, `_cache_depth`）

**抽象方法**：
```python
@abstractmethod
def load_weights(self, physical_to_logical_map_cpu: torch.Tensor) -> None: ...

@abstractmethod
def init_lora_weights(self, gate_lora_a, gate_lora_b, ...) -> None: ...

@abstractmethod
def forward_sft(self, hidden_states, expert_ids, weights, save_for_backward) -> torch.Tensor: ...

@abstractmethod
def backward(self, grad_output) -> Tuple[torch.Tensor, Dict[str, torch.Tensor]]: ...

@abstractmethod
def update_lora_weights(self) -> None: ...
```

### 3.3 AMXSFTMoEWrapper AMX SFT 实现

**位置**：`kt_kernel/utils/amx_sft.py`

**职责**：
- 实现 `BaseSFTMoEWrapper` 的所有抽象方法
- 管理与 C++ AMX 内核的交互
- 处理缓冲区分配和梯度计算

**支持的量化方法映射**：
```python
_SFT_METHOD_TO_CLASS = {
    "AMXBF16_SFT": AMXBF16_SFT_MOE,
    "AMXINT8_SFT": AMXInt8_SFT_MOE,
    "AMXINT4_SFT": AMXInt4_SFT_MOE,
    "AMXINT4_1_SFT": AMXInt4_1_SFT_MOE,
    "AMXINT4_KGroup_SFT": AMXInt4_KGroup_SFT_MOE,
    "AMXINT4_1KGroup_SFT": AMXInt4_1KGroup_SFT_MOE,
}
```

### 3.4 KExpertsSFTBuffer 缓冲区管理

**位置**：`kt_kernel/experts_sft.py`

**职责**：
- 管理 SFT 前向/反向所需的 CPU 缓冲区
- 缓冲池模式，根据参数组合复用缓冲区
- 提供 LoRA 梯度访问接口

**缓冲区内容**：
```python
# 前向缓冲
input_cpu: torch.Tensor              # [qlen, hidden_size]
expert_ids_cpu: torch.Tensor         # [qlen, num_experts_per_tok]
weights_cpu: torch.Tensor            # [qlen, num_experts_per_tok]
output_cpu: torch.Tensor             # [qlen, hidden_size]

# 反向缓冲
grad_output_cpu: torch.Tensor        # [qlen, hidden_size]
grad_input_cpu: torch.Tensor         # [qlen, hidden_size]

# LoRA 梯度缓冲 (6 个)
grad_gate_lora_a: torch.Tensor       # [num_experts, lora_rank, hidden_size]
grad_gate_lora_b: torch.Tensor       # [num_experts, intermediate_size, lora_rank]
grad_up_lora_a: torch.Tensor         # [num_experts, lora_rank, hidden_size]
grad_up_lora_b: torch.Tensor         # [num_experts, intermediate_size, lora_rank]
grad_down_lora_a: torch.Tensor       # [num_experts, lora_rank, intermediate_size]
grad_down_lora_b: torch.Tensor       # [num_experts, hidden_size, lora_rank]
```

---

## 4. TP/no-TP 设计

### 4.1 TP 模式详解

**触发条件**：`threadpool_count > 1`

**权重分割**：
```
原始权重形状: [num_experts, intermediate_size, hidden_size]

TP 分割后 (threadpool_count=4):
  NUMA 0: [num_experts, intermediate_size/4, hidden_size]
  NUMA 1: [num_experts, intermediate_size/4, hidden_size]
  NUMA 2: [num_experts, intermediate_size/4, hidden_size]
  NUMA 3: [num_experts, intermediate_size/4, hidden_size]
```

**计算流程**：
```
1. 输入广播到所有 NUMA 节点
2. 每个 NUMA 节点计算部分输出
3. 归并各 NUMA 节点的结果
```

### 4.2 no-TP 模式详解

**触发条件**：`threadpool_count == 1`

**特点**：
- 所有计算在单个 NUMA 节点
- 无权重分割
- 适合单 NUMA 系统或调试

### 4.3 约束条件

```python
# C++ 层约束
if intermediate_size % threadpool_count != 0:
    raise RuntimeError(
        f"For TP, intermediate_size ({intermediate_size}) must be "
        f"a multiple of threadpool_count ({threadpool_count})"
    )
```

---

## 5. 可扩展性设计

### 5.1 添加新 SFT 方法的步骤

1. **在 experts.py 中注册方法**：
   ```python
   SFT_METHODS = frozenset([
       "AMXBF16_SFT",
       # ... 现有方法 ...
       "NEW_METHOD_SFT",  # 新方法
   ])
   ```

2. **在 amx_sft.py 中注册 C++ 类映射**：
   ```python
   _SFT_METHOD_TO_CLASS = {
       # ... 现有映射 ...
       "NEW_METHOD_SFT": NewMethod_SFT_MOE,  # 新映射
   }
   ```

3. **无需修改 KTMoEWrapper 或 BaseSFTMoEWrapper**

### 5.2 预留接口

| 参数 | 当前值 | 用途 | 未来扩展 |
|------|-------|------|---------|
| `num_gpu_experts` | 0 | 控制 GPU 上的专家数量 | 支持混合 CPU/GPU 专家 |
| `group_size` | 128 | K-Group 量化的组大小 | 条件字段，仅 K-Group 方法使用 |
| `zero_point` | True | 是否使用零点量化 | 条件字段，仅 K-Group 方法使用 |

### 5.3 配置对象的条件字段模式

```python
# amx_sft.py 中的实现
config = MOESFTConfig()
config.expert_num = self.num_experts
# ... 必需字段 ...

# 条件添加 K-Group 特定配置
if self.method in ("AMXINT4_KGroup_SFT", "AMXINT4_1KGroup_SFT"):
    config.quant_config.group_size = self.group_size
    config.quant_config.zero_point = self.zero_point
```

---

## 6. 数据流设计

### 6.1 前向传播流程 (forward_sft)

```
输入: hidden_states [batch, seq, hidden_size]
      expert_ids [batch*seq, k]
      weights [batch*seq, k]

┌─────────────────────────────────────────────────┐
│ 1. 输入预处理                                    │
│    - Flatten: [batch, seq, hidden] → [qlen, hidden] │
│    - 转换 dtype 和 device (GPU → CPU, BF16)        │
└─────────────────────────────────────────────────┘
                      ↓
┌─────────────────────────────────────────────────┐
│ 2. 获取缓冲区                                    │
│    - KExpertsSFTBuffer.get_buffer(qlen, ...)     │
│    - 复制输入数据到 CPU 缓冲区                    │
└─────────────────────────────────────────────────┘
                      ↓
┌─────────────────────────────────────────────────┐
│ 3. 提交 C++ 任务                                 │
│    - cpu_infer.submit(moe.forward_sft_task(...)) │
│    - 如果 save_for_backward=True，保存激活值     │
└─────────────────────────────────────────────────┘
                      ↓
┌─────────────────────────────────────────────────┐
│ 4. 同步等待                                      │
│    - cpu_infer.sync()                            │
│    - 更新 _cache_depth                           │
└─────────────────────────────────────────────────┘
                      ↓
┌─────────────────────────────────────────────────┐
│ 5. 输出后处理                                    │
│    - 转换 dtype 和 device (CPU → GPU)            │
│    - Reshape: [qlen, hidden] → [batch, seq, hidden] │
└─────────────────────────────────────────────────┘

输出: output [batch, seq, hidden_size]
```

### 6.2 反向传播流程 (backward)

```
输入: grad_output [batch, seq, hidden_size]

┌─────────────────────────────────────────────────┐
│ 1. 检查状态                                      │
│    - 确保 _cache_depth > 0（有前向缓存）          │
│    - 获取对应的缓冲区                            │
└─────────────────────────────────────────────────┘
                      ↓
┌─────────────────────────────────────────────────┐
│ 2. 输入预处理                                    │
│    - Flatten 和 dtype/device 转换                │
│    - 零初始化梯度缓冲区                          │
└─────────────────────────────────────────────────┘
                      ↓
┌─────────────────────────────────────────────────┐
│ 3. 提交 C++ 任务                                 │
│    - cpu_infer.submit(moe.backward_task(...))    │
│    - 使用前向缓存的激活值                        │
└─────────────────────────────────────────────────┘
                      ↓
┌─────────────────────────────────────────────────┐
│ 4. 同步等待                                      │
│    - cpu_infer.sync()                            │
│    - _cache_depth -= 1                           │
└─────────────────────────────────────────────────┘
                      ↓
┌─────────────────────────────────────────────────┐
│ 5. 输出后处理                                    │
│    - grad_input: dtype/device 转换               │
│    - grad_loras: 返回 6 个梯度张量的字典         │
└─────────────────────────────────────────────────┘

输出: (grad_input, grad_loras_dict)
       grad_input: [batch, seq, hidden_size]
       grad_loras_dict: {
           "grad_gate_lora_a": [...],
           "grad_gate_lora_b": [...],
           ...
       }
```

### 6.3 LoRA 权重同步流程 (update_lora_weights)

```
触发时机: 优化器 step() 之后

┌─────────────────────────────────────────────────┐
│ 1. 检查 LoRA 权重状态                            │
│    - 确保 _lora_initialized = True               │
│    - 获取 Python 端 LoRA 张量指针                │
└─────────────────────────────────────────────────┘
                      ↓
┌─────────────────────────────────────────────────┐
│ 2. 提交同步任务                                  │
│    - cpu_infer.submit(                          │
│        moe.update_lora_weights_task(             │
│          gate_lora_a.data_ptr(),                 │
│          gate_lora_b.data_ptr(),                 │
│          ...                                     │
│        )                                         │
│      )                                           │
└─────────────────────────────────────────────────┘
                      ↓
┌─────────────────────────────────────────────────┐
│ 3. 同步等待                                      │
│    - cpu_infer.sync()                            │
│    - C++ 端读取更新后的 LoRA 权重                 │
└─────────────────────────────────────────────────┘

注意: 这是零拷贝操作，只传递指针
```

---

## 7. LLaMA-Factory-KT 集成设计

### 7.1 新增 KTMoELayerWrapper

```python
class KTMoELayerWrapper(nn.Module):
    """
    适配 KTMoEWrapper 到 LLaMA-Factory 的 nn.Module 封装。

    职责：
    1. 包装 KTMoEWrapper 实例
    2. 处理 Router 调用
    3. 管理 LoRA 参数（用于 PyTorch 优化器）
    4. 处理 Shared Experts
    """

    def __init__(self, kt_wrapper, router, shared_experts, moe_config, lora_params):
        super().__init__()
        self.kt_wrapper = kt_wrapper  # BaseSFTMoEWrapper 实例
        self.router = router
        self.shared_experts = shared_experts
        self.moe_config = moe_config
        self.lora_params = nn.ParameterDict(lora_params)  # 用于优化器
        self._is_kt_moe_wrapper = True  # PEFT 识别标记

    def forward(self, hidden_states):
        # 1. Router 调用
        if self.moe_config.router_type == "deepseek_gate":
            topk_ids, topk_weights, _ = self.router(hidden_states)
        else:
            router_logits = self.router(hidden_states.view(-1, self.hidden_size))
            routing_weights = F.softmax(router_logits, dim=-1)
            topk_weights, topk_ids = torch.topk(routing_weights, k, dim=-1)
            topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)

        # 2. 调用 KT Wrapper
        output = KTMoEFunction.apply(
            hidden_states, topk_ids, topk_weights,
            self.kt_wrapper, self.training
        )

        # 3. Shared Experts
        if self.shared_experts is not None:
            output = output + self.shared_experts(hidden_states)

        return output

    def update_lora_pointers(self):
        """在 LoRA 权重更新后调用"""
        self.kt_wrapper.update_lora_weights()
```

### 7.2 新增 KTMoEFunction

```python
class KTMoEFunction(torch.autograd.Function):
    """
    PyTorch autograd Function，桥接 KTMoEWrapper 和 PyTorch 自动微分。
    """

    @staticmethod
    def forward(ctx, hidden_states, topk_ids, topk_weights, kt_wrapper, training):
        output = kt_wrapper.forward_sft(
            hidden_states.view(-1, hidden_states.size(-1)),
            topk_ids.view(-1, topk_ids.size(-1)),
            topk_weights.view(-1, topk_weights.size(-1)),
            save_for_backward=training
        )

        ctx.kt_wrapper = kt_wrapper
        ctx.training = training
        ctx.shape = hidden_states.shape

        return output.view(ctx.shape)

    @staticmethod
    def backward(ctx, grad_output):
        grad_input, grad_loras = ctx.kt_wrapper.backward(
            grad_output.view(-1, grad_output.size(-1))
        )

        # 梯度累积到 LoRA 参数（在 KTMoELayerWrapper 中处理）
        ctx.grad_loras = grad_loras

        return grad_input.view(ctx.shape), None, None, None, None
```
