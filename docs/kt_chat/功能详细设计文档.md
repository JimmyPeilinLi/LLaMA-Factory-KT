# KTransformers Chat 推理功能详细设计文档

## 1. 架构设计

### 1.1 双通道 LoRA 加载架构

```
┌─────────────────────────────────────────────────────────────┐
│                    PEFT Adapter File                         │
│  adapter_model.safetensors + adapter_config.json            │
└─────────────────┬───────────────────────────┬───────────────┘
                  │                           │
                  ▼                           ▼
        ┌─────────────────┐         ┌─────────────────┐
        │  Attention LoRA │         │    MoE LoRA     │
        │  (q/k/v/o_proj) │         │ (gate/up/down)  │
        └────────┬────────┘         └────────┬────────┘
                 │                           │
                 ▼                           ▼
        ┌─────────────────┐         ┌─────────────────┐
        │      PEFT       │         │   KT Wrapper    │
        │ PeftModel.from_ │         │ load_moe_lora_  │
        │   pretrained()  │         │ from_adapter()  │
        └────────┬────────┘         └────────┬────────┘
                 │                           │
                 ▼                           ▼
        ┌─────────────────┐         ┌─────────────────┐
        │   GPU (CUDA)    │         │   CPU (AMX)     │
        │  Attention 计算  │         │   MoE 计算      │
        └─────────────────┘         └─────────────────┘
```

### 1.2 核心问题与解决方案

**问题**：PEFT `PeftModel.from_pretrained()` 会尝试包装所有在 `adapter_config.json` 中指定的 `target_modules`，包括 MoE 层。但 KT 模式下 MoE 层的基础权重已被移到 CPU，GPU 上只有空壳。

**解决方案**：
1. 创建临时目录，复制 adapter 文件
2. 修改 `adapter_config.json`，移除 MoE 模块
3. 从临时目录加载 Attention LoRA
4. 单独加载 MoE LoRA 到 KT wrapper

---

## 2. 代码修改详解

### 2.1 adapter.py 修改

**文件**: `src/llamafactory/model/adapter.py`

**修改位置**: `_setup_lora_tuning()` 函数

**核心逻辑**:

```python
# 检测 KT 模式
is_kt_mode = getattr(model, "_kt_wrappers", None) is not None

for adapter in adapter_to_merge:
    if is_kt_mode:
        # 1. 读取原始 adapter 配置
        adapter_config_path = os.path.join(adapter, "adapter_config.json")
        with open(adapter_config_path, "r") as f:
            adapter_config = json.load(f)

        # 2. 过滤 MoE 模块
        moe_modules = {"gate_proj", "up_proj", "down_proj"}
        original_targets = adapter_config.get("target_modules", [])
        attention_targets = [m for m in original_targets if m not in moe_modules]

        if attention_targets != original_targets:
            # 3. 创建临时目录
            temp_dir = tempfile.mkdtemp(prefix="kt_adapter_")
            try:
                # 4. 复制文件，修改配置
                for file in os.listdir(adapter):
                    src = os.path.join(adapter, file)
                    dst = os.path.join(temp_dir, file)
                    if file == "adapter_config.json":
                        # 写入修改后的配置
                        modified_config = adapter_config.copy()
                        modified_config["target_modules"] = attention_targets
                        with open(dst, "w") as f:
                            json.dump(modified_config, f)
                    else:
                        # 其他文件使用符号链接
                        os.symlink(src, dst)

                # 5. 从临时目录加载 Attention LoRA
                model = PeftModel.from_pretrained(model, temp_dir, **init_kwargs)
            finally:
                # 6. 清理临时目录
                shutil.rmtree(temp_dir, ignore_errors=True)

            # 7. 加载 MoE LoRA 到 KT wrapper
            from .model_utils.kt_moe import load_moe_lora_from_adapter
            load_moe_lora_from_adapter(model, adapter)
```

**关键点**:
- 使用符号链接避免复制大文件 (`adapter_model.safetensors`)
- 临时目录在 `finally` 中清理，确保异常时也能清理
- 不跳过 `merge_and_unload()`，因为 KT 模式下不需要合并

### 2.2 kt_moe.py 修改

**文件**: `src/llamafactory/model/model_utils/kt_moe.py`

**新增函数**: `load_moe_lora_from_adapter()`

```python
def load_moe_lora_from_adapter(model: "PreTrainedModel", adapter_path: str):
    """
    从 PEFT adapter 加载 MoE LoRA 权重到 KT wrappers。

    PEFT 保存格式:
    - base_model.model.model.layers.{layer}.mlp.original_moe.experts.{expert}.gate_proj.lora_A.weight

    KT 期望格式:
    - gate_lora_a: [num_experts, lora_rank, hidden_size]
    - gate_lora_b: [num_experts, intermediate_size, lora_rank]
    """
```

**数据格式转换**:

| 来源 | 维度 | 说明 |
|------|------|------|
| PEFT lora_A | `[lora_rank, in_features]` | 每个 expert 单独存储 |
| PEFT lora_B | `[out_features, lora_rank]` | 每个 expert 单独存储 |
| KT lora_a | `[num_experts, lora_rank, in_features]` | 所有 expert 堆叠 |
| KT lora_b | `[num_experts, out_features, lora_rank]` | 所有 expert 堆叠 |

**权重映射表**:

| PEFT Key 模式 | KT 参数名 |
|--------------|----------|
| `layers.{L}.mlp.original_moe.experts.{E}.gate_proj.lora_A.weight` | `gate_lora_a[E]` |
| `layers.{L}.mlp.original_moe.experts.{E}.gate_proj.lora_B.weight` | `gate_lora_b[E]` |
| `layers.{L}.mlp.original_moe.experts.{E}.up_proj.lora_A.weight` | `up_lora_a[E]` |
| `layers.{L}.mlp.original_moe.experts.{E}.up_proj.lora_B.weight` | `up_lora_b[E]` |
| `layers.{L}.mlp.original_moe.experts.{E}.down_proj.lora_A.weight` | `down_lora_a[E]` |
| `layers.{L}.mlp.original_moe.experts.{E}.down_proj.lora_B.weight` | `down_lora_b[E]` |

**实现步骤**:

1. **获取 KT wrappers**: 从 `model._kt_wrappers` 获取所有 MoE 层包装器
2. **加载 adapter 文件**: 使用 `safetensors.safe_open()` 读取权重
3. **解析 key pattern**: 使用正则表达式匹配 MoE LoRA 权重
4. **按层分组**: 将权重按 layer_idx 分组
5. **格式转换**: 将 per-expert 格式转换为 stacked 格式
6. **复制到 wrapper**: 使用 `data.copy_()` 保持张量指针不变
7. **更新 AMX 指针**: 调用 `update_kt_lora_pointers()` 更新 C++ 端指针

### 2.3 loader.py 调试代码

**文件**: `src/llamafactory/model/loader.py`

**新增调试输出**:

```python
# Debug: Print KT and LoRA loading status
if int(os.getenv("LOCAL_RANK", "0")) == 0:
    print("\n" + "=" * 60)
    print("DEBUG: Model structure after loading")
    print("=" * 60)

    # Check KT wrappers
    kt_wrappers = getattr(model, "_kt_wrappers", [])
    print(f"KT wrappers count: {len(kt_wrappers)}")

    # Check MoE LoRA params
    kt_moe_lora = getattr(model, "_kt_moe_lora_params", {})
    print(f"KT MoE LoRA layers: {list(kt_moe_lora.keys())}")

    # Print LoRA-related parameters
    lora_params = [(name, param.shape) for name, param in model.named_parameters()
                   if "lora" in name.lower()]
    print(f"\nLoRA parameters count: {len(lora_params)}")

    # Check PEFT config
    if hasattr(model, "peft_config"):
        print(f"\nPEFT config: {model.peft_config}")
```

---

## 3. 配置文件

### 3.1 推理配置

**文件**: `examples/ktransformers/inference/deepseek2_lora_chat_kt.yaml`

```yaml
### model
model_name_or_path: /mnt/data3/models/DeepSeek-V2-Lite-Chat
adapter_name_or_path: /mnt/data/lpl/ls/saves/Kllama_deepseekV2
template: deepseek
trust_remote_code: true

### inference backend
infer_backend: huggingface

### ktransformers
use_kt: true
kt_backend: AMXBF16
kt_num_threads: 60
kt_tp_enabled: false
```

**关键配置说明**:

| 配置项 | 值 | 说明 |
|--------|---|------|
| `infer_backend` | `huggingface` | 必须使用 HF 后端，不支持 vllm/sglang |
| `use_kt` | `true` | 启用 KTransformers MoE 后端 |
| `kt_backend` | `AMXBF16` | AMX BF16 加速 |
| `kt_num_threads` | `60` | CPU 线程数 |
| `kt_tp_enabled` | `false` | 禁用 Tensor Parallelism |

---

## 4. 关键技术点

### 4.1 为什么不能直接传 target_modules

**问题**: 尝试使用 `PeftModel.from_pretrained(model, adapter, target_modules=attention_targets)` 无效

**原因**: PEFT 的 `from_pretrained()` 方法会忽略传入的 `target_modules` 参数，直接使用保存在 `adapter_config.json` 中的配置

**解决**: 必须修改 `adapter_config.json` 文件本身

### 4.2 为什么使用临时目录 + 符号链接

**目的**: 避免复制大文件（`adapter_model.safetensors` 可能很大）

**方案**:
- `adapter_config.json`: 创建修改后的副本
- 其他文件: 使用 `os.symlink()` 创建符号链接

### 4.3 为什么使用 data.copy_() 而不是赋值

**原因**: KT wrapper 的 `lora_params` 张量指针已经传递给 C++ 端的 AMX 算子，直接赋值会改变指针地址

**正确做法**: 使用 `tensor.data.copy_(new_data)` 保持原张量指针不变

---

## 5. 修改文件清单

| 文件路径 | 修改类型 | 说明 |
|---------|---------|------|
| `src/llamafactory/model/adapter.py` | 修改 | KT 模式下过滤 MoE 模块，使用临时目录加载 |
| `src/llamafactory/model/model_utils/kt_moe.py` | 新增函数 | `load_moe_lora_from_adapter()` |
| `src/llamafactory/model/model_utils/kt_loader.py` | 修复 | Layer 0 Dense MLP 设备检测逻辑 |
| `src/llamafactory/model/loader.py` | 新增调试 | 调试输出代码 |
| `examples/ktransformers/inference/deepseek2_lora_chat_kt.yaml` | 新增 | KT Chat 推理配置 |

## 6. 当前状态

**推理代码已完成，已修复以下问题：**
1. ✅ Bug 1: `merge_and_unload()` 维度不匹配 - 跳过合并
2. ✅ Bug 2: MoE 设备不匹配 - 过滤 MoE 模块
3. ✅ Bug 3: PEFT target_modules 无效 - 临时目录方案
4. ✅ Bug 4: Layer 0 Dense MLP 设备不匹配 - 修复 kt_loader.py 检测逻辑

**待解决（训练问题）：**
- Bug 5: adapter 文件包含 NaN 值 - 需要修复训练流程
