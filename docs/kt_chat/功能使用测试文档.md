# KTransformers Chat 推理功能使用测试文档

## 1. 环境准备

### 1.1 硬件要求

- **CPU**: Intel Sapphire Rapids 或更新（支持 AMX 指令集）
- **GPU**: NVIDIA GPU（CUDA 兼容）
- **内存**: 足够容纳 MoE 权重（DeepSeek-V2-Lite 约 32GB）

### 1.2 软件依赖

```bash
# 核心依赖
pip install torch>=2.0
pip install transformers>=4.40
pip install peft>=0.10
pip install safetensors

# KTransformers 依赖
pip install kt_kernel  # KTransformers AMX 内核
```

### 1.3 目录结构

```
/home/lpl/LLaMA-Factory-KT/
├── examples/ktransformers/inference/
│   └── deepseek2_lora_chat_kt.yaml    # 推理配置
├── src/llamafactory/model/
│   ├── adapter.py                      # LoRA 加载逻辑
│   ├── loader.py                       # 模型加载
│   └── model_utils/kt_moe.py          # KT MoE 工具
└── docs/kt_chat/                       # 本文档目录
```

---

## 2. 配置文件

### 2.1 推理配置文件

**文件**: `examples/ktransformers/inference/deepseek2_lora_chat_kt.yaml`

```yaml
### model
model_name_or_path: /mnt/data3/models/DeepSeek-V2-Lite-Chat
adapter_name_or_path: /mnt/data/lpl/ls/saves/Kllama_deepseekV2
template: deepseek
trust_remote_code: true

### inference backend
infer_backend: huggingface

### ktransformers
use_kt: true
kt_backend: AMXBF16
kt_num_threads: 60
kt_tp_enabled: false
```

### 2.2 配置参数说明

| 参数 | 类型 | 说明 |
|------|------|------|
| `model_name_or_path` | string | 基础模型路径 |
| `adapter_name_or_path` | string | LoRA adapter 路径 |
| `template` | string | 对话模板名称 |
| `trust_remote_code` | bool | 信任远程代码 |
| `infer_backend` | string | 推理后端（必须为 `huggingface`） |
| `use_kt` | bool | 启用 KTransformers |
| `kt_backend` | string | KT 后端类型 (`AMXBF16`, `AMXInt8`) |
| `kt_num_threads` | int | CPU 线程数 |
| `kt_tp_enabled` | bool | 是否启用 Tensor Parallelism |

---

## 3. 使用方法

### 3.1 基本命令

```bash
CUDA_VISIBLE_DEVICES=0 llamafactory-cli chat \
    examples/ktransformers/inference/deepseek2_lora_chat_kt.yaml
```

### 3.2 环境变量

| 变量 | 说明 | 示例 |
|------|------|------|
| `CUDA_VISIBLE_DEVICES` | 指定 GPU 设备 | `0` |
| `OMP_NUM_THREADS` | OpenMP 线程数（可选） | `60` |
| `KMP_AFFINITY` | CPU 亲和性（可选） | `granularity=fine,compact,1,0` |

### 3.3 完整示例

```bash
# 设置环境变量
export CUDA_VISIBLE_DEVICES=0
export OMP_NUM_THREADS=60

# 启动 Chat
llamafactory-cli chat examples/ktransformers/inference/deepseek2_lora_chat_kt.yaml
```

---

## 4. 测试用例

### 4.1 基础功能测试

**测试目标**: 验证 Chat 推理基本功能

**测试步骤**:
1. 启动 Chat 命令
2. 输入测试问题
3. 检查输出是否正常

**测试输入**:
```
你好，请介绍一下你自己。
```

**预期行为**:
- 模型正常加载（无报错）
- 生成合理的回复
- 输出包含中文

### 4.2 LoRA 效果验证

**测试目标**: 验证 LoRA adapter 是否生效

**测试方法**: 对比有/无 adapter 的输出差异

**配置 A（无 adapter）**:
```yaml
model_name_or_path: /mnt/data3/models/DeepSeek-V2-Lite-Chat
# adapter_name_or_path: 注释掉
template: deepseek
use_kt: true
```

**配置 B（有 adapter）**:
```yaml
model_name_or_path: /mnt/data3/models/DeepSeek-V2-Lite-Chat
adapter_name_or_path: /mnt/data/lpl/ls/saves/Kllama_deepseekV2
template: deepseek
use_kt: true
```

**验证**: 两种配置的输出应该有明显差异

### 4.3 MoE LoRA 加载验证

**测试目标**: 验证 MoE LoRA 是否正确加载到 KT wrapper

**验证方法**: 检查 DEBUG 输出

**预期输出**:
```
============================================================
DEBUG: Model structure after loading
============================================================
KT wrappers count: 26
KT MoE LoRA layers: []

LoRA parameters count: XXX
LoRA parameters:
  base_model.model.model.layers.0.self_attn.q_proj.lora_A.default.weight: torch.Size([8, 2048])
  base_model.model.model.layers.0.self_attn.q_proj.lora_B.default.weight: torch.Size([1536, 8])
  ...

PEFT config: {'default': LoraConfig(..., target_modules={'kv_a_proj_with_mqa', 'o_proj', 'q_proj', 'kv_b_proj'})}
============================================================
```

**关键检查点**:
1. `KT wrappers count` > 0：KT wrapper 已创建
2. PEFT `target_modules` 不包含 `gate_proj`, `up_proj`, `down_proj`：MoE 模块已过滤
3. LoRA parameters 只包含 Attention 层

---

## 5. 常见问题

### 5.1 报错: Expected all tensors to be on the same device

**原因**: PEFT 尝试包装 MoE 层，但 MoE 权重在 CPU

**解决**: 检查是否正确使用了修改后的代码版本

### 5.2 报错: kt_kernel not found

**原因**: 未安装 KTransformers AMX 内核

**解决**:
```bash
pip install kt_kernel
```

### 5.3 Chat 无输出

**可能原因**:
1. 模型加载失败
2. Tokenizer 配置错误
3. Template 不匹配

**排查步骤**:
1. 检查日志中是否有错误
2. 尝试不使用 adapter 运行
3. 验证 template 名称是否正确

### 5.4 MoE LoRA 未生效

**可能原因**: `load_moe_lora_from_adapter()` 未正确执行

**排查步骤**:
1. 检查 DEBUG 输出中的 KT wrappers count
2. 检查日志中是否有 "Loaded MoE LoRA" 信息
3. 验证 adapter 文件中是否包含 MoE LoRA 权重

---

## 6. 性能基准

### 6.1 测试环境

| 项目 | 配置 |
|------|------|
| CPU | Intel Xeon 8480+ (56 cores) |
| GPU | NVIDIA A100 80GB |
| 内存 | 512GB DDR5 |
| 模型 | DeepSeek-V2-Lite-Chat |

### 6.2 预期性能

| 指标 | 值 |
|------|---|
| 首 token 延迟 | < 1s |
| 生成速度 | > 20 tokens/s |
| GPU 显存 | < 16GB (只有 Attention) |
| CPU 内存 | ~32GB (MoE 权重) |

---

## 7. 日志示例

### 7.1 正常加载日志

```
[INFO] Loading model with KTransformers MoE backend
[INFO] KT mode: Loading only Attention LoRA. Original targets: ['q_proj', 'kv_a_proj_with_mqa', 'kv_b_proj', 'o_proj', 'gate_proj', 'up_proj', 'down_proj'], Filtered: ['q_proj', 'kv_a_proj_with_mqa', 'kv_b_proj', 'o_proj']
[INFO] Loading MoE LoRA from /mnt/data/lpl/ls/saves/Kllama_deepseekV2/adapter_model.safetensors
[INFO] Loaded MoE LoRA into 26 KT wrappers from /mnt/data/lpl/ls/saves/Kllama_deepseekV2
[INFO] Loaded adapter(s): /mnt/data/lpl/ls/saves/Kllama_deepseekV2
[INFO] trainable params: 0 || all params: XXX || trainable%: 0.0000
```

### 7.2 错误日志示例

```
# 错误: MoE 设备不匹配
RuntimeError: Expected all tensors to be on the same device, but found at least two devices, cuda:0 and cpu! (when checking argument for argument mat2 in method wrapper_CUDA_mm)

# 解决: 确保使用修改后的 adapter.py
```
