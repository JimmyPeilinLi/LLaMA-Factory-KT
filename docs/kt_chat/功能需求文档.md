# KTransformers Chat 推理功能需求文档

## 1. 项目背景

### 1.1 项目概述

本项目旨在为 LLaMA-Factory-KT 实现 KTransformers MoE 训练后的 Chat 推理功能。使用 KT 训练的 LoRA adapter（包含 MoE + Attention 两部分）需要在推理时正确加载和使用。

### 1.2 需求来源

在使用 KTransformers AMX 后端训练 DeepSeek-V2 等 MoE 模型后，用户希望通过 `llamafactory-cli chat` 进行交互式推理。由于 KT 训练时 MoE 层和 Attention 层的 LoRA 分别由不同系统管理，推理时需要正确处理这种双通道架构。

### 1.3 核心问题

使用 KT 训练的 LoRA adapter 在推理时无法正常工作：

1. **训练配置**：`lora_target: all`，PEFT 为所有模块（包括 MoE）创建了 LoRA
2. **KT 加载后**：MoE 层的原始权重被移到 CPU，GPU 上只有空壳
3. **PEFT 加载 adapter**：尝试包装 MoE 层，但 base_layer 权重是空的
4. **推理报错**：`Expected all tensors to be on the same device`

---

## 2. 功能需求

### 2.1 核心功能

| 功能编号 | 功能名称 | 描述 | 优先级 |
|---------|---------|------|--------|
| F-001 | KT Chat 推理 | 支持 KT 模式下的 HuggingFace 推理后端 | P0 |
| F-002 | Attention LoRA 加载 | 由 PEFT 正常加载 Attention 层的 LoRA | P0 |
| F-003 | MoE LoRA 加载 | 从 adapter 提取 MoE LoRA，加载到 KT wrapper | P0 |
| F-004 | 格式转换 | PEFT 格式到 KT 格式的自动转换 | P0 |

### 2.2 双通道 LoRA 管理

在 KT 推理模式下：

| LoRA 类型 | 管理方 | 存储位置 | 加载方式 |
|----------|--------|---------|---------|
| Attention LoRA | PEFT | GPU | `PeftModel.from_pretrained()` |
| MoE LoRA | KT Wrapper | CPU | `load_moe_lora_from_adapter()` |

### 2.3 支持的模块

**Attention LoRA (PEFT 管理)**:
- `q_proj` - Query 投影
- `k_proj` / `kv_a_proj_with_mqa` / `kv_b_proj` - Key/Value 投影
- `o_proj` - Output 投影

**MoE LoRA (KT 管理)**:
- `gate_proj` - Gate 投影 (每个 expert)
- `up_proj` - Up 投影 (每个 expert)
- `down_proj` - Down 投影 (每个 expert)

---

## 3. 非功能需求

### 3.1 兼容性要求

- 与 LLaMA-Factory 现有 chat 流程兼容
- 与 HuggingFace transformers 模型加载兼容
- 与 peft 库的 LoRA 实现兼容
- 与现有 KT 训练配置兼容

### 3.2 性能要求

| 指标 | 要求 |
|------|------|
| 加载时间 | 与非 KT 模式持平 |
| 推理速度 | MoE 层使用 AMX 加速 |
| 内存使用 | MoE 权重在 CPU，Attention 在 GPU |

---

## 4. 约束条件

### 4.1 硬件约束

- 需要支持 Intel AMX 指令集的 CPU（Sapphire Rapids 或更新）
- GPU 用于 Attention 层计算（CUDA 兼容）
- 足够的系统内存容纳 MoE 权重

### 4.2 软件约束

- Python 3.10+
- PyTorch 2.0+
- kt_kernel 库（KTransformers AMX 内核）
- transformers 4.40+
- peft 0.10+

### 4.3 配置约束

必须使用 HuggingFace 推理后端：
```yaml
infer_backend: huggingface  # 不支持 vllm/sglang
use_kt: true
```

---

## 5. 验收标准

### 5.1 功能验收

- [ ] Attention LoRA 正常加载（使用 PEFT）
- [ ] MoE LoRA 正常加载（使用 KT wrapper）
- [ ] Chat 推理输出正确
- [ ] 无设备不匹配错误

### 5.2 兼容性验收

- [ ] 支持 DeepSeek V2/V2-Lite 模型
- [ ] 支持 `lora_target: all` 训练的 adapter
- [ ] 与 LLaMA-Factory chat 命令集成

---

## 6. 参考资料

- KTransformers kt-kernel 仓库：`~/ktransformers/kt-kernel`
- LLaMA-Factory-KT refactor 文档：`docs/refactor/`
- PEFT 官方文档：https://huggingface.co/docs/peft
