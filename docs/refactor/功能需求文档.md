# KTransformers MoE 集成功能需求文档

## 1. 项目背景

### 1.1 项目概述

本项目旨在将 KTransformers 新的 SFT MOE AMX 接口集成到 LLaMA-Factory-KT，替换现有的 KTransformers 集成代码。新接口专注于 MoE (Mixture of Experts) 层的高性能 CPU 计算，基于 Intel AMX (Advanced Matrix Extensions) 指令集加速。

### 1.2 需求来源

在 DeepSeek-V3 等大规模 MoE 模型的微调场景中，GPU 显存无法容纳完整的 MoE 专家权重。KTransformers 提供了将 MoE 层卸载到 CPU 并使用 AMX 加速的解决方案。新版本接口相比旧版本有以下核心改进：

1. **不管 Attention 部分** - Attention 使用 HuggingFace/LLaMA-Factory 原生实现（GPU）
2. **不使用 kt_optimize_rule** - 无需 YAML 优化规则文件，配置更简单
3. **零拷贝设计** - 通过 tensor.data_ptr() 传递数据，减少内存拷贝开销
4. **LoRA 在 C++ 算子中处理** - MoE 层的 LoRA 计算由 AMX 算子完成

### 1.3 目标

1. 完全替换现有的 KTransformers 集成代码（use_kt 相关逻辑）
2. 实现 KT-MoE 作为 LLaMA-Factory 处理 MoE 的高性能后端
3. 支持 MoE 层和 Attention 层的 LoRA 微调
4. 先完成 SFT 阶段支持，后续扩展到 DPO

---

## 2. 功能需求

### 2.1 核心功能

| 功能编号 | 功能名称 | 描述 | 优先级 |
|---------|---------|------|--------|
| F-001 | MoE 层 AMX 加速 | 将 MoE 层的前向/反向传播卸载到 CPU 使用 AMX 加速 | P0 |
| F-002 | MoE LoRA 微调 | 在 MoE 的 gate/up/down 投影矩阵上应用 LoRA 适配器 | P0 |
| F-003 | Attention LoRA 微调 | 在 Attention 层（q/k/v/o_proj）上应用 peft LoRA | P0 |
| F-004 | SFT 训练支持 | 支持 Supervised Fine-Tuning 训练阶段 | P0 |
| F-005 | DPO 训练支持 | 支持 Direct Preference Optimization 训练阶段 | P1 |
| F-006 | 多模型架构支持 | 支持 DeepSeek V2/V3、Qwen MoE、Mixtral 等架构 | P0 |

### 2.2 MoE AMX 算子功能

| 功能编号 | 功能名称 | 描述 | 优先级 |
|---------|---------|------|--------|
| F-101 | LoRA 前向传播 | 在 gate/up/down 投影矩阵上应用 LoRA 适配器前向计算 | P0 |
| F-102 | LoRA 反向传播 | 计算 LoRA A/B 矩阵的梯度 | P0 |
| F-103 | 输入梯度计算 | 计算输入张量的梯度用于链式求导 | P0 |
| F-104 | 零拷贝权重访问 | C++ 直接访问 Python tensor 内存 | P0 |
| F-105 | 梯度检查点支持 | 支持多次 forward 后统一 backward | P1 |
| F-106 | TP 模式支持 | 支持 Tensor Parallelism 跨 NUMA 节点 | P1 |

### 2.3 配置简化

**旧配置方式（需废弃）：**
```yaml
use_kt: true
kt_optimize_rule: examples/kt_optimize_rules/DeepSeek-V3-Chat-sft-amx.yaml
cpu_infer: 32
chunk_size: 8192
```

**新配置方式：**
```yaml
use_kt: true
kt_backend: AMXBF16  # 或 AMXInt8
kt_num_threads: 60
kt_tp_enabled: true
kt_num_gpu_experts: 0  # GPU 上运行的专家数量 (0=全部在CPU)
# kt_weight_path: null  # 预处理权重路径 (可选，用于 INT4/INT8)
```

---

## 3. 非功能需求

### 3.1 性能要求

| 指标 | 要求 | 说明 |
|------|------|------|
| 前向吞吐 | 与推理持平 | LoRA 额外开销 < 15% |
| 反向时延 | < 2x 前向 | 合理的训练开销 |
| 内存效率 | 共享缓冲区 | 复用 NUMA 内存池 |
| 显存节省 | 显著 | MoE 权重在 CPU，只有 Attention 在 GPU |

### 3.2 精度要求

| 模式 | 前向误差阈值 | 反向误差阈值 |
|------|-------------|-------------|
| BF16 | < 0.05 | < 0.10 |
| INT8 | < 0.15 | < 0.25 |

**误差计算方式**:
```python
relative_diff = mean(abs(output - reference)) / mean(abs(reference))
```

### 3.3 兼容性要求

- 与 LLaMA-Factory 现有训练流程兼容
- 与 HuggingFace transformers 模型加载兼容
- 与 peft 库的 LoRA 实现兼容（用于 Attention 层）
- 与 PyTorch 优化器无缝集成

### 3.4 可维护性要求

- 代码结构清晰，符合 LLaMA-Factory 代码规范
- 提供完整的文档说明
- 提供测试用例

---

## 4. 支持的模型架构

| 模型系列 | 模型名称 | MoE 层类名 |
|---------|---------|----------|
| DeepSeek | DeepSeek-V2, DeepSeek-V2-Lite | DeepseekV2MoE |
| DeepSeek | DeepSeek-V3 | DeepseekV3MoE |
| Qwen | Qwen2-MoE | Qwen2MoeSparseMoeBlock |
| Qwen | Qwen3-MoE | Qwen3MoeSparseMoeBlock |
| Mistral | Mixtral | MixtralSparseMoeBlock |

---

## 5. 约束条件

### 5.1 硬件约束

- 需要支持 Intel AMX 指令集的 CPU（Sapphire Rapids 或更新）
- GPU 用于 Attention 层计算（CUDA 兼容）
- 足够的系统内存容纳 MoE 权重

### 5.2 软件约束

- Python 3.10+
- PyTorch 2.0+
- kt_kernel 库（KTransformers AMX 内核）
- transformers 4.40+

### 5.3 接口约束

- 使用 CPUInfer 异步任务模型
- LoRA 权重通过指针传递（零拷贝）
- 梯度缓冲区由 Python 端分配

---

## 6. 验收标准

### 6.1 功能验收

- [ ] MoE 层前向传播输出与 PyTorch 参考实现一致（BF16 误差 < 5%）
- [ ] MoE 层反向传播梯度与 PyTorch 参考实现一致（BF16 误差 < 10%）
- [ ] 支持完整 SFT 训练循环（forward → backward → optimizer.step）
- [ ] Attention LoRA 正常工作（使用 peft）
- [ ] 配置简化：无需 kt_optimize_rule YAML 文件

### 6.2 性能验收

- [ ] 前向吞吐保持与推理持平
- [ ] 无内存泄漏（长时训练稳定）
- [ ] NUMA 亲和性保持

### 6.3 兼容性验收

- [ ] 支持 DeepSeek V2/V3 模型
- [ ] 支持 Qwen MoE 模型
- [ ] 与 LLaMA-Factory 训练流程集成

---

## 7. 参考资料

- KTransformers kt-kernel 仓库：`/home/lpl/ktransformers/kt-kernel`
- KT 算子接口文档：`/home/lpl/ktransformers/kt-kernel/docs/sft_moe_amx/算子接口文档.md`
- KT 测试文件：`/home/lpl/ktransformers/kt-kernel/examples/test_moe_sft_amx.py`
- SGLang + KTransformers 集成参考
