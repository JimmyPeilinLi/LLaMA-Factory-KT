# KTransformers MoE 集成功能上下接口文档

## 1. 上游接口（调用方）

### 1.1 模型加载接口

**调用位置**: `src/llamafactory/model/loader.py`

```python
def load_model(
    tokenizer: "PreTrainedTokenizer",
    model_args: "ModelArguments",
    finetuning_args: "FinetuningArguments",
    is_trainable: bool = False,
    add_valuehead: bool = False,
) -> "PreTrainedModel":
    """
    加载模型，如果 use_kt=True 则使用 KT AMX 后端

    调用 KT AMX 的方式：
        if model_args.use_kt:
            from .model_utils.kt_moe import load_kt_model
            model = load_kt_model(config, model_args, finetuning_args)
    """
```

### 1.2 Adapter 初始化接口

**调用位置**: `src/llamafactory/model/adapter.py`

```python
def init_adapter(
    config: "PretrainedConfig",
    model: "PreTrainedModel",
    model_args: "ModelArguments",
    finetuning_args: "FinetuningArguments",
    is_trainable: bool,
) -> "PreTrainedModel":
    """
    初始化 LoRA 适配器

    KT AMX 模式下的处理：
        if model_args.use_kt:
            # MoE LoRA 已由 kt_moe 处理
            # 只对 Attention 层应用 peft LoRA
            target_modules = filter_attention_modules(finetuning_args.lora_target)
            ...
    """
```

### 1.3 Trainer 工作流接口

**调用位置**: `src/llamafactory/train/sft/workflow.py`

```python
def run_sft(
    model_args: "ModelArguments",
    data_args: "DataArguments",
    training_args: "Seq2SeqTrainingArguments",
    finetuning_args: "FinetuningArguments",
    generating_args: "GeneratingArguments",
    callbacks: Optional[list["TrainerCallback"]] = None,
):
    """
    运行 SFT 训练

    KT AMX 模式下的 Trainer 选择：
        if model_args.use_kt:
            from .kt_amx_trainer import KTrainer
            trainer = KTrainer(...)
        else:
            trainer = CustomSeq2SeqTrainer(...)
    """
```

---

## 2. 核心接口（kt_moe.py）

### 2.1 模型加载

```python
def load_kt_model(
    config: "PretrainedConfig",
    model_args: "ModelArguments",
    finetuning_args: "FinetuningArguments",
) -> "PreTrainedModel":
    """
    使用 KT AMX 后端加载模型

    Args:
        config: HuggingFace 模型配置
        model_args: 模型参数（包含 kt_amx_* 配置）
        finetuning_args: 微调参数（包含 lora_rank, lora_alpha 等）

    Returns:
        model: 已包装 MoE 层的模型
            - model._kt_wrappers: list[MOELayerWrapper]
            - model._kt_cpu_infer: CPUInfer
            - model._kt_moe_lora_params: dict[int, dict[str, nn.Parameter]]

    Raises:
        ImportError: kt_kernel 未安装
        ValueError: 模型架构不支持
        RuntimeError: AMX 硬件不支持
    """
```

### 2.2 CPUInfer 初始化

```python
def init_cpu_infer(model_args: "ModelArguments") -> "CPUInfer":
    """
    初始化 KT CPUInfer 实例

    Args:
        model_args: 模型参数
            - kt_num_threads: CPU 线程数
            - kt_tp_enabled: 是否启用 TP

    Returns:
        cpu_infer: CPUInfer 实例
    """
```

### 2.3 MoE 层包装

```python
def wrap_moe_layers_with_amx(
    model: "PreTrainedModel",
    model_args: "ModelArguments",
    finetuning_args: "FinetuningArguments",
    cpu_infer: "CPUInfer",
) -> list["MOELayerWrapper"]:
    """
    将模型的 MoE 层替换为 AMX 包装

    Args:
        model: HuggingFace 模型
        model_args: 模型参数
        finetuning_args: 微调参数
        cpu_infer: CPUInfer 实例

    Returns:
        wrappers: MOELayerWrapper 列表

    Side Effects:
        - 修改 model.model.layers[i].mlp 为 MOELayerWrapper
        - MoE 权重移动到 CPU
    """
```

### 2.4 MOELayerWrapper 类 【2026-01-05 更新】

```python
class MOELayerWrapper(nn.Module):
    """
    MoE 层的 AMX 包装器

    Attributes:
        original_moe: 原始 MoE 层（保留用于访问 router）
        moe_amx: AMXBF16_SFT_MOE 或 AMXInt8_SFT_MOE 实例
        cpu_infer: CPUInfer 实例
        moe_config: MOEArchConfig - MoE 架构配置
        hidden_size: int - 隐藏层维度
        layer_idx: int - 层索引
        router_type: str - Router 类型 ("linear" 或 "deepseek_gate") 【新增】
        lora_params: nn.ParameterDict - LoRA 参数
            - gate_lora_a: [expert_num, lora_rank, hidden_size]
            - gate_lora_b: [expert_num, intermediate_size, lora_rank]
            - up_lora_a, up_lora_b, down_lora_a, down_lora_b: 类似
        router: 路由器模块（从 original_moe 获取）
        shared_experts: 共享专家模块（如果存在）

    Methods:
        forward(hidden_states) -> Tensor
        update_lora_pointers() -> None
    """

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """
        前向传播 【2026-01-05 更新】

        Args:
            hidden_states: [batch, seq_len, hidden_size] (GPU)

        Returns:
            output: [batch, seq_len, hidden_size] (GPU)

        实现逻辑:
            # 1. 根据 router_type 获取 topk_ids 和 topk_weights
            if self.router_type == "deepseek_gate":
                # DeepSeek: 3D input, returns (topk_idx, topk_weight, aux_loss)
                topk_ids, topk_weights, aux_loss = self.router(hidden_states)
            else:
                # Qwen/Mixtral: 2D input, returns raw logits
                router_logits = self.router(hidden_states.view(-1, hidden_size))
                routing_weights = F.softmax(router_logits, dim=-1)
                topk_weights, topk_ids = torch.topk(routing_weights, k, dim=-1)
                topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)

            # 2. 调用统一的 MOEAMXFunction
            moe_output = MOEAMXFunction.apply(
                hidden_states, topk_ids, topk_weights, ...
            )

            # 3. 处理 shared_experts (如果存在)
            if self.shared_experts is not None:
                moe_output += self.shared_experts(hidden_states)

            return moe_output
        """

    def update_lora_pointers(self) -> None:
        """
        更新 AMX 算子的 LoRA 权重指针

        Notes:
            - TP 模式下每次 optimizer.step() 后必须调用
            - no-TP 模式下使用零拷贝，无需调用
        """
```

### 2.5 MOEArchConfig 配置类 【2026-01-05 更新】

```python
@dataclass
class MOEArchConfig:
    """
    MoE 架构配置，支持多种模型架构

    Attributes:
        moe_layer_attr: str  # MoE 层属性名，如 "mlp"
        router_attr: str  # Router 属性名，如 "gate"
        experts_attr: str  # Experts 属性名，如 "experts"
        weight_names: tuple[str, str, str]  # (gate_proj, up_proj, down_proj) 名称
        expert_num: int  # 专家总数
        intermediate_size: int  # MLP 中间层维度
        num_experts_per_tok: int  # 每 token 激活专家数 (top-k)
        has_shared_experts: bool = False  # 是否有共享专家
        router_type: str = "linear"  # Router 类型

    Router 类型说明:
        - "linear": Qwen/Mixtral 的 nn.Linear router
            - 输入: 2D [qlen, hidden_size]
            - 输出: Raw logits [qlen, num_experts]
            - 需要在 MOELayerWrapper 中手动做 softmax + topk

        - "deepseek_gate": DeepSeek 的 MoEGate router
            - 输入: 3D [batch, seq_len, hidden_size]
            - 输出: tuple(topk_idx, topk_weight, aux_loss)
            - 已包含 softmax + topk 处理
    """
```

### 2.6 MOEAMXFunction 类 【2026-01-05 更新】

```python
class MOEAMXFunction(torch.autograd.Function):
    """
    自定义 autograd 函数，桥接 PyTorch 和 AMX

    Static Methods:
        forward(ctx, hidden_states, topk_ids, topk_weights, moe_amx, cpu_infer,
                lora_params, moe_config, hidden_size, num_experts_per_tok) -> Tensor
        backward(ctx, grad_output) -> tuple[Tensor, None, ...]
    """

    @staticmethod
    def forward(
        ctx,
        hidden_states: torch.Tensor,
        topk_ids: torch.Tensor,           # 【新增】已处理的 expert indices
        topk_weights: torch.Tensor,       # 【新增】已处理的 routing weights
        moe_amx: "AMXBF16_SFT_MOE",
        cpu_infer: "CPUInfer",
        lora_params: dict[str, nn.Parameter],
        moe_config: "MOEArchConfig",
        hidden_size: int,
        num_experts_per_tok: int,
    ) -> torch.Tensor:
        """
        前向传播

        Args:
            ctx: autograd 上下文
            hidden_states: [batch, seq_len, hidden_size] (GPU)
            topk_ids: [num_tokens, num_experts_per_tok] - 已选择的专家索引
            topk_weights: [num_tokens, num_experts_per_tok] - 已归一化的路由权重
            moe_amx: AMX MOE 实例
            cpu_infer: CPUInfer 实例
            lora_params: LoRA 参数字典
            moe_config: MoE 架构配置
            hidden_size: 隐藏层维度
            num_experts_per_tok: 每 token 激活专家数

        Returns:
            output: [batch, seq_len, hidden_size] (GPU)

        Notes:
            - topk_ids 和 topk_weights 由 MOELayerWrapper.forward() 统一处理
            - 不同 router 类型的差异在 MOELayerWrapper 层处理
        """

    @staticmethod
    def backward(ctx, grad_output: torch.Tensor) -> tuple:
        """
        反向传播

        Args:
            ctx: autograd 上下文
            grad_output: [batch, seq_len, hidden_size] (GPU)

        Returns:
            (grad_input, None, None, None, None, None, None, None, None)  # 【9个返回值】
            - grad_input: [batch, seq_len, hidden_size] (GPU)
            - 其他 8 个 None 对应 forward 的非 Tensor 参数

        Side Effects:
            - 累积 LoRA 梯度到 lora_params[*].grad
        """
```

---

## 3. 下游接口（被调用方）

### 3.1 KT Kernel 接口

**来源**: `kt_kernel.kt_kernel_ext`

#### 3.1.1 CPUInfer

```python
class CPUInfer:
    """CPU 推理管理器"""

    def __init__(self, num_threads: int):
        """TP 模式初始化"""

    def __init__(self, config: WorkerPoolConfig):
        """自定义配置初始化"""

    @property
    def backend_(self) -> WorkerPool:
        """获取后端线程池"""

    def submit(self, task: Task) -> None:
        """提交任务"""

    def sync(self) -> None:
        """等待所有任务完成"""
```

#### 3.1.2 MOESFTConfig

```python
class MOESFTConfig:
    """MoE SFT 配置"""

    # 模型配置
    expert_num: int  # 专家总数
    num_experts_per_tok: int  # 每 token 激活专家数
    hidden_size: int  # 隐藏层维度
    intermediate_size: int  # MLP 中间层维度

    # LoRA 配置
    lora_rank: int  # LoRA 秩
    lora_alpha: float  # LoRA 缩放因子

    # 缓存配置
    max_cache_depth: int  # 梯度检查点缓存深度
    max_len: int  # 最大序列长度
    layer_idx: int  # 层索引

    # 基础权重指针
    gate_proj: int  # data_ptr()
    up_proj: int
    down_proj: int

    # LoRA 权重指针
    gate_lora_a: int
    gate_lora_b: int
    up_lora_a: int
    up_lora_b: int
    down_lora_a: int
    down_lora_b: int

    # 线程池
    pool: WorkerPool
```

#### 3.1.3 AMXBF16_SFT_MOE

```python
class AMXBF16_SFT_MOE:
    """BF16 模式的 AMX MoE 实例"""

    def __init__(self, config: MOESFTConfig):
        """初始化"""

    def load_weights_task(self) -> Task:
        """加载基础权重任务"""

    def warm_up_task(self) -> Task:
        """预热任务"""

    def forward_sft_task(
        self,
        bsz_ptr: int,  # 批大小张量指针
        num_experts_per_tok: int,
        expert_ids_ptr: int,  # [qlen, k] int64
        weights_ptr: int,  # [qlen, k] float32
        input_ptr: int,  # [qlen, hidden_size] bf16
        output_ptr: int,  # [qlen, hidden_size] float32
        save_for_backward: bool,
    ) -> Task:
        """前向传播任务"""

    def backward_task(
        self,
        grad_output_ptr: int,  # [qlen, hidden_size] bf16
        grad_input_ptr: int,  # [qlen, hidden_size] bf16
        grad_gate_lora_a_ptr: int,
        grad_gate_lora_b_ptr: int,
        grad_up_lora_a_ptr: int,
        grad_up_lora_b_ptr: int,
        grad_down_lora_a_ptr: int,
        grad_down_lora_b_ptr: int,
    ) -> Task:
        """反向传播任务"""

    def update_lora_weights_task(
        self,
        gate_lora_a_ptr: int,
        gate_lora_b_ptr: int,
        up_lora_a_ptr: int,
        up_lora_b_ptr: int,
        down_lora_a_ptr: int,
        down_lora_b_ptr: int,
    ) -> Task:
        """更新 LoRA 权重指针任务 (TP 模式需要)"""
```

---

## 4. 配置参数接口

### 4.1 ModelArguments 新增参数

**位置**: `src/llamafactory/hparams/model_args.py`

```python
@dataclass
class ModelArguments:
    # ... 现有参数 ...

    # KT AMX 配置
    use_kt: bool = field(
        default=False,
        metadata={
            "help": "Whether to use KT AMX MOE backend for MoE layers."
        },
    )

    kt_backend: Literal["AMXBF16", "AMXInt8"] = field(
        default="AMXBF16",
        metadata={
            "help": "AMX backend type: AMXBF16 for BF16 mode, AMXInt8 for INT8 quantization."
        },
    )

    kt_num_threads: int = field(
        default=60,
        metadata={
            "help": "Number of CPU threads for AMX inference."
        },
    )

    kt_tp_enabled: bool = field(
        default=True,
        metadata={
            "help": "Enable Tensor Parallelism across NUMA nodes."
        },
    )

    kt_num_gpu_experts: int = field(
        default=0,
        metadata={
            "help": "Number of MoE experts to run on GPU. "
                    "0 = all experts on CPU, >0 = hybrid GPU/CPU mode (Phase 2)."
        },
    )

    kt_weight_path: Optional[str] = field(
        default=None,
        metadata={
            "help": "Path to preprocessed CPU expert weights. "
                    "Used for loading INT4/INT8 quantized weights (Phase 2)."
        },
    )

    kt_max_cache_depth: int = field(
        default=1,
        metadata={
            "help": "Maximum cache depth for gradient checkpointing. "
                    "Set to gradient_accumulation_steps for best memory efficiency."
        },
    )
```

---

## 5. 权重格式

### 5.1 基础权重（冻结）

```python
# 形状定义
gate_proj: Tensor  # [expert_num, intermediate_size, hidden_size], bf16
up_proj: Tensor    # [expert_num, intermediate_size, hidden_size], bf16
down_proj: Tensor  # [expert_num, hidden_size, intermediate_size], bf16

# 内存要求
# 必须是 contiguous 的 CPU tensor
gate_proj = gate_proj.cpu().contiguous()
```

### 5.2 LoRA 权重（可训练）

```python
# Gate 投影 LoRA
gate_lora_a: nn.Parameter  # [expert_num, lora_rank, hidden_size], bf16
gate_lora_b: nn.Parameter  # [expert_num, intermediate_size, lora_rank], bf16

# Up 投影 LoRA
up_lora_a: nn.Parameter    # [expert_num, lora_rank, hidden_size], bf16
up_lora_b: nn.Parameter    # [expert_num, intermediate_size, lora_rank], bf16

# Down 投影 LoRA
down_lora_a: nn.Parameter  # [expert_num, lora_rank, intermediate_size], bf16
down_lora_b: nn.Parameter  # [expert_num, hidden_size, lora_rank], bf16

# 初始化
# A 矩阵：kaiming_uniform
# B 矩阵：zeros
nn.init.kaiming_uniform_(gate_lora_a, a=math.sqrt(5))
nn.init.zeros_(gate_lora_b)
```

### 5.3 输入/输出格式

```python
# 输入
hidden_states: Tensor  # [batch, seq_len, hidden_size], fp16/bf16, GPU

# 路由信息
expert_ids: Tensor     # [batch * seq_len, num_experts_per_tok], int64, CPU
weights: Tensor        # [batch * seq_len, num_experts_per_tok], float32, CPU

# 输出
output: Tensor         # [batch, seq_len, hidden_size], fp32 (AMX) → fp16/bf16 (GPU)

# 梯度
grad_output: Tensor    # [batch, seq_len, hidden_size], bf16, CPU
grad_input: Tensor     # [batch, seq_len, hidden_size], bf16, CPU
grad_lora_*: Tensor    # 与对应权重相同形状
```

---

## 6. 错误码和异常

### 6.1 异常类型

```python
class KTAMXError(Exception):
    """KT AMX 基础异常"""
    pass

class KTAMXNotAvailableError(KTAMXError):
    """kt_kernel 未安装或 AMX 不支持"""
    pass

class KTAMXModelNotSupportedError(KTAMXError):
    """模型架构不支持"""
    pass

class KTAMXConfigError(KTAMXError):
    """配置错误"""
    pass

class KTAMXForwardError(KTAMXError):
    """前向传播错误"""
    pass

class KTAMXBackwardError(KTAMXError):
    """反向传播错误"""
    pass
```

### 6.2 错误处理示例

```python
try:
    model = load_kt_model(config, model_args, finetuning_args)
except KTAMXNotAvailableError as e:
    logger.error(f"KT AMX not available: {e}")
    logger.info("Falling back to standard model loading")
    model = AutoModelForCausalLM.from_pretrained(...)
except KTAMXModelNotSupportedError as e:
    logger.error(f"Model not supported: {e}")
    raise
```

---

## 7. 使用示例

### 7.1 配置文件

```yaml
# train_kt_amx_sft.yaml
model_name_or_path: deepseek-ai/DeepSeek-V2-Lite-Chat
stage: sft
finetuning_type: lora

# KT AMX 配置
use_kt: true
kt_backend: AMXBF16
kt_num_threads: 60
kt_tp_enabled: true
kt_num_gpu_experts: 0  # GPU 上运行的专家数量 (0=全部在CPU)
# kt_weight_path: null  # 预处理权重路径 (可选，用于 INT4/INT8)

# LoRA 配置（同时应用于 MoE 和 Attention）
lora_rank: 16
lora_alpha: 32
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

# 训练配置
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 1
output_dir: outputs/deepseek_v2_sft_kt_amx
```

### 7.2 命令行启动

```bash
llamafactory-cli train examples/train_kt_amx_sft.yaml
```

### 7.3 代码调用

```python
from llamafactory.model import load_model, load_tokenizer
from llamafactory.hparams import get_train_args

# 解析参数
model_args, data_args, training_args, finetuning_args, generating_args = (
    get_train_args()
)

# 加载模型（自动使用 KT AMX）
tokenizer = load_tokenizer(model_args)["tokenizer"]
model = load_model(tokenizer, model_args, finetuning_args, is_trainable=True)

# 访问 KT AMX 组件
if hasattr(model, "_kt_wrappers"):
    print(f"Loaded {len(model._kt_wrappers)} MoE layers with KT AMX")
    for wrapper in model._kt_wrappers:
        print(f"  Layer: {wrapper.lora_params.keys()}")
```
