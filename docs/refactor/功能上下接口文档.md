# KTransformers MoE 集成功能上下接口文档

## 1. 上游接口（调用方）

### 1.1 模型加载接口

**调用位置**: `src/llamafactory/model/loader.py`

```python
def load_model(
    tokenizer: "PreTrainedTokenizer",
    model_args: "ModelArguments",
    finetuning_args: "FinetuningArguments",
    is_trainable: bool = False,
    add_valuehead: bool = False,
) -> "PreTrainedModel":
    """
    加载模型，如果 use_kt=True 则使用 KT AMX 后端

    调用 KT AMX 的方式：
        if model_args.use_kt:
            from .model_utils.kt_moe import load_kt_model
            model = load_kt_model(config, model_args, finetuning_args)
    """
```

### 1.2 Adapter 初始化接口

**调用位置**: `src/llamafactory/model/adapter.py`

```python
def init_adapter(
    config: "PretrainedConfig",
    model: "PreTrainedModel",
    model_args: "ModelArguments",
    finetuning_args: "FinetuningArguments",
    is_trainable: bool,
) -> "PreTrainedModel":
    """
    初始化 LoRA 适配器

    KT AMX 模式下的处理：
        if model_args.use_kt:
            # MoE LoRA 已由 kt_moe 处理
            # 只对 Attention 层应用 peft LoRA
            target_modules = filter_attention_modules(finetuning_args.lora_target)
            ...
    """
```

### 1.3 Trainer 工作流接口

**调用位置**: `src/llamafactory/train/sft/workflow.py`

```python
def run_sft(
    model_args: "ModelArguments",
    data_args: "DataArguments",
    training_args: "Seq2SeqTrainingArguments",
    finetuning_args: "FinetuningArguments",
    generating_args: "GeneratingArguments",
    callbacks: Optional[list["TrainerCallback"]] = None,
):
    """
    运行 SFT 训练

    KT AMX 模式下的 Trainer 选择：
        if model_args.use_kt:
            from .kt_amx_trainer import KTTrainer
            trainer = KTTrainer(...)
        else:
            trainer = CustomSeq2SeqTrainer(...)
    """
```

---

## 2. 核心接口（kt_moe.py）

### 2.1 模型加载

```python
def load_kt_model(
    config: "PretrainedConfig",
    model_args: "ModelArguments",
    finetuning_args: "FinetuningArguments",
) -> "PreTrainedModel":
    """
    使用 KT AMX 后端加载模型

    Args:
        config: HuggingFace 模型配置
        model_args: 模型参数（包含 kt_amx_* 配置）
        finetuning_args: 微调参数（包含 lora_rank, lora_alpha 等）

    Returns:
        model: 已包装 MoE 层的模型
            - model._kt_wrappers: list[MOELayerWrapper]
            - model._kt_cpu_infer: CPUInfer
            - model._kt_moe_lora_params: dict[int, dict[str, nn.Parameter]]

    Raises:
        ImportError: kt_kernel 未安装
        ValueError: 模型架构不支持
        RuntimeError: AMX 硬件不支持
    """
```

### 2.2 CPUInfer 初始化

```python
def init_cpu_infer(model_args: "ModelArguments") -> "CPUInfer":
    """
    初始化 KT CPUInfer 实例

    Args:
        model_args: 模型参数
            - kt_num_threads: CPU 线程数
            - kt_tp_enabled: 是否启用 TP

    Returns:
        cpu_infer: CPUInfer 实例
    """
```

### 2.3 MoE 层包装

```python
def wrap_moe_layers_with_amx(
    model: "PreTrainedModel",
    model_args: "ModelArguments",
    finetuning_args: "FinetuningArguments",
    cpu_infer: "CPUInfer",
) -> list["MOELayerWrapper"]:
    """
    将模型的 MoE 层替换为 AMX 包装

    Args:
        model: HuggingFace 模型
        model_args: 模型参数
        finetuning_args: 微调参数
        cpu_infer: CPUInfer 实例

    Returns:
        wrappers: MOELayerWrapper 列表

    Side Effects:
        - 修改 model.model.layers[i].mlp 为 MOELayerWrapper
        - MoE 权重移动到 CPU
    """
```

### 2.4 MOELayerWrapper 类

```python
class MOELayerWrapper(nn.Module):
    """
    MoE 层的 AMX 包装器

    Attributes:
        original_moe: 原始 MoE 层（保留用于访问 router）
        moe_amx: AMXBF16_SFT_MOE 或 AMXInt8_SFT_MOE 实例
        config: MOESFTConfig
        cpu_infer: CPUInfer 实例
        lora_params: dict[str, nn.Parameter]
            - gate_lora_a: [expert_num, lora_rank, hidden_size]
            - gate_lora_b: [expert_num, intermediate_size, lora_rank]
            - up_lora_a, up_lora_b, down_lora_a, down_lora_b: 类似
        moe_config: MOEConfig
        router: 路由器模块

    Methods:
        forward(hidden_states) -> Tensor
        update_lora_pointers() -> None
    """

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """
        前向传播

        Args:
            hidden_states: [batch, seq_len, hidden_size] (GPU)

        Returns:
            output: [batch, seq_len, hidden_size] (GPU)

        Notes:
            - 使用 MOEAMXFunction.apply 进行前向计算
            - 自动保存中间值用于反向传播
        """

    def update_lora_pointers(self) -> None:
        """
        更新 AMX 算子的 LoRA 权重指针

        Notes:
            - TP 模式下每次 optimizer.step() 后必须调用
            - no-TP 模式下使用零拷贝，无需调用
        """
```

### 2.5 MOEAMXFunction 类

```python
class MOEAMXFunction(torch.autograd.Function):
    """
    自定义 autograd 函数，桥接 PyTorch 和 AMX

    Static Methods:
        forward(ctx, hidden_states, router, moe_amx, cpu_infer,
                lora_params, config, moe_config) -> Tensor
        backward(ctx, grad_output) -> tuple[Tensor, None, ...]
    """

    @staticmethod
    def forward(
        ctx,
        hidden_states: torch.Tensor,
        router: nn.Module,
        moe_amx: "AMXBF16_SFT_MOE",
        cpu_infer: "CPUInfer",
        lora_params: dict[str, nn.Parameter],
        config: "MOESFTConfig",
        moe_config: "MOEConfig",
    ) -> torch.Tensor:
        """
        前向传播

        Args:
            ctx: autograd 上下文
            hidden_states: [batch, seq_len, hidden_size] (GPU)
            router: MoE 路由器模块
            moe_amx: AMX MOE 实例
            cpu_infer: CPUInfer 实例
            lora_params: LoRA 参数字典
            config: MOESFTConfig
            moe_config: 模型 MoE 配置

        Returns:
            output: [batch, seq_len, hidden_size] (GPU)
        """

    @staticmethod
    def backward(ctx, grad_output: torch.Tensor) -> tuple:
        """
        反向传播

        Args:
            ctx: autograd 上下文
            grad_output: [batch, seq_len, hidden_size] (GPU)

        Returns:
            (grad_input, None, None, None, None, None, None)
            - grad_input: [batch, seq_len, hidden_size] (GPU)
            - 其他位置对应 forward 的非 Tensor 参数，返回 None

        Side Effects:
            - 累积 LoRA 梯度到 lora_params[*].grad
        """
```

---

## 3. 下游接口（被调用方）

### 3.1 KT Kernel 接口

**来源**: `kt_kernel.kt_kernel_ext`

#### 3.1.1 CPUInfer

```python
class CPUInfer:
    """CPU 推理管理器"""

    def __init__(self, num_threads: int):
        """TP 模式初始化"""

    def __init__(self, config: WorkerPoolConfig):
        """自定义配置初始化"""

    @property
    def backend_(self) -> WorkerPool:
        """获取后端线程池"""

    def submit(self, task: Task) -> None:
        """提交任务"""

    def sync(self) -> None:
        """等待所有任务完成"""
```

#### 3.1.2 MOESFTConfig

```python
class MOESFTConfig:
    """MoE SFT 配置"""

    # 模型配置
    expert_num: int  # 专家总数
    num_experts_per_tok: int  # 每 token 激活专家数
    hidden_size: int  # 隐藏层维度
    intermediate_size: int  # MLP 中间层维度

    # LoRA 配置
    lora_rank: int  # LoRA 秩
    lora_alpha: float  # LoRA 缩放因子

    # 缓存配置
    max_cache_depth: int  # 梯度检查点缓存深度
    max_len: int  # 最大序列长度
    layer_idx: int  # 层索引

    # 基础权重指针
    gate_proj: int  # data_ptr()
    up_proj: int
    down_proj: int

    # LoRA 权重指针
    gate_lora_a: int
    gate_lora_b: int
    up_lora_a: int
    up_lora_b: int
    down_lora_a: int
    down_lora_b: int

    # 线程池
    pool: WorkerPool
```

#### 3.1.3 AMXBF16_SFT_MOE

```python
class AMXBF16_SFT_MOE:
    """BF16 模式的 AMX MoE 实例"""

    def __init__(self, config: MOESFTConfig):
        """初始化"""

    def load_weights_task(self) -> Task:
        """加载基础权重任务"""

    def warm_up_task(self) -> Task:
        """预热任务"""

    def forward_sft_task(
        self,
        bsz_ptr: int,  # 批大小张量指针
        num_experts_per_tok: int,
        expert_ids_ptr: int,  # [qlen, k] int64
        weights_ptr: int,  # [qlen, k] float32
        input_ptr: int,  # [qlen, hidden_size] bf16
        output_ptr: int,  # [qlen, hidden_size] float32
        save_for_backward: bool,
    ) -> Task:
        """前向传播任务"""

    def backward_task(
        self,
        grad_output_ptr: int,  # [qlen, hidden_size] bf16
        grad_input_ptr: int,  # [qlen, hidden_size] bf16
        grad_gate_lora_a_ptr: int,
        grad_gate_lora_b_ptr: int,
        grad_up_lora_a_ptr: int,
        grad_up_lora_b_ptr: int,
        grad_down_lora_a_ptr: int,
        grad_down_lora_b_ptr: int,
    ) -> Task:
        """反向传播任务"""

    def update_lora_weights_task(
        self,
        gate_lora_a_ptr: int,
        gate_lora_b_ptr: int,
        up_lora_a_ptr: int,
        up_lora_b_ptr: int,
        down_lora_a_ptr: int,
        down_lora_b_ptr: int,
    ) -> Task:
        """更新 LoRA 权重指针任务 (TP 模式需要)"""
```

---

## 4. 配置参数接口

### 4.1 ModelArguments 新增参数

**位置**: `src/llamafactory/hparams/model_args.py`

```python
@dataclass
class ModelArguments:
    # ... 现有参数 ...

    # KT AMX 配置
    use_kt: bool = field(
        default=False,
        metadata={
            "help": "Whether to use KT AMX MOE backend for MoE layers."
        },
    )

    kt_backend: Literal["AMXBF16", "AMXInt8"] = field(
        default="AMXBF16",
        metadata={
            "help": "AMX backend type: AMXBF16 for BF16 mode, AMXInt8 for INT8 quantization."
        },
    )

    kt_num_threads: int = field(
        default=60,
        metadata={
            "help": "Number of CPU threads for AMX inference."
        },
    )

    kt_tp_enabled: bool = field(
        default=True,
        metadata={
            "help": "Enable Tensor Parallelism across NUMA nodes."
        },
    )

    kt_num_gpu_experts: int = field(
        default=0,
        metadata={
            "help": "Number of MoE experts to run on GPU. "
                    "0 = all experts on CPU, >0 = hybrid GPU/CPU mode (Phase 2)."
        },
    )

    kt_weight_path: Optional[str] = field(
        default=None,
        metadata={
            "help": "Path to preprocessed CPU expert weights. "
                    "Used for loading INT4/INT8 quantized weights (Phase 2)."
        },
    )

    kt_max_cache_depth: int = field(
        default=1,
        metadata={
            "help": "Maximum cache depth for gradient checkpointing. "
                    "Set to gradient_accumulation_steps for best memory efficiency."
        },
    )
```

---

## 5. 权重格式

### 5.1 基础权重（冻结）

```python
# 形状定义
gate_proj: Tensor  # [expert_num, intermediate_size, hidden_size], bf16
up_proj: Tensor    # [expert_num, intermediate_size, hidden_size], bf16
down_proj: Tensor  # [expert_num, hidden_size, intermediate_size], bf16

# 内存要求
# 必须是 contiguous 的 CPU tensor
gate_proj = gate_proj.cpu().contiguous()
```

### 5.2 LoRA 权重（可训练）

```python
# Gate 投影 LoRA
gate_lora_a: nn.Parameter  # [expert_num, lora_rank, hidden_size], bf16
gate_lora_b: nn.Parameter  # [expert_num, intermediate_size, lora_rank], bf16

# Up 投影 LoRA
up_lora_a: nn.Parameter    # [expert_num, lora_rank, hidden_size], bf16
up_lora_b: nn.Parameter    # [expert_num, intermediate_size, lora_rank], bf16

# Down 投影 LoRA
down_lora_a: nn.Parameter  # [expert_num, lora_rank, intermediate_size], bf16
down_lora_b: nn.Parameter  # [expert_num, hidden_size, lora_rank], bf16

# 初始化
# A 矩阵：kaiming_uniform
# B 矩阵：zeros
nn.init.kaiming_uniform_(gate_lora_a, a=math.sqrt(5))
nn.init.zeros_(gate_lora_b)
```

### 5.3 输入/输出格式

```python
# 输入
hidden_states: Tensor  # [batch, seq_len, hidden_size], fp16/bf16, GPU

# 路由信息
expert_ids: Tensor     # [batch * seq_len, num_experts_per_tok], int64, CPU
weights: Tensor        # [batch * seq_len, num_experts_per_tok], float32, CPU

# 输出
output: Tensor         # [batch, seq_len, hidden_size], fp32 (AMX) → fp16/bf16 (GPU)

# 梯度
grad_output: Tensor    # [batch, seq_len, hidden_size], bf16, CPU
grad_input: Tensor     # [batch, seq_len, hidden_size], bf16, CPU
grad_lora_*: Tensor    # 与对应权重相同形状
```

---

## 6. 错误码和异常

### 6.1 异常类型

```python
class KTAMXError(Exception):
    """KT AMX 基础异常"""
    pass

class KTAMXNotAvailableError(KTAMXError):
    """kt_kernel 未安装或 AMX 不支持"""
    pass

class KTAMXModelNotSupportedError(KTAMXError):
    """模型架构不支持"""
    pass

class KTAMXConfigError(KTAMXError):
    """配置错误"""
    pass

class KTAMXForwardError(KTAMXError):
    """前向传播错误"""
    pass

class KTAMXBackwardError(KTAMXError):
    """反向传播错误"""
    pass
```

### 6.2 错误处理示例

```python
try:
    model = load_kt_model(config, model_args, finetuning_args)
except KTAMXNotAvailableError as e:
    logger.error(f"KT AMX not available: {e}")
    logger.info("Falling back to standard model loading")
    model = AutoModelForCausalLM.from_pretrained(...)
except KTAMXModelNotSupportedError as e:
    logger.error(f"Model not supported: {e}")
    raise
```

---

## 7. 使用示例

### 7.1 配置文件

```yaml
# train_kt_amx_sft.yaml
model_name_or_path: deepseek-ai/DeepSeek-V2-Lite-Chat
stage: sft
finetuning_type: lora

# KT AMX 配置
use_kt: true
kt_backend: AMXBF16
kt_num_threads: 60
kt_tp_enabled: true
kt_num_gpu_experts: 0  # GPU 上运行的专家数量 (0=全部在CPU)
# kt_weight_path: null  # 预处理权重路径 (可选，用于 INT4/INT8)

# LoRA 配置（同时应用于 MoE 和 Attention）
lora_rank: 16
lora_alpha: 32
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

# 训练配置
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 1
output_dir: outputs/deepseek_v2_sft_kt_amx
```

### 7.2 命令行启动

```bash
llamafactory-cli train examples/train_kt_amx_sft.yaml
```

### 7.3 代码调用

```python
from llamafactory.model import load_model, load_tokenizer
from llamafactory.hparams import get_train_args

# 解析参数
model_args, data_args, training_args, finetuning_args, generating_args = (
    get_train_args()
)

# 加载模型（自动使用 KT AMX）
tokenizer = load_tokenizer(model_args)["tokenizer"]
model = load_model(tokenizer, model_args, finetuning_args, is_trainable=True)

# 访问 KT AMX 组件
if hasattr(model, "_kt_wrappers"):
    print(f"Loaded {len(model._kt_wrappers)} MoE layers with KT AMX")
    for wrapper in model._kt_wrappers:
        print(f"  Layer: {wrapper.lora_params.keys()}")
```
