# KTransformers SFT Bug è°ƒè¯•è®°å½•

## BUG-007: backward_down SIGSEGVï¼ˆâœ… å·²è§£å†³ï¼‰

### é—®é¢˜æè¿°
åœ¨ `backward_down` å‡½æ•°ä¸­è®¿é—® `grad_down_b` ç¼“å†²åŒºæ—¶å‘ç”Ÿ SIGSEGVã€‚

### è°ƒè¯•å†ç¨‹

#### ç¬¬ä¸€é˜¶æ®µï¼šå‘ç° lora_rank Object Slicing

**ç—‡çŠ¶**ï¼šPython ç«¯è®¾ç½® `lora_rank=8`ï¼Œä½† C++ ç«¯æ˜¾ç¤º `lora_rank=16`

**æ ¹å› **ï¼š
- `TP_MOE_SFT` æ„é€ å‡½æ•°å°† `MOESFTConfig` è½¬ä¸º `GeneralMOEConfig` ä¼ é€’ç»™åŸºç±»
- `GeneralMOEConfig` ä¸åŒ…å« `lora_rank` å­—æ®µï¼ˆObject Slicingï¼‰
- åˆ›å»º `AMX_SFT_MOE_TP` æ—¶ä½¿ç”¨é»˜è®¤å€¼ `lora_rank=16`

**ä¿®å¤**ï¼š
```cpp
// sft_moe.hpp
void set_lora_params(int rank, float alpha) {
    lora_rank_ = rank;
    lora_scaling_ = alpha / rank;
}

// moe-sft-tp.hpp æ„é€ å‡½æ•°
for (int i = 0; i < tp_count; i++) {
    tps[i]->set_lora_params(config.lora_rank, config.lora_alpha);
}
```

**çŠ¶æ€**ï¼šâœ“ å·²ä¿®å¤ï¼Œlora_rank ç°åœ¨æ­£ç¡®æ˜¾ç¤ºä¸º 8

#### ç¬¬äºŒé˜¶æ®µï¼šæ·»åŠ è¯¦ç»†è°ƒè¯•ä¿¡æ¯

æ·»åŠ äº†ä»¥ä¸‹è°ƒè¯•è¾“å‡ºï¼š
1. `[DEBUG backward_down]` - æ‰“å° task_id, expert_idx, config ä¿¡æ¯
2. `[DEBUG grad_B]` - æ‰“å°å¾ªç¯å‰çš„ offset å’ŒæŒ‡é’ˆä¿¡æ¯
3. è¾¹ç•Œæ£€æŸ¥ - å¦‚æœç´¢å¼•è¶Šç•Œæ‰“å° `[CRASH]`

#### ç¬¬ä¸‰é˜¶æ®µï¼šè°ƒè¯•è¾“å‡ºåˆ†æï¼ˆ2026-01-06ï¼‰

**è°ƒè¯•è¾“å‡º**ï¼š
```
[DEBUG backward_down] task_id=0, expert_idx=0, num_tokens=48, qlen=48, k=6
[DEBUG backward_down] config: hidden_size=2048, intermediate_size=1408, expert_num=64, lora_rank=8
[DEBUG backward_down] lora_b_offset=0, max_valid_offset=1048576
[DEBUG backward_down] grad_down_b=0x7fa926980000, down_lora_b_=0x881e4200
...
[DEBUG grad_B] expert_idx=2, lora_b_offset=32768, hidden_size=2048, lora_rank=8
[DEBUG grad_B] grad_down_b=0x7fa926980000, max_valid_idx=1048576
```

**å…³é”®å‘ç°**ï¼š
| æ£€æŸ¥é¡¹ | ç»“æœ |
|--------|------|
| lora_rank | 8 âœ“ æ­£ç¡® |
| max_valid_idx | 1048576 âœ“ (64Ã—2048Ã—8) |
| grad_down_b æŒ‡é’ˆ | 0x7fa926980000 (éç©º) |
| `[CRASH]` è¾“å‡º | æ—  - ç´¢å¼•åœ¨è¾¹ç•Œå†… |
| SIGSEGV | ä»ç„¶å‘ç”Ÿ |

**ç»“è®º**ï¼šç´¢å¼•è®¡ç®—æ­£ç¡®ä¸”åœ¨è¾¹ç•Œå†…ï¼Œä½†ä»ç„¶å´©æºƒ

### âœ… æ ¹å› ç¡®è®¤ï¼ˆ2026-01-06ï¼‰

**ç¬¬å››é˜¶æ®µè°ƒè¯•è¾“å‡º**ï¼š
```
grad_down_lora_b: shape=torch.Size([64, 2048, 8]), numel=1048576,
                  ptr=0x7ff9d6980000, device=cuda:0  â† GPU!
```

**GDB memory mappings ç¡®è®¤**ï¼š
```
0x7ff9d6000000 - 0x7ffa20000000  ---p  (æ— æƒé™åŒºåŸŸ)
```

åœ°å€ `0x7ff9d6980000` ä¸åœ¨ä»»ä½•æœ‰æ•ˆçš„ CPU å†…å­˜æ˜ å°„ä¸­ï¼

**æ ¹å› **ï¼š`torch.zeros_like()` ç»§æ‰¿åŸ tensor çš„ deviceã€‚LoRA å‚æ•°åœ¨ GPU ä¸Šï¼Œæ¢¯åº¦ tensor ä¹Ÿåœ¨ GPU ä¸Šï¼Œä½† C++ AMX ä»£ç éœ€è¦ CPU å†…å­˜è®¿é—®ï¼

### âœ… ä¿®å¤æ–¹æ¡ˆ

```python
# kt_moe.py backward() - æ·»åŠ  device="cpu"
grad_gate_lora_a = torch.zeros_like(ctx.lora_params["gate_lora_a"].data, device="cpu")
grad_gate_lora_b = torch.zeros_like(ctx.lora_params["gate_lora_b"].data, device="cpu")
grad_up_lora_a = torch.zeros_like(ctx.lora_params["up_lora_a"].data, device="cpu")
grad_up_lora_b = torch.zeros_like(ctx.lora_params["up_lora_b"].data, device="cpu")
grad_down_lora_a = torch.zeros_like(ctx.lora_params["down_lora_a"].data, device="cpu")
grad_down_lora_b = torch.zeros_like(ctx.lora_params["down_lora_b"].data, device="cpu")
```

**çŠ¶æ€**ï¼šå·²ä¿®å¤ (kt_moe.py:481-489)

### ç¬¬äº”é˜¶æ®µï¼šæ¢¯åº¦è®¾å¤‡ä¸åŒ¹é…ï¼ˆ2026-01-06ï¼‰

**é”™è¯¯**ï¼š
```
RuntimeError: attempting to assign a gradient with device type 'cpu'
to a tensor with device type 'cuda'
```

**åŸå› **ï¼šæ¢¯åº¦åœ¨ CPUï¼ˆAMX éœ€è¦ï¼‰ï¼Œä½† LoRA å‚æ•°åœ¨ GPUï¼ˆ`model.to("cuda")` ä¼šç§»åŠ¨ï¼‰

**ä¿®å¤ï¼ˆæ–¹æ¡ˆ Aï¼‰**ï¼š
```python
# kt_moe.py:516-521
def accumulate_grad(param: nn.Parameter, grad: torch.Tensor):
    grad_on_device = grad.to(param.device)  # CPU â†’ GPU
    if param.grad is None:
        param.grad = grad_on_device.clone()
    else:
        param.grad.add_(grad_on_device)
```

**æ–°å¢é…ç½®é¡¹**ï¼š
- `kt_moe_lora_device: gpu` (model_args.py:514-521, YAML line 48)
- æ”¯æŒ `gpu`ï¼ˆæ–¹æ¡ˆ Aï¼Œå·²å®ç°ï¼‰å’Œ `cpu`ï¼ˆæ–¹æ¡ˆ Bï¼ŒæŠ›å‡º NotImplementedErrorï¼‰

**çŠ¶æ€**ï¼šå·²ä¿®å¤

---

## BUG-006: Forward cache stack overflowï¼ˆå·²è§£å†³ï¼‰

**ç—‡çŠ¶**ï¼šforward è¿‡ç¨‹ä¸­ cache stack overflow

**æ ¹å› **ï¼š`gradient_checkpointing` ä¼šå¤šæ¬¡è°ƒç”¨ forwardï¼Œæ¯æ¬¡éƒ½ push cache ä½†ä¸ pop

**ä¿®å¤**ï¼šåœ¨ YAML ä¸­æ·»åŠ  `disable_gradient_checkpointing: true`

**çŠ¶æ€**ï¼šâœ“ å·²è§£å†³

---

## ä¿®æ”¹æ–‡ä»¶æ±‡æ€»

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ | Bug |
|------|----------|-----|
| `kt_moe.py` | æ¢¯åº¦ tensor æ·»åŠ  `device="cpu"` (line 481-489) | BUG-007 |
| `kt_moe.py` | `accumulate_grad` æ·»åŠ  CPUâ†’GPU ä¼ è¾“ (line 508-513) | BUG-007 |
| `model_args.py` | æ·»åŠ  `kt_moe_lora_device` é…ç½® | BUG-007 |
| `sft_moe.hpp` | æ·»åŠ  `set_lora_params()` ä¿®å¤ Object Slicing | BUG-007 |
| `moe-sft-tp.hpp` | è°ƒç”¨ `set_lora_params()` | BUG-007 |
| `deepseek2_lora_sft_kt.yaml` | æ·»åŠ  `disable_gradient_checkpointing: true` | BUG-006 |
| `deepseek2_lora_sft_kt.yaml` | æ·»åŠ  `kt_moe_lora_device: gpu` | BUG-007 |

---

## æ¸…ç†è®°å½•ï¼ˆ2026-01-06ï¼‰

å·²åˆ é™¤æ‰€æœ‰è°ƒè¯•ä»£ç ï¼š
- `kt_moe.py`: åˆ é™¤ `[DEBUG BUG-007]` logger.info è¯­å¥
- `sft_moe.hpp`: åˆ é™¤ `set_lora_params()`ã€`backward_down()` ä¸­çš„ printf è°ƒè¯•è¾“å‡º

---

## BUG-008: KTrainer._maybe_log_save_evaluate() å‚æ•°ä¸å…¼å®¹ï¼ˆâœ… å·²è§£å†³ï¼‰

### é—®é¢˜æè¿°

```
TypeError: KTrainer._maybe_log_save_evaluate() got an unexpected keyword argument 'learning_rate'
```

è®­ç»ƒæˆåŠŸè¿è¡Œç¬¬ä¸€ä¸ª step åï¼Œåœ¨ `_maybe_log_save_evaluate()` è°ƒç”¨æ—¶æŠ¥é”™ã€‚

### æ ¹å› 

`KTrainer` é‡å†™äº†çˆ¶ç±» `_maybe_log_save_evaluate()` æ–¹æ³•ï¼Œä½†æ–¹æ³•ç­¾åç¼ºå°‘æ–°ç‰ˆ transformers Trainer ä¼ é€’çš„ `learning_rate` å‚æ•°ã€‚

### ä¿®å¤

åœ¨ `kt_trainer.py:255` æ–¹æ³•ç­¾åä¸­æ·»åŠ  `learning_rate=None` å‚æ•°ï¼š

```python
def _maybe_log_save_evaluate(self, tr_loss, grad_norm, model, trial, epoch, ignore_keys_for_eval, learning_rate=None):
    self._update_lora_pointers()
    return super()._maybe_log_save_evaluate(..., learning_rate=learning_rate)
```

**çŠ¶æ€**ï¼šâœ“ å·²ä¿®å¤

---

## BUG-009: è®­ç»ƒäº§ç”Ÿ NaN - PEFT åŒ…è£…ç©ºæƒé‡å±‚ï¼ˆâœ… å·²è§£å†³ï¼‰

### é—®é¢˜æè¿°

è®­ç»ƒäº§ç”Ÿçš„ adapter æ–‡ä»¶åŒ…å«å¤§é‡ NaN å€¼ï¼š

| ç»„ä»¶ | NaN å¼ é‡æ•° | è¯´æ˜ |
|------|------------|------|
| shared_experts | 312 | æ‰€æœ‰å±‚çš„ shared experts LoRA |
| self_attn | 216 | æ‰€æœ‰å±‚çš„ Attention LoRA |
| dense_mlp | 6 | Layer 0 çš„ Dense MLP LoRA |
| routed_experts | **0** | MoE è·¯ç”±ä¸“å®¶ LoRA **æ­£å¸¸** |

### å…³é”®è§‚å¯Ÿ

NaN **åªå‡ºç°åœ¨ GPU ä¸Šè®­ç»ƒçš„éƒ¨åˆ†**ï¼ˆPEFT ç®¡ç†ï¼‰ï¼ŒKT AMX (CPU) å¤„ç†çš„ routed_experts **æ²¡æœ‰ NaN**ã€‚

### æ ¹å› åˆ†æ

**ä»£ç æ‰§è¡Œé¡ºåº** (`loader.py`):
1. `load_kt_model()` - åˆ›å»º `MOELayerWrapper`ï¼Œè°ƒç”¨ `_clear_original_expert_weights()` æ¸…ç©ºä¸“å®¶æƒé‡ä¸º `torch.empty(0)`
2. `init_adapter()` - PEFT éå†æ¨¡å‹ï¼ŒåŒ…è£…æ‰€æœ‰ Linear å±‚

**é—®é¢˜**:
- `MOELayerWrapper.__init__()` å­˜å‚¨äº† `self.original_moe = original_moe`
- PEFT çš„ `get_peft_model()` é€šè¿‡ `named_modules()` éå†æ¨¡å‹
- å‘ç° `wrapper.original_moe.experts.N.{gate,up,down}_proj` å±‚ï¼ˆLinear ç±»å‹ï¼‰
- **ä½†è¿™äº›å±‚çš„ weight å·²è¢«æ¸…ç©ºä¸º `torch.empty(0)`ï¼**
- PEFT ä»ç„¶åŒ…è£…è¿™äº›å±‚ï¼Œåˆ›å»º LoRA

**å¯¼è‡´çš„é—®é¢˜**:
1. LoRA A/B çŸ©é˜µåŸºäºç©ºæƒé‡åˆ›å»ºï¼Œç»´åº¦å¯èƒ½ä¸æ­£ç¡®
2. å‰å‘ä¼ æ’­å¯¹ç©ºæƒé‡æ“ä½œï¼Œäº§ç”Ÿå¼‚å¸¸å€¼
3. æ¢¯åº¦è®¡ç®—æ•°å€¼ä¸ç¨³å®šï¼Œäº§ç”Ÿ NaN

### ä¿®å¤æ–¹æ¡ˆ

**åˆ é™¤ `self.original_moe = original_moe` èµ‹å€¼**

ä¿®æ”¹ `kt_moe.py` çš„ `MOELayerWrapper.__init__()`:

```python
# ä¿®æ”¹å‰ (line 567)
self.original_moe = original_moe

# ä¿®æ”¹å
# NOTE: Do NOT store original_moe as self.original_moe!
# PEFT's get_peft_model() uses named_modules() to find Linear layers.
# If we store original_moe, PEFT will find original_moe.experts.N.{gate,up,down}_proj
# which have empty weights (cleared by _clear_original_expert_weights).
# This causes NaN during training.
# We only need router and shared_experts, which are stored separately below.
```

`router` å’Œ `shared_experts` å·²ç»è¢«å•ç‹¬å­˜å‚¨ï¼Œä¸éœ€è¦ä¿ç•™ `original_moe` å¼•ç”¨ã€‚

### å½±å“

- **ä¿®å¤å‰**: PEFT å‘ç°å¹¶åŒ…è£… ~64Ã—26 = 1664 ä¸ªç©ºæƒé‡çš„ä¸“å®¶å±‚
- **ä¿®å¤å**: PEFT åªåŒ…è£…æœ‰æ•ˆæƒé‡çš„å±‚ (self_attn, shared_experts, dense_mlp)

**çŠ¶æ€**ï¼šâœ… å·²ä¿®å¤ (kt_moe.py:567)

---

## ä¿®æ”¹æ–‡ä»¶æ±‡æ€»ï¼ˆæ›´æ–°ï¼‰

| æ–‡ä»¶ | ä¿®æ”¹å†…å®¹ | Bug |
|------|----------|-----|
| `kt_moe.py` | åˆ é™¤ `self.original_moe = original_moe` | BUG-009 |
| `kt_moe.py` | æ·»åŠ  `self._is_kt_moe_wrapper = True` æ ‡è®° | BUG-010 |
| `kt_moe.py` | æ¢¯åº¦ tensor æ·»åŠ  `device="cpu"` | BUG-007 |
| `kt_moe.py` | `accumulate_grad` æ·»åŠ  CPUâ†’GPU ä¼ è¾“ | BUG-007 |
| `adapter.py` | è·³è¿‡ KT MoE LoRA å‚æ•°çš„ float32 upcast | BUG-010 |
| `model_args.py` | æ·»åŠ  `kt_moe_lora_device` é…ç½® | BUG-007 |
| `sft_moe.hpp` | æ·»åŠ  `set_lora_params()` ä¿®å¤ Object Slicing | BUG-007 |
| `moe-sft-tp.hpp` | è°ƒç”¨ `set_lora_params()` | BUG-007 |
| `deepseek2_lora_sft_kt.yaml` | æ·»åŠ  `disable_gradient_checkpointing: true` | BUG-006 |
| `deepseek2_lora_sft_kt.yaml` | æ·»åŠ  `kt_moe_lora_device: gpu` | BUG-007 |

---

## BUG-010: AMX Forward äº§ç”Ÿ NaNï¼ˆğŸ”„ è°ƒæŸ¥ä¸­ï¼‰

### é—®é¢˜æè¿°

è®­ç»ƒç¬¬ä¸€ä¸ª forward pass å°±äº§ç”Ÿ NaNï¼Œä» Layer 1 çš„ AMX forward å¼€å§‹ã€‚

### è¯Šæ–­æ—¥å¿—åˆ†æï¼ˆ2026-01-09ï¼‰

**æ—¥å¿—æ–‡ä»¶**: `/home/lpl/LLaMA-Factory-KT/kt_nan_diag.log`

#### å…³é”®å‘ç° 1: NaN é¦–æ¬¡å‡ºç°åœ¨ Layer 1 çš„ AMX forward

```
[ERROR] [Layer 1] NaN in moe_output (from AMX)!
[ERROR] [Layer 2] NaN in moe_output (from AMX)!
[ERROR] [Layer 2] NaN in shared_experts output!
...
```

**æ³¨æ„**: Layer 1 åªæœ‰ `moe_output` NaNï¼Œ**shared_experts æ²¡æœ‰ NaN**ï¼
- è¿™è¯æ˜ NaN æ¥æºæ˜¯ **AMX forward è®¡ç®—**ï¼Œä¸æ˜¯ shared_experts
- Layer 2+ çš„ shared_experts æœ‰ NaN æ˜¯å› ä¸ºè¾“å…¥ `hidden_states` å·²è¢« Layer 1 æ±¡æŸ“

#### å…³é”®å‘ç° 2: Upcasting å¯¼è‡´ dtype ä¸åŒ¹é…

```
[INFO] Upcasting trainable params to float32.  â† é—®é¢˜æ ¹æºï¼
...
[ERROR] [Layer 1] NaN in moe_output (from AMX)!  â† ç¬¬ä¸€å±‚å°± NaN
```

### âœ… æ ¹å› ç¡®è®¤

**é—®é¢˜é“¾è·¯**:

1. `create_lora_params()` åœ¨ CPU ä¸Šåˆ›å»º **bfloat16** çš„ LoRA å‚æ•°
2. `MOESFTConfig` å­˜å‚¨è¿™äº›å‚æ•°çš„ CPU åœ°å€ï¼ˆæŒ‡å‘ bfloat16 æ•°æ®ï¼‰
3. `init_adapter()` æ‰§è¡Œ `param.data = param.data.to(torch.float32)` **å°†æ‰€æœ‰ trainable å‚æ•°è½¬æ¢æˆ float32**
4. AMX forward ä½¿ç”¨åŸæ¥çš„æŒ‡é’ˆè¯»å–æ•°æ®ï¼Œä½†å†…å­˜ä¸­å·²ç»æ˜¯ **float32**ï¼ˆ4 å­—èŠ‚ï¼‰
5. **AMX ä»¥ bfloat16ï¼ˆ2 å­—èŠ‚ï¼‰è§£é‡Š float32 æ•°æ® â†’ äº§ç”Ÿåƒåœ¾å€¼/NaN**

**é—®é¢˜ä»£ç ä½ç½®** (`adapter.py:343-345`):
```python
if is_trainable and cast_trainable_params_to_fp32:
    for param in filter(lambda p: p.requires_grad, model.parameters()):
        param.data = param.data.to(torch.float32)  # â† æŠŠ bf16 LoRA è½¬æˆ fp32!
```

### âœ… ä¿®å¤æ–¹æ¡ˆ

**ä¿®æ”¹ 1**: åœ¨ `MOELayerWrapper` æ·»åŠ æ ‡è®° (`kt_moe.py:584-586`)

```python
# Marker for adapter.py to identify KT MoE wrappers
# Used to skip float32 upcast for LoRA parameters (BUG-010 fix)
self._is_kt_moe_wrapper = True
```

**ä¿®æ”¹ 2**: è·³è¿‡ MoE LoRA å‚æ•°çš„ upcast (`adapter.py:343-363`)

```python
if is_trainable and cast_trainable_params_to_fp32:
    # BUG-010 fix: Collect KT MoE LoRA parameters that must stay in bfloat16
    kt_moe_lora_param_ids = set()
    for name, module in model.named_modules():
        if getattr(module, '_is_kt_moe_wrapper', False):
            for param in module.parameters():
                kt_moe_lora_param_ids.add(id(param))

    # Upcast trainable params except KT MoE LoRA parameters
    upcast_count = 0
    for param in filter(lambda p: p.requires_grad, model.parameters()):
        if id(param) not in kt_moe_lora_param_ids:
            param.data = param.data.to(torch.float32)
            upcast_count += 1

    if kt_moe_lora_param_ids:
        logger.info_rank0(
            f"Kept {len(kt_moe_lora_param_ids)} KT MoE LoRA parameters in bfloat16, "
            f"upcast {upcast_count} other parameters to float32"
        )
```

### ä¿®å¤åé¢„æœŸæ—¥å¿—

```
[INFO] Kept 156 KT MoE LoRA parameters in bfloat16, upcast 378 other parameters to float32
```

### çŠ¶æ€ï¼ˆå°è¯• 1 - upcast ä¿®å¤ï¼‰

âš ï¸ **upcast ä¿®å¤å·²å®æ–½ä½† NaN ä»å­˜åœ¨**

æ—¥å¿—ç¡®è®¤ upcast è·³è¿‡ç”Ÿæ•ˆï¼š
```
[INFO] Kept 416 KT MoE LoRA parameters in bfloat16, upcast 222 other parameters to float32
```

ä½† NaN ä»ç„¶å‡ºç°ï¼Œè¯´æ˜ **upcast ä¸æ˜¯çœŸæ­£çš„æ ¹å› **ã€‚

### ç»§ç»­è°ƒæŸ¥ï¼ˆ2026-01-09ï¼‰

#### æ–°çš„è¯Šæ–­ä»£ç 

åœ¨ `MOEAMXFunction.forward()` ä¸­æ·»åŠ äº†æ›´è¯¦ç»†çš„è¯Šæ–­ï¼š

1. **è¾“å…¥æ•°æ®æ£€æŸ¥** - æ£€æŸ¥ `hidden_states` æ˜¯å¦æœ‰ NaN/Inf
2. **Routing weights æ£€æŸ¥** - æ£€æŸ¥ `topk_weights` æ˜¯å¦æœ‰ NaN/Inf
3. **LoRA å‚æ•°æ£€æŸ¥** - æ£€æŸ¥ LoRA æƒé‡æ˜¯å¦æœ‰ NaNï¼Œæ˜¯å¦åœ¨æ­£ç¡®çš„è®¾å¤‡ä¸Š
4. **è¾“å‡ºæ£€æŸ¥** - æ£€æŸ¥ AMX forward è¾“å‡ºï¼Œå¹¶æ‰“å°è¾“å…¥/æƒé‡èŒƒå›´

#### å…³é”®å·®å¼‚ï¼šLlamaFactory vs ktransformers æµ‹è¯•ä»£ç 

| é¡¹ç›® | ktransformers æµ‹è¯• | LlamaFactory |
|------|-------------------|--------------|
| LoRA A åˆå§‹åŒ– | `randn / 100` (å°å€¼) | `kaiming_uniform` (è¾ƒå¤§å€¼) |
| LoRA B åˆå§‹åŒ– | `zeros` | `zeros` |
| è¾“å…¥æ•°æ® | `randn / 100` (ç¼©å°100å€) | çœŸå®æ¨¡å‹ hidden_states |
| Base weights | `randn` (éšæœº) | é¢„è®­ç»ƒæƒé‡ |

**å¯èƒ½çš„é—®é¢˜**ï¼šæ•°å€¼èŒƒå›´è¶…å‡º bfloat16 ç²¾åº¦èŒƒå›´ï¼Œå¯¼è‡´è®¡ç®—æº¢å‡º/NaNã€‚

ktransformers æµ‹è¯•æ–‡ä»¶ï¼š`/home/lpl/ktransformers/kt-kernel/examples/test_moe_sft_amx_no_tp.py`
