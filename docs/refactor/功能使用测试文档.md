# KTransformers MoE é›†æˆåŠŸèƒ½ä½¿ç”¨æµ‹è¯•æ–‡æ¡£

## 1. ç¯å¢ƒå‡†å¤‡

### 1.1 ç¡¬ä»¶è¦æ±‚

- **CPU**: æ”¯æŒ Intel AMX æŒ‡ä»¤é›†ï¼ˆSapphire Rapids æˆ–æ›´æ–°ï¼‰
- **GPU**: CUDA å…¼å®¹ GPUï¼ˆç”¨äº Attention å±‚è®¡ç®—ï¼‰
- **å†…å­˜**: è¶³å¤Ÿå®¹çº³ MoE æƒé‡ï¼ˆå»ºè®® >= 128GBï¼‰

### 1.2 è½¯ä»¶ä¾èµ–

```bash
# Python ç¯å¢ƒ
python >= 3.10

# æ ¸å¿ƒä¾èµ–
torch >= 2.0
transformers >= 4.40
peft >= 0.8.0

# KT Kernel
pip install kt-kernel  # æˆ–ä»æºç å®‰è£…
```

### 1.3 éªŒè¯ AMX æ”¯æŒ

```python
# æ£€æŸ¥ AMX ç¡¬ä»¶æ”¯æŒ
def check_amx_support():
    try:
        import subprocess
        result = subprocess.run(
            ["lscpu"],
            capture_output=True,
            text=True,
        )
        return "amx" in result.stdout.lower()
    except Exception:
        return False

if check_amx_support():
    print("AMX is supported")
else:
    print("AMX is NOT supported")
```

```python
# æ£€æŸ¥ kt_kernel å®‰è£…
try:
    from kt_kernel import kt_kernel_ext
    print("kt_kernel is available")
except ImportError:
    print("kt_kernel is NOT installed")
```

---

## 2. å¿«é€Ÿå¼€å§‹

### 2.1 æœ€å°é…ç½®ç¤ºä¾‹

åˆ›å»ºé…ç½®æ–‡ä»¶ `train_kt_amx_minimal.yaml`:

```yaml
# æ¨¡å‹é…ç½®
model_name_or_path: deepseek-ai/DeepSeek-V2-Lite-Chat

# è®­ç»ƒé˜¶æ®µ
stage: sft
finetuning_type: lora

# KTransformers é…ç½®ï¼ˆæ ¸å¿ƒï¼ï¼‰
use_kt: true

# LoRA é…ç½®
lora_rank: 16
lora_alpha: 32
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

# æ•°æ®é…ç½®
dataset: alpaca_en
template: deepseek2

# è®­ç»ƒé…ç½®
per_device_train_batch_size: 1
gradient_accumulation_steps: 4
num_train_epochs: 1
output_dir: outputs/kt_amx_test
```

è¿è¡Œè®­ç»ƒï¼š

```bash
llamafactory-cli train train_kt_amx_minimal.yaml
```

### 2.2 å®Œæ•´é…ç½®ç¤ºä¾‹

```yaml
# train_kt_amx_full.yaml

# æ¨¡å‹é…ç½®
model_name_or_path: deepseek-ai/DeepSeek-V3
trust_remote_code: true
torch_dtype: bfloat16

# è®­ç»ƒé˜¶æ®µ
stage: sft
finetuning_type: lora

# KTransformers é…ç½®
use_kt: true
kt_backend: AMXBF16  # å¯é€‰: AMXBF16, AMXInt8
kt_num_threads: 60   # CPU çº¿ç¨‹æ•°
kt_tp_enabled: true  # å¯ç”¨ TP è·¨ NUMA èŠ‚ç‚¹

# LoRA é…ç½®
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.1
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

# æ•°æ®é…ç½®
dataset: alpaca_en
template: deepseek3
cutoff_len: 2048
preprocessing_num_workers: 8

# è®­ç»ƒé…ç½®
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_ratio: 0.1
logging_steps: 10
save_steps: 500
save_total_limit: 3

# è¾“å‡ºé…ç½®
output_dir: outputs/deepseek_v3_kt_amx
overwrite_output_dir: true
```

---

## 3. é…ç½®å‚æ•°è¯´æ˜

### 3.1 KTransformers æ ¸å¿ƒå‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `use_kt` | bool | False | **å¿…éœ€**ã€‚å¯ç”¨ KTransformers MOE åç«¯ |
| `kt_backend` | str | "AMXBF16" | AMX åç«¯ç±»å‹ã€‚"AMXBF16" ä½¿ç”¨ BF16 ç²¾åº¦ï¼Œ"AMXInt8" ä½¿ç”¨ INT8 é‡åŒ– |
| `kt_num_threads` | int | 60 | CPU çº¿ç¨‹æ•°ã€‚å»ºè®®è®¾ç½®ä¸º CPU æ ¸å¿ƒæ•° |
| `kt_tp_enabled` | bool | True | å¯ç”¨ Tensor Parallelismã€‚å¤š NUMA ç³»ç»Ÿå»ºè®®å¼€å¯ |
| `kt_num_gpu_experts` | int | 0 | GPU ä¸Šè¿è¡Œçš„ä¸“å®¶æ•°é‡ã€‚0=å…¨éƒ¨åœ¨CPUï¼Œ>0=æ··åˆGPU/CPUæ¨¡å¼ (Phase 2) |
| `kt_weight_path` | str | None | é¢„å¤„ç† CPU ä¸“å®¶æƒé‡è·¯å¾„ã€‚ç”¨äºåŠ è½½ INT4/INT8 é‡åŒ–æƒé‡ (Phase 2) |

### 3.2 LoRA å‚æ•°

| å‚æ•° | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|------|------|--------|------|
| `lora_rank` | int | 8 | LoRA ç§©ã€‚å½±å“å¯è®­ç»ƒå‚æ•°é‡ |
| `lora_alpha` | float | 16 | LoRA ç¼©æ”¾å› å­ã€‚å®é™…ç¼©æ”¾ = alpha / rank |
| `lora_target` | str | - | LoRA ç›®æ ‡æ¨¡å—ã€‚è§ä¸‹æ–¹è¯¦ç»†è¯´æ˜ |

### 3.3 lora_target é…ç½®è¯´æ˜

`lora_target` æ”¯æŒä»¥ä¸‹é…ç½®æ–¹å¼ï¼š

| é…ç½®å€¼ | è¯´æ˜ |
|--------|------|
| `all` | åŒ…å«æ‰€æœ‰ Linear æ¨¡å—ï¼ˆåŒ…æ‹¬ MoE ä¸“å®¶: gate_proj, up_proj, down_projï¼‰ |
| `all_attention` | **æ–°å¢**ï¼šåªåŒ…å« Attention æ¨¡å—ï¼Œæ’é™¤ MoE ä¸“å®¶å±‚ |
| `q_proj,k_proj,...` | æ˜¾å¼æŒ‡å®šæ¨¡å—åˆ—è¡¨ï¼ˆé€—å·åˆ†éš”ï¼‰ |

**ä½¿ç”¨ç¤ºä¾‹ï¼š**

```yaml
# è®­ç»ƒæ‰€æœ‰å±‚ï¼ˆåŒ…å« MoE ä¸“å®¶ï¼‰
lora_target: all

# åªè®­ç»ƒ Attentionï¼ˆæ’é™¤ MoE ä¸“å®¶ï¼‰
lora_target: all_attention

# æ˜¾å¼æŒ‡å®šæ¨¡å—
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj
```

**æ³¨æ„ï¼š** åœ¨ KTransformers æ¨¡å¼ä¸‹ï¼ŒMoE å±‚ï¼ˆgate_proj, up_proj, down_projï¼‰çš„ LoRA ç”± AMX ç®—å­å¤„ç†ï¼ŒAttention å±‚çš„ LoRA ç”± peft å¤„ç†ã€‚ä¸¤è€…åˆ†å¼€ç®¡ç†ä½†ç»Ÿä¸€è®­ç»ƒã€‚

---

## 4. æ”¯æŒçš„æ¨¡å‹

### 4.1 DeepSeek ç³»åˆ—

```yaml
# DeepSeek-V2
model_name_or_path: deepseek-ai/DeepSeek-V2-Lite-Chat
template: deepseek2

# DeepSeek-V3
model_name_or_path: deepseek-ai/DeepSeek-V3
template: deepseek3
```

### 4.2 Qwen MoE ç³»åˆ—

```yaml
# Qwen2-MoE
model_name_or_path: Qwen/Qwen2-57B-A14B-Instruct
template: qwen

# Qwen3-MoE (å¦‚æœå­˜åœ¨)
model_name_or_path: Qwen/Qwen3-MoE-xxx
template: qwen
```

### 4.3 Mixtral ç³»åˆ—

```yaml
model_name_or_path: mistralai/Mixtral-8x7B-Instruct-v0.1
template: mistral
```

---

## 5. æµ‹è¯•ç”¨ä¾‹

### 5.1 å•å…ƒæµ‹è¯•ï¼šMOELayerWrapper

```python
# tests/test_kt_moe.py

import torch
import pytest
from llamafactory.model.model_utils.kt_moe import (
    MOELayerWrapper,
    MOEAMXFunction,
    init_cpu_infer,
)


@pytest.fixture
def moe_setup():
    """åˆ›å»ºæµ‹è¯•ç”¨çš„ MOE è®¾ç½®"""
    # è¿™é‡Œéœ€è¦æ¨¡æ‹Ÿæˆ–ä½¿ç”¨çœŸå®çš„ kt_kernel
    pass


def test_moe_layer_wrapper_forward(moe_setup):
    """æµ‹è¯• MOELayerWrapper å‰å‘ä¼ æ’­"""
    wrapper = moe_setup["wrapper"]
    hidden_states = torch.randn(2, 128, 7168, dtype=torch.bfloat16, device="cuda")

    output = wrapper(hidden_states)

    assert output.shape == hidden_states.shape
    assert output.device == hidden_states.device


def test_moe_layer_wrapper_backward(moe_setup):
    """æµ‹è¯• MOELayerWrapper åå‘ä¼ æ’­"""
    wrapper = moe_setup["wrapper"]
    hidden_states = torch.randn(
        2, 128, 7168, dtype=torch.bfloat16, device="cuda", requires_grad=True
    )

    output = wrapper(hidden_states)
    loss = output.sum()
    loss.backward()

    # æ£€æŸ¥ LoRA æ¢¯åº¦
    for name, param in wrapper.lora_params.items():
        assert param.grad is not None, f"{name} grad is None"
        assert not torch.isnan(param.grad).any(), f"{name} grad has NaN"


def test_lora_pointer_update(moe_setup):
    """æµ‹è¯• LoRA æƒé‡æŒ‡é’ˆæ›´æ–°"""
    wrapper = moe_setup["wrapper"]

    # æ¨¡æ‹Ÿ optimizer.step()
    for param in wrapper.lora_params.values():
        param.data.add_(0.1)

    # æ›´æ–°æŒ‡é’ˆ
    wrapper.update_lora_pointers()

    # éªŒè¯æ›´æ–°æˆåŠŸï¼ˆæ— å¼‚å¸¸ï¼‰
    assert True
```

### 5.2 é›†æˆæµ‹è¯•ï¼šå®Œæ•´è®­ç»ƒå¾ªç¯

```python
# tests/test_kt_amx_training.py

import os
import tempfile
import pytest
from llamafactory.train.sft.workflow import run_sft
from llamafactory.hparams import get_train_args


@pytest.fixture
def training_args():
    """åˆ›å»ºè®­ç»ƒå‚æ•°"""
    with tempfile.TemporaryDirectory() as tmpdir:
        args = {
            "model_name_or_path": "deepseek-ai/DeepSeek-V2-Lite-Chat",
            "stage": "sft",
            "finetuning_type": "lora",
            "use_kt": True,
            "kt_backend": "AMXBF16",
            "kt_num_threads": 8,
            "lora_rank": 8,
            "lora_target": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj",
            "dataset": "alpaca_en",
            "template": "deepseek2",
            "cutoff_len": 256,
            "per_device_train_batch_size": 1,
            "gradient_accumulation_steps": 1,
            "max_steps": 10,
            "output_dir": tmpdir,
        }
        yield args


def test_sft_training_loop(training_args):
    """æµ‹è¯•å®Œæ•´ SFT è®­ç»ƒå¾ªç¯"""
    model_args, data_args, training_args, finetuning_args, generating_args = (
        get_train_args(training_args)
    )

    run_sft(
        model_args=model_args,
        data_args=data_args,
        training_args=training_args,
        finetuning_args=finetuning_args,
        generating_args=generating_args,
    )

    # éªŒè¯è¾“å‡ºæ–‡ä»¶
    assert os.path.exists(os.path.join(training_args.output_dir, "trainer_state.json"))


def test_checkpoint_save_load(training_args):
    """æµ‹è¯• checkpoint ä¿å­˜å’ŒåŠ è½½"""
    training_args["save_steps"] = 5
    training_args["max_steps"] = 10

    # ç¬¬ä¸€æ¬¡è®­ç»ƒ
    model_args, data_args, training_args, finetuning_args, generating_args = (
        get_train_args(training_args)
    )
    run_sft(...)

    # ä» checkpoint æ¢å¤
    training_args["resume_from_checkpoint"] = True
    training_args["max_steps"] = 20
    run_sft(...)

    assert True
```

### 5.3 ç²¾åº¦æµ‹è¯•ï¼šä¸å‚è€ƒå®ç°å¯¹æ¯”

```python
# tests/test_kt_amx_precision.py

import torch
import pytest


def reference_moe_forward(hidden_states, gate_proj, up_proj, down_proj,
                          gate_lora_a, gate_lora_b, up_lora_a, up_lora_b,
                          down_lora_a, down_lora_b, lora_scaling):
    """PyTorch å‚è€ƒå®ç°"""
    # Gate
    gate_out = hidden_states @ gate_proj.t()
    gate_out += (hidden_states @ gate_lora_a.t() @ gate_lora_b.t()) * lora_scaling

    # Up
    up_out = hidden_states @ up_proj.t()
    up_out += (hidden_states @ up_lora_a.t() @ up_lora_b.t()) * lora_scaling

    # Activation
    intermediate = torch.nn.functional.silu(gate_out) * up_out

    # Down
    output = intermediate @ down_proj.t()
    output += (intermediate @ down_lora_a.t() @ down_lora_b.t()) * lora_scaling

    return output


def test_forward_precision_bf16():
    """æµ‹è¯• BF16 æ¨¡å¼å‰å‘ä¼ æ’­ç²¾åº¦"""
    # è®¾ç½®éšæœºæƒé‡
    hidden_size = 7168
    intermediate_size = 2048
    lora_rank = 16
    batch_size = 4

    hidden_states = torch.randn(batch_size, hidden_size, dtype=torch.bfloat16)
    gate_proj = torch.randn(intermediate_size, hidden_size, dtype=torch.bfloat16)
    # ... å…¶ä»–æƒé‡

    # å‚è€ƒå®ç°
    ref_output = reference_moe_forward(...)

    # AMX å®ç°
    amx_output = moe_amx_forward(...)

    # è®¡ç®—è¯¯å·®
    relative_error = (ref_output - amx_output).abs().mean() / ref_output.abs().mean()

    assert relative_error < 0.05, f"Forward error {relative_error} exceeds threshold 0.05"


def test_backward_precision_bf16():
    """æµ‹è¯• BF16 æ¨¡å¼åå‘ä¼ æ’­ç²¾åº¦"""
    # ç±»ä¼¼å‰å‘æµ‹è¯•ï¼Œä½†éªŒè¯æ¢¯åº¦
    # ...

    relative_error = compute_gradient_error(...)
    assert relative_error < 0.10, f"Backward error {relative_error} exceeds threshold 0.10"
```

### 5.4 æ€§èƒ½æµ‹è¯•

```python
# tests/test_kt_amx_performance.py

import time
import torch
import pytest


def test_forward_throughput():
    """æµ‹è¯•å‰å‘ä¼ æ’­ååé‡"""
    batch_size = 4
    seq_len = 2048
    hidden_size = 7168
    num_iterations = 100

    hidden_states = torch.randn(
        batch_size, seq_len, hidden_size, dtype=torch.bfloat16, device="cuda"
    )

    # é¢„çƒ­
    for _ in range(10):
        output = moe_wrapper(hidden_states)

    # è®¡æ—¶
    torch.cuda.synchronize()
    start = time.time()

    for _ in range(num_iterations):
        output = moe_wrapper(hidden_states)

    torch.cuda.synchronize()
    elapsed = time.time() - start

    tokens_per_sec = (batch_size * seq_len * num_iterations) / elapsed
    print(f"Forward throughput: {tokens_per_sec:.2f} tokens/sec")


def test_memory_usage():
    """æµ‹è¯•å†…å­˜ä½¿ç”¨"""
    import psutil

    # è®°å½•åˆå§‹å†…å­˜
    process = psutil.Process()
    initial_memory = process.memory_info().rss / 1024 / 1024 / 1024  # GB

    # åŠ è½½æ¨¡å‹
    model = load_kt_model(...)

    # è®°å½•åŠ è½½åå†…å­˜
    after_load_memory = process.memory_info().rss / 1024 / 1024 / 1024

    # è¿è¡Œè®­ç»ƒæ­¥éª¤
    for _ in range(10):
        output = model(inputs)
        loss = output.loss
        loss.backward()

    # è®°å½•è®­ç»ƒåå†…å­˜
    after_train_memory = process.memory_info().rss / 1024 / 1024 / 1024

    print(f"Initial memory: {initial_memory:.2f} GB")
    print(f"After load: {after_load_memory:.2f} GB")
    print(f"After train: {after_train_memory:.2f} GB")
```

---

## 6. å¸¸è§é—®é¢˜

### 6.1 kt_kernel æœªæ‰¾åˆ°

**é”™è¯¯ä¿¡æ¯ï¼š**
```
ImportError: kt_kernel not found
```

**è§£å†³æ–¹æ¡ˆï¼š**
```bash
# ä»æºç å®‰è£…
cd /path/to/ktransformers/kt-kernel
pip install -e .

# æˆ–ä½¿ç”¨ pip
pip install kt-kernel
```

### 6.2 AMX ä¸æ”¯æŒ

**é”™è¯¯ä¿¡æ¯ï¼š**
```
RuntimeError: Intel AMX not supported on this CPU
```

**è§£å†³æ–¹æ¡ˆï¼š**
- ç¡®è®¤ CPU å‹å·æ”¯æŒ AMXï¼ˆSapphire Rapids æˆ–æ›´æ–°ï¼‰
- æ£€æŸ¥ BIOS æ˜¯å¦å¯ç”¨ AMX

### 6.3 æ¨¡å‹æ¶æ„ä¸æ”¯æŒ

**é”™è¯¯ä¿¡æ¯ï¼š**
```
ValueError: Model architecture XXX not supported for KTransformers
```

**è§£å†³æ–¹æ¡ˆï¼š**
- æ£€æŸ¥æ¨¡å‹æ˜¯å¦åœ¨æ”¯æŒåˆ—è¡¨ä¸­
- ä½¿ç”¨æ ‡å‡†è®­ç»ƒæ¨¡å¼ï¼ˆä¸å¯ç”¨ use_ktï¼‰

### 6.4 å†…å­˜ä¸è¶³

**é”™è¯¯ä¿¡æ¯ï¼š**
```
RuntimeError: CUDA out of memory
```

**è§£å†³æ–¹æ¡ˆï¼š**
- å‡å° `per_device_train_batch_size`
- å¢åŠ  `gradient_accumulation_steps`
- å¯ç”¨ gradient checkpointing

### 6.5 TP æ¨¡å¼æƒé‡ä¸åŒæ­¥

**ç—‡çŠ¶ï¼š**
è®­ç»ƒ loss ä¸ä¸‹é™æˆ–æ¢¯åº¦å¼‚å¸¸

**è§£å†³æ–¹æ¡ˆï¼š**
ç¡®ä¿ `kt_tp_enabled=True` æ—¶ï¼ŒTrainer æ­£ç¡®è°ƒç”¨ `update_lora_pointers()`

---

## 7. è°ƒè¯•æŠ€å·§

### 7.1 å¯ç”¨è¯¦ç»†æ—¥å¿—

```python
import logging
logging.getLogger("llamafactory").setLevel(logging.DEBUG)
```

### 7.2 éªŒè¯ MoE åŒ…è£…

```python
# æ£€æŸ¥ MoE å±‚æ˜¯å¦æ­£ç¡®åŒ…è£…
model = load_model(...)

if hasattr(model, "_kt_wrappers"):
    print(f"Number of MoE layers: {len(model._kt_wrappers)}")
    for i, wrapper in enumerate(model._kt_wrappers):
        print(f"  Layer {i}: {type(wrapper).__name__}")
        print(f"    LoRA params: {list(wrapper.lora_params.keys())}")
else:
    print("No KTransformers wrappers found")
```

### 7.3 æ£€æŸ¥ LoRA æ¢¯åº¦

```python
# è®­ç»ƒä¸€æ­¥åæ£€æŸ¥æ¢¯åº¦
for wrapper in model._kt_wrappers:
    for name, param in wrapper.lora_params.items():
        if param.grad is not None:
            print(f"{name}: grad_norm = {param.grad.norm().item():.6f}")
        else:
            print(f"{name}: grad is None")
```

### 7.4 å¯¹æ¯”ç²¾åº¦

```python
# å¯¹æ¯” AMX å’Œ PyTorch å‚è€ƒå®ç°çš„è¾“å‡º
ref_output = reference_implementation(...)
amx_output = amx_implementation(...)

diff = (ref_output - amx_output).abs()
print(f"Max diff: {diff.max().item():.6f}")
print(f"Mean diff: {diff.mean().item():.6f}")
print(f"Relative error: {(diff.mean() / ref_output.abs().mean()).item():.6f}")
```

---

## 8. Bug è°ƒè¯•è®°å½• ã€2026-01-05 æ–°å¢ã€‘

### 8.1 å·²è§£å†³çš„ Bug

#### BUG-001: Meta Device æƒé‡åŠ è½½é”™è¯¯ âœ…

**é—®é¢˜**: ä½¿ç”¨ `device_map={"...experts": "cpu"}` æ—¶ï¼ŒTrainer dispatch å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**: æ”¹ç”¨ `device_map="cpu"` åŠ è½½æ•´ä¸ªæ¨¡å‹ï¼Œç„¶åæ‰‹åŠ¨ç§»åŠ¨éä¸“å®¶éƒ¨åˆ†åˆ° GPU

### 8.2 å¾…éªŒè¯çš„ Bug

#### BUG-005: Router è¾“å…¥å½¢çŠ¶ä¸åŒ¹é… ğŸŸ¡

**é—®é¢˜**:
- DeepSeek router æœŸæœ› 3D è¾“å…¥ `[batch, seq, hidden]` ä½†æ”¶åˆ° 2D
- DeepSeek router è¿”å› `(topk_idx, topk_weight, aux_loss)` è€Œé raw logits

**ä¿®æ”¹å†…å®¹**:
1. æ–°å¢ `router_type` å­—æ®µåˆ° `MOEArchConfig`
2. åœ¨ `MOELayerWrapper.forward()` ä¸­æ ¹æ® router_type åˆ†åˆ«å¤„ç†
3. ä¿®æ”¹ `MOEAMXFunction` æ¥å£æ¥æ”¶ `topk_ids, topk_weights`

**çŠ¶æ€**: ä»£ç å·²ä¿®æ”¹ï¼Œç­‰å¾…ç«¯åˆ°ç«¯æµ‹è¯•éªŒè¯

### 8.3 æœªè§£å†³çš„ Bug

#### BUG-004: INT8 æ¨¡å¼ Segfault ğŸ”´

**é—®é¢˜**: ä½¿ç”¨ `kt_backend=AMXInt8` + `kt_weight_path` æ—¶å´©æºƒ

**é…ç½®**:
```yaml
kt_backend: AMXInt8
kt_weight_path: /path/to/int8/weights
```

**çŠ¶æ€**: éœ€è¦æ£€æŸ¥ kt-kernel çš„ AMXInt8_SFT_MOE å®ç°

### 8.4 éªŒè¯æµ‹è¯•å‘½ä»¤

```bash
# éªŒè¯ BF16 æ¨¡å¼ (BUG-005 ä¿®å¤)
CUDA_VISIBLE_DEVICES=1 llamafactory-cli train examples/ktransformers/train_lora/deepseek2_lora_sft_kt.yaml

# éªŒè¯ INT8 æ¨¡å¼ (BUG-004ï¼Œé¢„æœŸå¤±è´¥)
# éœ€è¦å…ˆè®¾ç½® kt_backend: AMXInt8 å’Œ kt_weight_path
```

---

## 9. å‚è€ƒèµ„æ–™

- KT-kernel ä»“åº“ï¼š`/home/lpl/ktransformers/kt-kernel`
- KT ç®—å­æ¥å£æ–‡æ¡£ï¼š`/home/lpl/ktransformers/kt-kernel/docs/sft_moe_amx/ç®—å­æ¥å£æ–‡æ¡£.md`
- KT æµ‹è¯•æ–‡ä»¶ï¼š
  - `/home/lpl/ktransformers/kt-kernel/examples/test_moe_sft_amx.py`ï¼ˆTP æ¨¡å¼ï¼‰
  - `/home/lpl/ktransformers/kt-kernel/examples/test_moe_sft_amx_no_tp.py`ï¼ˆé TP æ¨¡å¼ï¼‰
- Bug è°ƒè¯•è¯¦ç»†è®°å½•ï¼š`/home/lpl/.claude/plans/wiggly-wishing-wadler.md`
