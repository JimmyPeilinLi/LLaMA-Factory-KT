# KTransformers MoE 集成功能使用测试文档

## 1. 环境准备

### 1.1 硬件要求

- **CPU**: 支持 Intel AMX 指令集（Sapphire Rapids 或更新）
- **GPU**: CUDA 兼容 GPU（用于 Attention 层计算）
- **内存**: 足够容纳 MoE 权重（建议 >= 128GB）

### 1.2 软件依赖

```bash
# Python 环境
python >= 3.10

# 核心依赖
torch >= 2.0
transformers >= 4.40
peft >= 0.8.0

# KT Kernel
pip install kt-kernel  # 或从源码安装
```

### 1.3 验证 AMX 支持

```python
# 检查 AMX 硬件支持
def check_amx_support():
    try:
        import subprocess
        result = subprocess.run(
            ["lscpu"],
            capture_output=True,
            text=True,
        )
        return "amx" in result.stdout.lower()
    except Exception:
        return False

if check_amx_support():
    print("AMX is supported")
else:
    print("AMX is NOT supported")
```

```python
# 检查 kt_kernel 安装
try:
    from kt_kernel import kt_kernel_ext
    print("kt_kernel is available")
except ImportError:
    print("kt_kernel is NOT installed")
```

---

## 2. 快速开始

### 2.1 最小配置示例

创建配置文件 `train_kt_amx_minimal.yaml`:

```yaml
# 模型配置
model_name_or_path: deepseek-ai/DeepSeek-V2-Lite-Chat

# 训练阶段
stage: sft
finetuning_type: lora

# KTransformers 配置（核心！）
use_kt: true

# LoRA 配置
lora_rank: 16
lora_alpha: 32
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

# 数据配置
dataset: alpaca_en
template: deepseek2

# 训练配置
per_device_train_batch_size: 1
gradient_accumulation_steps: 4
num_train_epochs: 1
output_dir: outputs/kt_amx_test
```

运行训练：

```bash
llamafactory-cli train train_kt_amx_minimal.yaml
```

### 2.2 完整配置示例

```yaml
# train_kt_amx_full.yaml

# 模型配置
model_name_or_path: deepseek-ai/DeepSeek-V3
trust_remote_code: true
torch_dtype: bfloat16

# 训练阶段
stage: sft
finetuning_type: lora

# KTransformers 配置
use_kt: true
kt_backend: AMXBF16  # 可选: AMXBF16, AMXInt8
kt_num_threads: 60   # CPU 线程数
kt_tp_enabled: true  # 启用 TP 跨 NUMA 节点

# LoRA 配置
lora_rank: 16
lora_alpha: 32
lora_dropout: 0.1
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj

# 数据配置
dataset: alpaca_en
template: deepseek3
cutoff_len: 2048
preprocessing_num_workers: 8

# 训练配置
per_device_train_batch_size: 1
gradient_accumulation_steps: 8
learning_rate: 1.0e-4
num_train_epochs: 3
lr_scheduler_type: cosine
warmup_ratio: 0.1
logging_steps: 10
save_steps: 500
save_total_limit: 3

# 输出配置
output_dir: outputs/deepseek_v3_kt_amx
overwrite_output_dir: true
```

---

## 3. 配置参数说明

### 3.1 KTransformers 核心参数

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `use_kt` | bool | False | **必需**。启用 KTransformers MOE 后端 |
| `kt_backend` | str | "AMXBF16" | AMX 后端类型。"AMXBF16" 使用 BF16 精度，"AMXInt8" 使用 INT8 量化 |
| `kt_num_threads` | int | 60 | CPU 线程数。建议设置为 CPU 核心数 |
| `kt_tp_enabled` | bool | True | 启用 Tensor Parallelism。多 NUMA 系统建议开启 |
| `kt_num_gpu_experts` | int | 0 | GPU 上运行的专家数量。0=全部在CPU，>0=混合GPU/CPU模式 (Phase 2) |
| `kt_weight_path` | str | None | 预处理 CPU 专家权重路径。用于加载 INT4/INT8 量化权重 (Phase 2) |

### 3.2 LoRA 参数

| 参数 | 类型 | 默认值 | 说明 |
|------|------|--------|------|
| `lora_rank` | int | 8 | LoRA 秩。影响可训练参数量 |
| `lora_alpha` | float | 16 | LoRA 缩放因子。实际缩放 = alpha / rank |
| `lora_target` | str | - | LoRA 目标模块。见下方详细说明 |

### 3.3 lora_target 配置说明

`lora_target` 支持以下配置方式：

| 配置值 | 说明 |
|--------|------|
| `all` | 包含所有 Linear 模块（包括 MoE 专家: gate_proj, up_proj, down_proj） |
| `all_attention` | **新增**：只包含 Attention 模块，排除 MoE 专家层 |
| `q_proj,k_proj,...` | 显式指定模块列表（逗号分隔） |

**使用示例：**

```yaml
# 训练所有层（包含 MoE 专家）
lora_target: all

# 只训练 Attention（排除 MoE 专家）
lora_target: all_attention

# 显式指定模块
lora_target: q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj
```

**注意：** 在 KTransformers 模式下，MoE 层（gate_proj, up_proj, down_proj）的 LoRA 由 AMX 算子处理，Attention 层的 LoRA 由 peft 处理。两者分开管理但统一训练。

---

## 4. 支持的模型

### 4.1 DeepSeek 系列

```yaml
# DeepSeek-V2
model_name_or_path: deepseek-ai/DeepSeek-V2-Lite-Chat
template: deepseek2

# DeepSeek-V3
model_name_or_path: deepseek-ai/DeepSeek-V3
template: deepseek3
```

### 4.2 Qwen MoE 系列

```yaml
# Qwen2-MoE
model_name_or_path: Qwen/Qwen2-57B-A14B-Instruct
template: qwen

# Qwen3-MoE (如果存在)
model_name_or_path: Qwen/Qwen3-MoE-xxx
template: qwen
```

### 4.3 Mixtral 系列

```yaml
model_name_or_path: mistralai/Mixtral-8x7B-Instruct-v0.1
template: mistral
```

---

## 5. 测试用例

### 5.1 单元测试：MOELayerWrapper

```python
# tests/test_kt_moe.py

import torch
import pytest
from llamafactory.model.model_utils.kt_moe import (
    MOELayerWrapper,
    MOEAMXFunction,
    init_cpu_infer,
)


@pytest.fixture
def moe_setup():
    """创建测试用的 MOE 设置"""
    # 这里需要模拟或使用真实的 kt_kernel
    pass


def test_moe_layer_wrapper_forward(moe_setup):
    """测试 MOELayerWrapper 前向传播"""
    wrapper = moe_setup["wrapper"]
    hidden_states = torch.randn(2, 128, 7168, dtype=torch.bfloat16, device="cuda")

    output = wrapper(hidden_states)

    assert output.shape == hidden_states.shape
    assert output.device == hidden_states.device


def test_moe_layer_wrapper_backward(moe_setup):
    """测试 MOELayerWrapper 反向传播"""
    wrapper = moe_setup["wrapper"]
    hidden_states = torch.randn(
        2, 128, 7168, dtype=torch.bfloat16, device="cuda", requires_grad=True
    )

    output = wrapper(hidden_states)
    loss = output.sum()
    loss.backward()

    # 检查 LoRA 梯度
    for name, param in wrapper.lora_params.items():
        assert param.grad is not None, f"{name} grad is None"
        assert not torch.isnan(param.grad).any(), f"{name} grad has NaN"


def test_lora_pointer_update(moe_setup):
    """测试 LoRA 权重指针更新"""
    wrapper = moe_setup["wrapper"]

    # 模拟 optimizer.step()
    for param in wrapper.lora_params.values():
        param.data.add_(0.1)

    # 更新指针
    wrapper.update_lora_pointers()

    # 验证更新成功（无异常）
    assert True
```

### 5.2 集成测试：完整训练循环

```python
# tests/test_kt_amx_training.py

import os
import tempfile
import pytest
from llamafactory.train.sft.workflow import run_sft
from llamafactory.hparams import get_train_args


@pytest.fixture
def training_args():
    """创建训练参数"""
    with tempfile.TemporaryDirectory() as tmpdir:
        args = {
            "model_name_or_path": "deepseek-ai/DeepSeek-V2-Lite-Chat",
            "stage": "sft",
            "finetuning_type": "lora",
            "use_kt": True,
            "kt_backend": "AMXBF16",
            "kt_num_threads": 8,
            "lora_rank": 8,
            "lora_target": "q_proj,k_proj,v_proj,o_proj,gate_proj,up_proj,down_proj",
            "dataset": "alpaca_en",
            "template": "deepseek2",
            "cutoff_len": 256,
            "per_device_train_batch_size": 1,
            "gradient_accumulation_steps": 1,
            "max_steps": 10,
            "output_dir": tmpdir,
        }
        yield args


def test_sft_training_loop(training_args):
    """测试完整 SFT 训练循环"""
    model_args, data_args, training_args, finetuning_args, generating_args = (
        get_train_args(training_args)
    )

    run_sft(
        model_args=model_args,
        data_args=data_args,
        training_args=training_args,
        finetuning_args=finetuning_args,
        generating_args=generating_args,
    )

    # 验证输出文件
    assert os.path.exists(os.path.join(training_args.output_dir, "trainer_state.json"))


def test_checkpoint_save_load(training_args):
    """测试 checkpoint 保存和加载"""
    training_args["save_steps"] = 5
    training_args["max_steps"] = 10

    # 第一次训练
    model_args, data_args, training_args, finetuning_args, generating_args = (
        get_train_args(training_args)
    )
    run_sft(...)

    # 从 checkpoint 恢复
    training_args["resume_from_checkpoint"] = True
    training_args["max_steps"] = 20
    run_sft(...)

    assert True
```

### 5.3 精度测试：与参考实现对比

```python
# tests/test_kt_amx_precision.py

import torch
import pytest


def reference_moe_forward(hidden_states, gate_proj, up_proj, down_proj,
                          gate_lora_a, gate_lora_b, up_lora_a, up_lora_b,
                          down_lora_a, down_lora_b, lora_scaling):
    """PyTorch 参考实现"""
    # Gate
    gate_out = hidden_states @ gate_proj.t()
    gate_out += (hidden_states @ gate_lora_a.t() @ gate_lora_b.t()) * lora_scaling

    # Up
    up_out = hidden_states @ up_proj.t()
    up_out += (hidden_states @ up_lora_a.t() @ up_lora_b.t()) * lora_scaling

    # Activation
    intermediate = torch.nn.functional.silu(gate_out) * up_out

    # Down
    output = intermediate @ down_proj.t()
    output += (intermediate @ down_lora_a.t() @ down_lora_b.t()) * lora_scaling

    return output


def test_forward_precision_bf16():
    """测试 BF16 模式前向传播精度"""
    # 设置随机权重
    hidden_size = 7168
    intermediate_size = 2048
    lora_rank = 16
    batch_size = 4

    hidden_states = torch.randn(batch_size, hidden_size, dtype=torch.bfloat16)
    gate_proj = torch.randn(intermediate_size, hidden_size, dtype=torch.bfloat16)
    # ... 其他权重

    # 参考实现
    ref_output = reference_moe_forward(...)

    # AMX 实现
    amx_output = moe_amx_forward(...)

    # 计算误差
    relative_error = (ref_output - amx_output).abs().mean() / ref_output.abs().mean()

    assert relative_error < 0.05, f"Forward error {relative_error} exceeds threshold 0.05"


def test_backward_precision_bf16():
    """测试 BF16 模式反向传播精度"""
    # 类似前向测试，但验证梯度
    # ...

    relative_error = compute_gradient_error(...)
    assert relative_error < 0.10, f"Backward error {relative_error} exceeds threshold 0.10"
```

### 5.4 性能测试

```python
# tests/test_kt_amx_performance.py

import time
import torch
import pytest


def test_forward_throughput():
    """测试前向传播吞吐量"""
    batch_size = 4
    seq_len = 2048
    hidden_size = 7168
    num_iterations = 100

    hidden_states = torch.randn(
        batch_size, seq_len, hidden_size, dtype=torch.bfloat16, device="cuda"
    )

    # 预热
    for _ in range(10):
        output = moe_wrapper(hidden_states)

    # 计时
    torch.cuda.synchronize()
    start = time.time()

    for _ in range(num_iterations):
        output = moe_wrapper(hidden_states)

    torch.cuda.synchronize()
    elapsed = time.time() - start

    tokens_per_sec = (batch_size * seq_len * num_iterations) / elapsed
    print(f"Forward throughput: {tokens_per_sec:.2f} tokens/sec")


def test_memory_usage():
    """测试内存使用"""
    import psutil

    # 记录初始内存
    process = psutil.Process()
    initial_memory = process.memory_info().rss / 1024 / 1024 / 1024  # GB

    # 加载模型
    model = load_kt_model(...)

    # 记录加载后内存
    after_load_memory = process.memory_info().rss / 1024 / 1024 / 1024

    # 运行训练步骤
    for _ in range(10):
        output = model(inputs)
        loss = output.loss
        loss.backward()

    # 记录训练后内存
    after_train_memory = process.memory_info().rss / 1024 / 1024 / 1024

    print(f"Initial memory: {initial_memory:.2f} GB")
    print(f"After load: {after_load_memory:.2f} GB")
    print(f"After train: {after_train_memory:.2f} GB")
```

---

## 6. 常见问题

### 6.1 kt_kernel 未找到

**错误信息：**
```
ImportError: kt_kernel not found
```

**解决方案：**
```bash
# 从源码安装
cd /path/to/ktransformers/kt-kernel
pip install -e .

# 或使用 pip
pip install kt-kernel
```

### 6.2 AMX 不支持

**错误信息：**
```
RuntimeError: Intel AMX not supported on this CPU
```

**解决方案：**
- 确认 CPU 型号支持 AMX（Sapphire Rapids 或更新）
- 检查 BIOS 是否启用 AMX

### 6.3 模型架构不支持

**错误信息：**
```
ValueError: Model architecture XXX not supported for KTransformers
```

**解决方案：**
- 检查模型是否在支持列表中
- 使用标准训练模式（不启用 use_kt）

### 6.4 内存不足

**错误信息：**
```
RuntimeError: CUDA out of memory
```

**解决方案：**
- 减小 `per_device_train_batch_size`
- 增加 `gradient_accumulation_steps`
- 启用 gradient checkpointing

### 6.5 TP 模式权重不同步

**症状：**
训练 loss 不下降或梯度异常

**解决方案：**
确保 `kt_tp_enabled=True` 时，Trainer 正确调用 `update_lora_pointers()`

---

## 7. 调试技巧

### 7.1 启用详细日志

```python
import logging
logging.getLogger("llamafactory").setLevel(logging.DEBUG)
```

### 7.2 验证 MoE 包装

```python
# 检查 MoE 层是否正确包装
model = load_model(...)

if hasattr(model, "_kt_wrappers"):
    print(f"Number of MoE layers: {len(model._kt_wrappers)}")
    for i, wrapper in enumerate(model._kt_wrappers):
        print(f"  Layer {i}: {type(wrapper).__name__}")
        print(f"    LoRA params: {list(wrapper.lora_params.keys())}")
else:
    print("No KTransformers wrappers found")
```

### 7.3 检查 LoRA 梯度

```python
# 训练一步后检查梯度
for wrapper in model._kt_wrappers:
    for name, param in wrapper.lora_params.items():
        if param.grad is not None:
            print(f"{name}: grad_norm = {param.grad.norm().item():.6f}")
        else:
            print(f"{name}: grad is None")
```

### 7.4 对比精度

```python
# 对比 AMX 和 PyTorch 参考实现的输出
ref_output = reference_implementation(...)
amx_output = amx_implementation(...)

diff = (ref_output - amx_output).abs()
print(f"Max diff: {diff.max().item():.6f}")
print(f"Mean diff: {diff.mean().item():.6f}")
print(f"Relative error: {(diff.mean() / ref_output.abs().mean()).item():.6f}")
```

---

## 8. 参考资料

- KT-kernel 仓库：`/home/lpl/ktransformers/kt-kernel`
- KT 算子接口文档：`/home/lpl/ktransformers/kt-kernel/docs/sft_moe_amx/算子接口文档.md`
- KT 测试文件：
  - `/home/lpl/ktransformers/kt-kernel/examples/test_moe_sft_amx.py`（TP 模式）
  - `/home/lpl/ktransformers/kt-kernel/examples/test_moe_sft_amx_no_tp.py`（非 TP 模式）
