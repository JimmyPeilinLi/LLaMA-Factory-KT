# KTransformers MoE 集成功能详细设计文档

## 1. 系统架构

### 1.1 整体架构

```
┌─────────────────────────────────────────────────────────────────────────┐
│                         LLaMA-Factory 训练流程                           │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ┌─────────────┐    ┌─────────────┐    ┌─────────────┐                 │
│  │  Tokenizer  │    │   Dataset   │    │  Training   │                 │
│  │   Module    │───>│   Module    │───>│   Args      │                 │
│  └─────────────┘    └─────────────┘    └──────┬──────┘                 │
│                                               │                         │
│  ┌────────────────────────────────────────────▼──────────────────────┐ │
│  │                        Model Loader                                │ │
│  │  ┌──────────────────────────────────────────────────────────────┐ │ │
│  │  │ if use_kt:                                                │ │ │
│  │  │   1. 加载 HuggingFace 模型 (Attention 在 GPU)                  │ │ │
│  │  │   2. 初始化 CPUInfer                                          │ │ │
│  │  │   3. 包装 MoE 层为 MOELayerWrapper                            │ │ │
│  │  │   4. 初始化 MoE LoRA 参数                                     │ │ │
│  │  └──────────────────────────────────────────────────────────────┘ │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                               │                         │
│  ┌────────────────────────────────────────────▼──────────────────────┐ │
│  │                      Adapter Initializer                           │ │
│  │  ┌──────────────────────────────────────────────────────────────┐ │ │
│  │  │ if use_kt:                                                │ │ │
│  │  │   - MoE LoRA 已由 kt_moe 处理                             │ │ │
│  │  │   - 只对 Attention 层应用 peft LoRA (q_proj, k_proj 等)       │ │ │
│  │  └──────────────────────────────────────────────────────────────┘ │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                               │                         │
│  ┌────────────────────────────────────────────▼──────────────────────┐ │
│  │                        KTTrainer                                │ │
│  │  ┌──────────────────────────────────────────────────────────────┐ │ │
│  │  │ training_step():                                              │ │ │
│  │  │   1. forward: Attention (GPU) + MoE (CPU AMX)                 │ │ │
│  │  │   2. loss 计算                                                │ │ │
│  │  │   3. backward: 自动传播到 AMX 算子                            │ │ │
│  │  │   4. optimizer.step()                                         │ │ │
│  │  │   5. update_lora_weights_task() (TP 模式)                     │ │ │
│  │  └──────────────────────────────────────────────────────────────┘ │ │
│  └───────────────────────────────────────────────────────────────────┘ │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
```

### 1.2 模块关系图

```
┌──────────────────────────────────────────────────────────────────────┐
│                            model_args.py                              │
│  - use_kt: bool                                                   │
│  - kt_backend: Literal["AMXBF16", "AMXInt8"]                      │
│  - kt_num_threads: int                                            │
│  - kt_tp_enabled: bool                                            │
└───────────────────────────────┬──────────────────────────────────────┘
                                │
                                ▼
┌──────────────────────────────────────────────────────────────────────┐
│                              loader.py                                │
│  - load_model()                                                       │
│    └──> kt_moe.load_kt_model()                               │
└───────────────────────────────┬──────────────────────────────────────┘
                                │
                                ▼
┌──────────────────────────────────────────────────────────────────────┐
│                           kt_moe.py                               │
│  - init_cpu_infer()                                                   │
│  - wrap_moe_layers_with_amx()                                         │
│  - initialize_lora_for_amx()                                          │
│  - MOELayerWrapper                                                    │
│  - MOEAMXFunction                                                     │
└───────────────────────────────┬──────────────────────────────────────┘
                                │
                                ▼
┌──────────────────────────────────────────────────────────────────────┐
│                             adapter.py                                │
│  - init_adapter()                                                     │
│    └──> 对 Attention 层应用 peft LoRA                                │
└───────────────────────────────┬──────────────────────────────────────┘
                                │
                                ▼
┌──────────────────────────────────────────────────────────────────────┐
│                         kt_trainer.py                             │
│  - KTTrainer                                                       │
│    └──> create_optimizer()                                            │
│    └──> training_step()                                               │
└──────────────────────────────────────────────────────────────────────┘
```

---

## 2. 核心模块设计

### 2.1 kt_moe.py 模块

#### 2.1.1 CPUInfer 初始化

```python
def init_cpu_infer(model_args: "ModelArguments") -> "CPUInfer":
    """初始化 CPUInfer 实例"""
    from kt_kernel import kt_kernel_ext

    if model_args.kt_tp_enabled:
        # TP 模式：自动跨 NUMA 节点分区
        cpu_infer = kt_kernel_ext.CPUInfer(model_args.kt_num_threads)
    else:
        # 单 NUMA 模式
        pool_config = kt_kernel_ext.WorkerPoolConfig()
        pool_config.subpool_count = 1
        pool_config.subpool_numa_map = [0]
        pool_config.subpool_thread_count = [model_args.kt_num_threads]
        cpu_infer = kt_kernel_ext.CPUInfer(pool_config)

    return cpu_infer
```

#### 2.1.2 MoE 层包装

```python
def wrap_moe_layers_with_amx(
    model: "PreTrainedModel",
    model_args: "ModelArguments",
    finetuning_args: "FinetuningArguments",
    cpu_infer: "CPUInfer",
) -> list["MOELayerWrapper"]:
    """将模型的 MoE 层替换为 AMX 包装"""

    wrappers = []
    moe_config = get_moe_config_for_model(model.config)

    for layer_idx, layer in enumerate(model.model.layers):
        moe_layer = get_moe_layer(layer, moe_config)
        if moe_layer is None:
            continue

        # 1. 提取 MoE 权重
        gate_proj, up_proj, down_proj = extract_moe_weights(moe_layer, moe_config)

        # 2. 移到 CPU 并确保连续
        gate_proj = gate_proj.cpu().contiguous()
        up_proj = up_proj.cpu().contiguous()
        down_proj = down_proj.cpu().contiguous()

        # 3. 初始化 LoRA 权重
        lora_params = create_lora_params(
            expert_num=moe_config.expert_num,
            hidden_size=model.config.hidden_size,
            intermediate_size=moe_config.intermediate_size,
            lora_rank=finetuning_args.lora_rank,
        )

        # 4. 创建 MOESFTConfig
        config = create_moe_sft_config(
            layer_idx=layer_idx,
            moe_config=moe_config,
            model_config=model.config,
            finetuning_args=finetuning_args,
            model_args=model_args,
            gate_proj=gate_proj,
            up_proj=up_proj,
            down_proj=down_proj,
            lora_params=lora_params,
            cpu_infer=cpu_infer,
        )

        # 5. 创建 AMX MOE 实例
        moe_amx = create_amx_moe(config, model_args.kt_backend)

        # 6. 加载权重
        cpu_infer.submit(moe_amx.load_weights_task())
        cpu_infer.sync()

        # 7. 创建包装器
        wrapper = MOELayerWrapper(
            original_moe=moe_layer,
            moe_amx=moe_amx,
            config=config,
            cpu_infer=cpu_infer,
            lora_params=lora_params,
            moe_config=moe_config,
        )

        # 8. 替换原始 MoE 层
        set_moe_layer(layer, wrapper, moe_config)
        wrappers.append(wrapper)

    return wrappers
```

#### 2.1.3 MOELayerWrapper 类

```python
class MOELayerWrapper(nn.Module):
    """包装 AMX MOE 算子，实现 PyTorch autograd 支持"""

    def __init__(
        self,
        original_moe: nn.Module,
        moe_amx: "AMXBF16_SFT_MOE",
        config: "MOESFTConfig",
        cpu_infer: "CPUInfer",
        lora_params: dict[str, nn.Parameter],
        moe_config: "MOEConfig",
    ):
        super().__init__()
        self.original_moe = original_moe
        self.moe_amx = moe_amx
        self.config = config
        self.cpu_infer = cpu_infer
        self.lora_params = lora_params
        self.moe_config = moe_config

        # 保留原始路由器
        self.router = getattr(original_moe, moe_config.router_name)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """前向传播：使用自定义 autograd 函数"""
        return MOEAMXFunction.apply(
            hidden_states,
            self.router,
            self.moe_amx,
            self.cpu_infer,
            self.lora_params,
            self.config,
            self.moe_config,
        )

    def update_lora_pointers(self):
        """更新 AMX 算子的 LoRA 权重指针（TP 模式需要）"""
        self.cpu_infer.submit(
            self.moe_amx.update_lora_weights_task(
                self.lora_params["gate_lora_a"].data.data_ptr(),
                self.lora_params["gate_lora_b"].data.data_ptr(),
                self.lora_params["up_lora_a"].data.data_ptr(),
                self.lora_params["up_lora_b"].data.data_ptr(),
                self.lora_params["down_lora_a"].data.data_ptr(),
                self.lora_params["down_lora_b"].data.data_ptr(),
            )
        )
        self.cpu_infer.sync()
```

#### 2.1.4 MOEAMXFunction 自定义 autograd

```python
class MOEAMXFunction(torch.autograd.Function):
    """自定义 autograd 函数，桥接 PyTorch 和 AMX"""

    @staticmethod
    def forward(
        ctx,
        hidden_states: torch.Tensor,
        router: nn.Module,
        moe_amx: "AMXBF16_SFT_MOE",
        cpu_infer: "CPUInfer",
        lora_params: dict[str, nn.Parameter],
        config: "MOESFTConfig",
        moe_config: "MOEConfig",
    ) -> torch.Tensor:
        # 1. 计算路由
        expert_ids, weights = compute_routing(hidden_states, router, moe_config)

        # 2. 准备输入
        bsz = hidden_states.shape[0]
        bsz_tensor = torch.tensor([bsz], device="cpu")
        input_data = hidden_states.to(dtype=torch.bfloat16).cpu().contiguous()
        output = torch.zeros(
            (bsz, config.hidden_size), dtype=torch.float32, device="cpu"
        ).contiguous()

        # 3. 调用 AMX forward
        cpu_infer.submit(
            moe_amx.forward_sft_task(
                bsz_tensor.data_ptr(),
                moe_config.num_experts_per_tok,
                expert_ids.data_ptr(),
                weights.data_ptr(),
                input_data.data_ptr(),
                output.data_ptr(),
                True,  # save_for_backward
            )
        )
        cpu_infer.sync()

        # 4. 保存反向传播所需信息
        ctx.moe_amx = moe_amx
        ctx.cpu_infer = cpu_infer
        ctx.lora_params = lora_params
        ctx.config = config
        ctx.bsz = bsz
        ctx.original_device = hidden_states.device
        ctx.original_dtype = hidden_states.dtype

        # 5. 返回输出
        return output.to(device=hidden_states.device, dtype=hidden_states.dtype)

    @staticmethod
    def backward(ctx, grad_output: torch.Tensor):
        # 1. 分配梯度缓冲区
        grad_input = torch.zeros(
            (ctx.bsz, ctx.config.hidden_size),
            dtype=torch.bfloat16,
            device="cpu",
        ).contiguous()

        grad_gate_lora_a = torch.zeros_like(ctx.lora_params["gate_lora_a"].data)
        grad_gate_lora_b = torch.zeros_like(ctx.lora_params["gate_lora_b"].data)
        grad_up_lora_a = torch.zeros_like(ctx.lora_params["up_lora_a"].data)
        grad_up_lora_b = torch.zeros_like(ctx.lora_params["up_lora_b"].data)
        grad_down_lora_a = torch.zeros_like(ctx.lora_params["down_lora_a"].data)
        grad_down_lora_b = torch.zeros_like(ctx.lora_params["down_lora_b"].data)

        # 2. 准备 grad_output
        grad_output_cpu = grad_output.to(dtype=torch.bfloat16).cpu().contiguous()

        # 3. 调用 AMX backward
        ctx.cpu_infer.submit(
            ctx.moe_amx.backward_task(
                grad_output_cpu.data_ptr(),
                grad_input.data_ptr(),
                grad_gate_lora_a.data_ptr(),
                grad_gate_lora_b.data_ptr(),
                grad_up_lora_a.data_ptr(),
                grad_up_lora_b.data_ptr(),
                grad_down_lora_a.data_ptr(),
                grad_down_lora_b.data_ptr(),
            )
        )
        ctx.cpu_infer.sync()

        # 4. 累积 LoRA 梯度
        accumulate_grad(ctx.lora_params["gate_lora_a"], grad_gate_lora_a)
        accumulate_grad(ctx.lora_params["gate_lora_b"], grad_gate_lora_b)
        accumulate_grad(ctx.lora_params["up_lora_a"], grad_up_lora_a)
        accumulate_grad(ctx.lora_params["up_lora_b"], grad_up_lora_b)
        accumulate_grad(ctx.lora_params["down_lora_a"], grad_down_lora_a)
        accumulate_grad(ctx.lora_params["down_lora_b"], grad_down_lora_b)

        # 5. 返回 grad_input
        return (
            grad_input.to(device=ctx.original_device, dtype=ctx.original_dtype),
            None, None, None, None, None, None,
        )
```

### 2.2 模型架构适配

```python
@dataclass
class MOEConfig:
    """MoE 架构配置"""
    moe_layer_name: str  # MoE 层属性名，如 "mlp"
    router_name: str  # 路由器属性名，如 "gate"
    experts_name: str  # 专家列表属性名，如 "experts"
    weight_names: tuple[str, str, str]  # gate/up/down 权重名
    expert_num: int
    intermediate_size: int
    num_experts_per_tok: int


def get_moe_config_for_model(config) -> MOEConfig:
    """根据模型配置获取 MoE 架构配置"""
    arch = config.architectures[0]

    if "DeepseekV2" in arch or "DeepseekV3" in arch:
        return MOEConfig(
            moe_layer_name="mlp",
            router_name="gate",
            experts_name="experts",
            weight_names=("gate_proj", "up_proj", "down_proj"),
            expert_num=config.n_routed_experts,
            intermediate_size=config.moe_intermediate_size,
            num_experts_per_tok=config.num_experts_per_tok,
        )
    elif "Qwen2Moe" in arch or "Qwen3Moe" in arch:
        return MOEConfig(
            moe_layer_name="mlp",
            router_name="gate",
            experts_name="experts",
            weight_names=("gate_proj", "up_proj", "down_proj"),
            expert_num=config.num_experts,
            intermediate_size=config.moe_intermediate_size,
            num_experts_per_tok=config.num_experts_per_tok,
        )
    elif "Mixtral" in arch:
        return MOEConfig(
            moe_layer_name="block_sparse_moe",
            router_name="gate",
            experts_name="experts",
            weight_names=("w1", "w3", "w2"),  # gate=w1, up=w3, down=w2
            expert_num=config.num_local_experts,
            intermediate_size=config.intermediate_size,
            num_experts_per_tok=config.num_experts_per_tok,
        )
    else:
        raise ValueError(f"Unsupported model architecture: {arch}")
```

### 2.3 KTTrainer

```python
class KTTrainer(CustomSeq2SeqTrainer):
    """支持 KT AMX MOE 的 SFT Trainer"""

    def __init__(
        self,
        model: "PreTrainedModel",
        model_args: "ModelArguments",
        finetuning_args: "FinetuningArguments",
        **kwargs,
    ):
        super().__init__(model=model, finetuning_args=finetuning_args, **kwargs)
        self.model_args = model_args
        self.finetuning_args = finetuning_args

        # 收集 MoE LoRA 参数
        self.moe_lora_params = []
        if hasattr(model, "_kt_wrappers"):
            for wrapper in model._kt_wrappers:
                self.moe_lora_params.extend(wrapper.lora_params.values())

    def create_optimizer(self):
        """创建优化器，包含 MoE LoRA 和 Attention LoRA 参数"""
        # 获取所有需要优化的参数
        params_to_optimize = []

        # 1. MoE LoRA 参数
        params_to_optimize.extend(self.moe_lora_params)

        # 2. Attention LoRA 参数（通过 peft 添加的）
        for name, param in self.model.named_parameters():
            if param.requires_grad and param not in self.moe_lora_params:
                params_to_optimize.append(param)

        # 创建优化器
        self.optimizer = torch.optim.AdamW(
            params_to_optimize,
            lr=self.args.learning_rate,
            weight_decay=self.args.weight_decay,
        )

        return self.optimizer

    def training_step(self, model, inputs) -> torch.Tensor:
        """训练步骤，包含 LoRA 权重同步"""
        loss = super().training_step(model, inputs)

        # TP 模式：同步 LoRA 权重
        if self.model_args.kt_tp_enabled and hasattr(model, "_kt_wrappers"):
            for wrapper in model._kt_wrappers:
                wrapper.update_lora_pointers()

        return loss
```

---

## 3. 数据流设计

### 3.1 前向传播数据流

```
Input: hidden_states [batch, seq_len, hidden_size] (GPU, fp16/bf16)
                │
                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                           Transformer Layer                              │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                     Attention (GPU)                              │   │
│  │   - q_proj, k_proj, v_proj, o_proj                              │   │
│  │   - LoRA: peft 标准实现                                          │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                               │                                         │
│                               ▼                                         │
│  ┌─────────────────────────────────────────────────────────────────┐   │
│  │                      MoE Layer (CPU AMX)                         │   │
│  │                                                                  │   │
│  │   1. hidden_states → CPU (bf16)                                 │   │
│  │   2. Router 计算 expert_ids, weights (GPU → CPU)                │   │
│  │   3. AMX forward_sft_task()                                     │   │
│  │      - gate_proj + LoRA                                         │   │
│  │      - up_proj + LoRA                                           │   │
│  │      - SiLU activation                                          │   │
│  │      - down_proj + LoRA                                         │   │
│  │   4. output → GPU                                               │   │
│  │                                                                  │   │
│  └─────────────────────────────────────────────────────────────────┘   │
│                               │                                         │
│                               ▼                                         │
│                        Layer Output                                     │
└─────────────────────────────────────────────────────────────────────────┘
                                │
                                ▼
                 Next Layer / Final Output
```

### 3.2 反向传播数据流

```
grad_output: [batch, seq_len, hidden_size] (GPU)
                │
                ▼
┌─────────────────────────────────────────────────────────────────────────┐
│                         MOEAMXFunction.backward                          │
├─────────────────────────────────────────────────────────────────────────┤
│                                                                         │
│   1. grad_output → CPU (bf16)                                          │
│                                                                         │
│   2. AMX backward_task()                                               │
│      ├─ backward_down: 计算 down LoRA 梯度                             │
│      ├─ backward_activation: 计算激活函数梯度                          │
│      └─ backward_gate_up: 计算 gate/up LoRA 梯度                       │
│                                                                         │
│   3. 输出:                                                             │
│      ├─ grad_input → GPU                                               │
│      └─ grad_lora_* → 累积到 Parameters                                │
│                                                                         │
└─────────────────────────────────────────────────────────────────────────┘
                │
                ▼
         grad_input → Attention backward
```

---

## 4. 配置参数设计

### 4.1 新增参数

| 参数名 | 类型 | 默认值 | 说明 |
|--------|------|--------|------|
| `use_kt` | bool | False | 是否使用 KT AMX MOE 后端 |
| `kt_backend` | str | "AMXBF16" | AMX 后端类型：AMXBF16 或 AMXInt8 |
| `kt_num_threads` | int | 60 | CPU 线程数 |
| `kt_tp_enabled` | bool | True | 是否启用 TP 跨 NUMA 节点 |
| `kt_num_gpu_experts` | int | 0 | GPU 上运行的专家数量。0=全部在CPU，>0=混合GPU/CPU模式 (Phase 2) |
| `kt_weight_path` | str | None | 预处理 CPU 专家权重路径。用于加载 INT4/INT8 量化权重 (Phase 2) |
| `kt_max_cache_depth` | int | 1 | 梯度检查点缓存深度 |

---

## 5. 错误处理

### 5.1 初始化阶段

```python
def validate_kt_amx_setup(model_args, model_config):
    """验证 KT AMX 设置"""
    # 检查 kt_kernel 是否可用
    try:
        from kt_kernel import kt_kernel_ext
    except ImportError:
        raise ImportError(
            "kt_kernel not found. Please install kt_kernel to enable KT MoE support."
        )

    # 检查模型架构是否支持
    arch = model_config.architectures[0]
    supported = ["DeepseekV2", "DeepseekV3", "Qwen2Moe", "Qwen3Moe", "Mixtral"]
    if not any(s in arch for s in supported):
        raise ValueError(
            f"Model architecture {arch} not supported for KT AMX. "
            f"Supported: {supported}"
        )

    # 检查 AMX 硬件支持
    if not check_amx_support():
        raise RuntimeError(
            "Intel AMX not supported on this CPU. "
            "Requires Sapphire Rapids or newer."
        )
```

### 5.2 运行时错误

```python
class KTAMXError(Exception):
    """KT AMX 相关错误"""
    pass


class KTAMXForwardError(KTAMXError):
    """前向传播错误"""
    pass


class KTAMXBackwardError(KTAMXError):
    """反向传播错误"""
    pass
```

---

## 6. 性能优化

### 6.1 零拷贝设计

- LoRA 权重通过 `tensor.data_ptr()` 传递，C++ 直接访问 Python tensor 内存
- 减少数据拷贝开销

### 6.2 异步执行

- 使用 `CPUInfer.submit()` 提交任务
- 使用 `CPUInfer.sync()` 等待完成
- 可以与 GPU 计算重叠

### 6.3 NUMA 优化

- TP 模式下自动跨 NUMA 节点分区
- 权重和计算分布到不同 NUMA 节点

---

## 7. 测试策略

### 7.1 单元测试

- MOELayerWrapper forward 正确性
- MOEAMXFunction backward 正确性
- 路由计算正确性

### 7.2 集成测试

- 完整 SFT 训练循环
- 多层 MoE 模型训练
- checkpoint 保存/加载

### 7.3 精度测试

- 与 PyTorch 参考实现对比
- BF16/INT8 精度验证

### 7.4 性能测试

- 训练吞吐量
- 内存使用
- GPU/CPU 负载均衡
