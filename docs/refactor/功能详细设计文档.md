# KTransformers MoE é›†æˆåŠŸèƒ½è¯¦ç»†è®¾è®¡æ–‡æ¡£

## 1. ç³»ç»Ÿæ¶æ„

### 1.1 æ•´ä½“æ¶æ„

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         LLaMA-Factory è®­ç»ƒæµç¨‹                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”    â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”                 â”‚
â”‚  â”‚  Tokenizer  â”‚    â”‚   Dataset   â”‚    â”‚  Training   â”‚                 â”‚
â”‚  â”‚   Module    â”‚â”€â”€â”€>â”‚   Module    â”‚â”€â”€â”€>â”‚   Args      â”‚                 â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜    â””â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”˜                 â”‚
â”‚                                               â”‚                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                        Model Loader                                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ if use_kt:                                                â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   1. åŠ è½½ HuggingFace æ¨¡å‹ (Attention åœ¨ GPU)                  â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   2. åˆå§‹åŒ– CPUInfer                                          â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   3. åŒ…è£… MoE å±‚ä¸º MOELayerWrapper                            â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   4. åˆå§‹åŒ– MoE LoRA å‚æ•°                                     â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                               â”‚                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                      Adapter Initializer                           â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ if use_kt:                                                â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   - MoE LoRA å·²ç”± kt_moe å¤„ç†                             â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   - åªå¯¹ Attention å±‚åº”ç”¨ peft LoRA (q_proj, k_proj ç­‰)       â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                               â”‚                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â–¼â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚
â”‚  â”‚                        KTrainer                                â”‚ â”‚
â”‚  â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â” â”‚ â”‚
â”‚  â”‚  â”‚ training_step():                                              â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   1. forward: Attention (GPU) + MoE (CPU AMX)                 â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   2. loss è®¡ç®—                                                â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   3. backward: è‡ªåŠ¨ä¼ æ’­åˆ° AMX ç®—å­                            â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   4. optimizer.step()                                         â”‚ â”‚ â”‚
â”‚  â”‚  â”‚   5. update_lora_weights_task() (TP æ¨¡å¼)                     â”‚ â”‚ â”‚
â”‚  â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚ â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜ â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 1.2 æ¨¡å—å…³ç³»å›¾

```
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                            model_args.py                              â”‚
â”‚  - use_kt: bool                                                   â”‚
â”‚  - kt_backend: Literal["AMXBF16", "AMXInt8"]                      â”‚
â”‚  - kt_num_threads: int                                            â”‚
â”‚  - kt_tp_enabled: bool                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                              loader.py                                â”‚
â”‚  - load_model()                                                       â”‚
â”‚    â””â”€â”€> kt_moe.load_kt_model()                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           kt_moe.py                               â”‚
â”‚  - init_cpu_infer()                                                   â”‚
â”‚  - wrap_moe_layers_with_amx()                                         â”‚
â”‚  - initialize_lora_for_amx()                                          â”‚
â”‚  - MOELayerWrapper                                                    â”‚
â”‚  - MOEAMXFunction                                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                             adapter.py                                â”‚
â”‚  - init_adapter()                                                     â”‚
â”‚    â””â”€â”€> å¯¹ Attention å±‚åº”ç”¨ peft LoRA                                â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¬â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         kt_trainer.py                             â”‚
â”‚  - KTrainer                                                       â”‚
â”‚    â””â”€â”€> create_optimizer()                                            â”‚
â”‚    â””â”€â”€> training_step()                                               â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

---

## 2. æ ¸å¿ƒæ¨¡å—è®¾è®¡

### 2.1 kt_moe.py æ¨¡å—

#### 2.1.1 CPUInfer åˆå§‹åŒ–

```python
def init_cpu_infer(model_args: "ModelArguments") -> "CPUInfer":
    """åˆå§‹åŒ– CPUInfer å®ä¾‹"""
    from kt_kernel import kt_kernel_ext

    if model_args.kt_tp_enabled:
        # TP æ¨¡å¼ï¼šè‡ªåŠ¨è·¨ NUMA èŠ‚ç‚¹åˆ†åŒº
        cpu_infer = kt_kernel_ext.CPUInfer(model_args.kt_num_threads)
    else:
        # å• NUMA æ¨¡å¼
        pool_config = kt_kernel_ext.WorkerPoolConfig()
        pool_config.subpool_count = 1
        pool_config.subpool_numa_map = [0]
        pool_config.subpool_thread_count = [model_args.kt_num_threads]
        cpu_infer = kt_kernel_ext.CPUInfer(pool_config)

    return cpu_infer
```

#### 2.1.2 MoE å±‚åŒ…è£…

```python
def wrap_moe_layers_with_amx(
    model: "PreTrainedModel",
    model_args: "ModelArguments",
    finetuning_args: "FinetuningArguments",
    cpu_infer: "CPUInfer",
) -> list["MOELayerWrapper"]:
    """å°†æ¨¡å‹çš„ MoE å±‚æ›¿æ¢ä¸º AMX åŒ…è£…"""

    wrappers = []
    moe_config = get_moe_config_for_model(model.config)

    for layer_idx, layer in enumerate(model.model.layers):
        moe_layer = get_moe_layer(layer, moe_config)
        if moe_layer is None:
            continue

        # 1. æå– MoE æƒé‡
        gate_proj, up_proj, down_proj = extract_moe_weights(moe_layer, moe_config)

        # 2. ç§»åˆ° CPU å¹¶ç¡®ä¿è¿ç»­
        gate_proj = gate_proj.cpu().contiguous()
        up_proj = up_proj.cpu().contiguous()
        down_proj = down_proj.cpu().contiguous()

        # 3. åˆå§‹åŒ– LoRA æƒé‡
        lora_params = create_lora_params(
            expert_num=moe_config.expert_num,
            hidden_size=model.config.hidden_size,
            intermediate_size=moe_config.intermediate_size,
            lora_rank=finetuning_args.lora_rank,
        )

        # 4. åˆ›å»º MOESFTConfig
        config = create_moe_sft_config(
            layer_idx=layer_idx,
            moe_config=moe_config,
            model_config=model.config,
            finetuning_args=finetuning_args,
            model_args=model_args,
            gate_proj=gate_proj,
            up_proj=up_proj,
            down_proj=down_proj,
            lora_params=lora_params,
            cpu_infer=cpu_infer,
        )

        # 5. åˆ›å»º AMX MOE å®ä¾‹
        moe_amx = create_amx_moe(config, model_args.kt_backend)

        # 6. åŠ è½½æƒé‡
        cpu_infer.submit(moe_amx.load_weights_task())
        cpu_infer.sync()

        # 7. åˆ›å»ºåŒ…è£…å™¨
        wrapper = MOELayerWrapper(
            original_moe=moe_layer,
            moe_amx=moe_amx,
            config=config,
            cpu_infer=cpu_infer,
            lora_params=lora_params,
            moe_config=moe_config,
        )

        # 8. æ›¿æ¢åŸå§‹ MoE å±‚
        set_moe_layer(layer, wrapper, moe_config)
        wrappers.append(wrapper)

    return wrappers
```

#### 2.1.3 MOELayerWrapper ç±»

```python
class MOELayerWrapper(nn.Module):
    """åŒ…è£… AMX MOE ç®—å­ï¼Œå®ç° PyTorch autograd æ”¯æŒ"""

    def __init__(
        self,
        original_moe: nn.Module,
        moe_amx: "AMXBF16_SFT_MOE",
        config: "MOESFTConfig",
        cpu_infer: "CPUInfer",
        lora_params: dict[str, nn.Parameter],
        moe_config: "MOEConfig",
    ):
        super().__init__()
        self.original_moe = original_moe
        self.moe_amx = moe_amx
        self.config = config
        self.cpu_infer = cpu_infer
        self.lora_params = lora_params
        self.moe_config = moe_config

        # ä¿ç•™åŸå§‹è·¯ç”±å™¨
        self.router = getattr(original_moe, moe_config.router_name)

    def forward(self, hidden_states: torch.Tensor) -> torch.Tensor:
        """å‰å‘ä¼ æ’­ï¼šä½¿ç”¨è‡ªå®šä¹‰ autograd å‡½æ•°"""
        return MOEAMXFunction.apply(
            hidden_states,
            self.router,
            self.moe_amx,
            self.cpu_infer,
            self.lora_params,
            self.config,
            self.moe_config,
        )

    def update_lora_pointers(self):
        """æ›´æ–° AMX ç®—å­çš„ LoRA æƒé‡æŒ‡é’ˆï¼ˆTP æ¨¡å¼éœ€è¦ï¼‰"""
        self.cpu_infer.submit(
            self.moe_amx.update_lora_weights_task(
                self.lora_params["gate_lora_a"].data.data_ptr(),
                self.lora_params["gate_lora_b"].data.data_ptr(),
                self.lora_params["up_lora_a"].data.data_ptr(),
                self.lora_params["up_lora_b"].data.data_ptr(),
                self.lora_params["down_lora_a"].data.data_ptr(),
                self.lora_params["down_lora_b"].data.data_ptr(),
            )
        )
        self.cpu_infer.sync()
```

#### 2.1.4 MOEAMXFunction è‡ªå®šä¹‰ autograd

```python
class MOEAMXFunction(torch.autograd.Function):
    """è‡ªå®šä¹‰ autograd å‡½æ•°ï¼Œæ¡¥æ¥ PyTorch å’Œ AMX"""

    @staticmethod
    def forward(
        ctx,
        hidden_states: torch.Tensor,
        router: nn.Module,
        moe_amx: "AMXBF16_SFT_MOE",
        cpu_infer: "CPUInfer",
        lora_params: dict[str, nn.Parameter],
        config: "MOESFTConfig",
        moe_config: "MOEConfig",
    ) -> torch.Tensor:
        # 1. è®¡ç®—è·¯ç”±
        expert_ids, weights = compute_routing(hidden_states, router, moe_config)

        # 2. å‡†å¤‡è¾“å…¥
        bsz = hidden_states.shape[0]
        bsz_tensor = torch.tensor([bsz], device="cpu")
        input_data = hidden_states.to(dtype=torch.bfloat16).cpu().contiguous()
        output = torch.zeros(
            (bsz, config.hidden_size), dtype=torch.float32, device="cpu"
        ).contiguous()

        # 3. è°ƒç”¨ AMX forward
        cpu_infer.submit(
            moe_amx.forward_sft_task(
                bsz_tensor.data_ptr(),
                moe_config.num_experts_per_tok,
                expert_ids.data_ptr(),
                weights.data_ptr(),
                input_data.data_ptr(),
                output.data_ptr(),
                True,  # save_for_backward
            )
        )
        cpu_infer.sync()

        # 4. ä¿å­˜åå‘ä¼ æ’­æ‰€éœ€ä¿¡æ¯
        ctx.moe_amx = moe_amx
        ctx.cpu_infer = cpu_infer
        ctx.lora_params = lora_params
        ctx.config = config
        ctx.bsz = bsz
        ctx.original_device = hidden_states.device
        ctx.original_dtype = hidden_states.dtype

        # 5. è¿”å›è¾“å‡º
        return output.to(device=hidden_states.device, dtype=hidden_states.dtype)

    @staticmethod
    def backward(ctx, grad_output: torch.Tensor):
        # 1. åˆ†é…æ¢¯åº¦ç¼“å†²åŒº
        grad_input = torch.zeros(
            (ctx.bsz, ctx.config.hidden_size),
            dtype=torch.bfloat16,
            device="cpu",
        ).contiguous()

        grad_gate_lora_a = torch.zeros_like(ctx.lora_params["gate_lora_a"].data)
        grad_gate_lora_b = torch.zeros_like(ctx.lora_params["gate_lora_b"].data)
        grad_up_lora_a = torch.zeros_like(ctx.lora_params["up_lora_a"].data)
        grad_up_lora_b = torch.zeros_like(ctx.lora_params["up_lora_b"].data)
        grad_down_lora_a = torch.zeros_like(ctx.lora_params["down_lora_a"].data)
        grad_down_lora_b = torch.zeros_like(ctx.lora_params["down_lora_b"].data)

        # 2. å‡†å¤‡ grad_output
        grad_output_cpu = grad_output.to(dtype=torch.bfloat16).cpu().contiguous()

        # 3. è°ƒç”¨ AMX backward
        ctx.cpu_infer.submit(
            ctx.moe_amx.backward_task(
                grad_output_cpu.data_ptr(),
                grad_input.data_ptr(),
                grad_gate_lora_a.data_ptr(),
                grad_gate_lora_b.data_ptr(),
                grad_up_lora_a.data_ptr(),
                grad_up_lora_b.data_ptr(),
                grad_down_lora_a.data_ptr(),
                grad_down_lora_b.data_ptr(),
            )
        )
        ctx.cpu_infer.sync()

        # 4. ç´¯ç§¯ LoRA æ¢¯åº¦
        accumulate_grad(ctx.lora_params["gate_lora_a"], grad_gate_lora_a)
        accumulate_grad(ctx.lora_params["gate_lora_b"], grad_gate_lora_b)
        accumulate_grad(ctx.lora_params["up_lora_a"], grad_up_lora_a)
        accumulate_grad(ctx.lora_params["up_lora_b"], grad_up_lora_b)
        accumulate_grad(ctx.lora_params["down_lora_a"], grad_down_lora_a)
        accumulate_grad(ctx.lora_params["down_lora_b"], grad_down_lora_b)

        # 5. è¿”å› grad_input
        return (
            grad_input.to(device=ctx.original_device, dtype=ctx.original_dtype),
            None, None, None, None, None, None,
        )
```

### 2.2 æ¨¡å‹æ¶æ„é€‚é… ã€2026-01-05 æ›´æ–°ã€‘

```python
@dataclass
class MOEArchConfig:
    """MoE æ¶æ„é…ç½®"""
    moe_layer_attr: str  # MoE å±‚å±æ€§åï¼Œå¦‚ "mlp"
    router_attr: str  # è·¯ç”±å™¨å±æ€§åï¼Œå¦‚ "gate"
    experts_attr: str  # ä¸“å®¶åˆ—è¡¨å±æ€§åï¼Œå¦‚ "experts"
    weight_names: tuple[str, str, str]  # gate/up/down æƒé‡å
    expert_num: int
    intermediate_size: int
    num_experts_per_tok: int
    has_shared_experts: bool = False
    router_type: str = "linear"  # ã€æ–°å¢ã€‘Router ç±»å‹


def get_moe_arch_config(config) -> MOEArchConfig:
    """æ ¹æ®æ¨¡å‹é…ç½®è·å– MoE æ¶æ„é…ç½®"""
    arch = config.architectures[0]

    if "DeepseekV2" in arch or "DeepseekV3" in arch:
        return MOEArchConfig(
            moe_layer_attr="mlp",
            router_attr="gate",
            experts_attr="experts",
            weight_names=("gate_proj", "up_proj", "down_proj"),
            expert_num=config.n_routed_experts,
            intermediate_size=config.moe_intermediate_size,
            num_experts_per_tok=config.num_experts_per_tok,
            has_shared_experts=getattr(config, "n_shared_experts", 0) > 0,
            router_type="deepseek_gate",  # ã€æ–°å¢ã€‘DeepSeek router è¿”å› tuple
        )
    elif "Qwen2Moe" in arch or "Qwen3Moe" in arch:
        return MOEArchConfig(
            moe_layer_attr="mlp",
            router_attr="gate",
            experts_attr="experts",
            weight_names=("gate_proj", "up_proj", "down_proj"),
            expert_num=config.num_experts,
            intermediate_size=config.moe_intermediate_size,
            num_experts_per_tok=config.num_experts_per_tok,
            has_shared_experts=getattr(config, "shared_expert_intermediate_size", 0) > 0,
            router_type="linear",  # Qwen router æ˜¯ nn.Linear
        )
    elif "Mixtral" in arch:
        return MOEArchConfig(
            moe_layer_attr="block_sparse_moe",
            router_attr="gate",
            experts_attr="experts",
            weight_names=("w1", "w3", "w2"),  # gate=w1, up=w3, down=w2
            expert_num=config.num_local_experts,
            intermediate_size=config.intermediate_size,
            num_experts_per_tok=config.num_experts_per_tok,
            has_shared_experts=False,
            router_type="linear",  # Mixtral router æ˜¯ nn.Linear
        )
    else:
        raise ValueError(f"Unsupported model architecture: {arch}")
```

### 2.2.1 Router ç±»å‹è¯´æ˜ ã€2026-01-05 æ–°å¢ã€‘

ä¸åŒ MoE æ¨¡å‹çš„ Router æ¥å£ä¸åŒï¼š

| Router ç±»å‹ | æ¨¡å‹ | è¾“å…¥å½¢çŠ¶ | è¾“å‡º |
|------------|------|----------|------|
| `linear` | Qwen, Mixtral | 2D `[qlen, hidden]` | Raw logits `[qlen, num_experts]` |
| `deepseek_gate` | DeepSeek V2/V3 | 3D `[batch, seq, hidden]` | `(topk_idx, topk_weight, aux_loss)` |

åœ¨ `MOELayerWrapper.forward()` ä¸­ç»Ÿä¸€å¤„ç†ï¼š

```python
if self.router_type == "deepseek_gate":
    # DeepSeek: 3D input, returns processed tuple
    topk_ids, topk_weights, aux_loss = self.router(hidden_states)
else:
    # Qwen/Mixtral: 2D input, returns raw logits, need manual softmax+topk
    router_logits = self.router(hidden_states.view(-1, hidden_size))
    routing_weights = F.softmax(router_logits, dim=-1)
    topk_weights, topk_ids = torch.topk(routing_weights, k, dim=-1)
    topk_weights = topk_weights / topk_weights.sum(dim=-1, keepdim=True)
```

### 2.3 KTrainer

```python
class KTrainer(CustomSeq2SeqTrainer):
    """æ”¯æŒ KT AMX MOE çš„ SFT Trainer"""

    def __init__(
        self,
        model: "PreTrainedModel",
        model_args: "ModelArguments",
        finetuning_args: "FinetuningArguments",
        **kwargs,
    ):
        super().__init__(model=model, finetuning_args=finetuning_args, **kwargs)
        self.model_args = model_args
        self.finetuning_args = finetuning_args

        # æ”¶é›† MoE LoRA å‚æ•°
        self.moe_lora_params = []
        if hasattr(model, "_kt_wrappers"):
            for wrapper in model._kt_wrappers:
                self.moe_lora_params.extend(wrapper.lora_params.values())

    def create_optimizer(self):
        """åˆ›å»ºä¼˜åŒ–å™¨ï¼ŒåŒ…å« MoE LoRA å’Œ Attention LoRA å‚æ•°"""
        # è·å–æ‰€æœ‰éœ€è¦ä¼˜åŒ–çš„å‚æ•°
        params_to_optimize = []

        # 1. MoE LoRA å‚æ•°
        params_to_optimize.extend(self.moe_lora_params)

        # 2. Attention LoRA å‚æ•°ï¼ˆé€šè¿‡ peft æ·»åŠ çš„ï¼‰
        for name, param in self.model.named_parameters():
            if param.requires_grad and param not in self.moe_lora_params:
                params_to_optimize.append(param)

        # åˆ›å»ºä¼˜åŒ–å™¨
        self.optimizer = torch.optim.AdamW(
            params_to_optimize,
            lr=self.args.learning_rate,
            weight_decay=self.args.weight_decay,
        )

        return self.optimizer

    def training_step(self, model, inputs) -> torch.Tensor:
        """è®­ç»ƒæ­¥éª¤ï¼ŒåŒ…å« LoRA æƒé‡åŒæ­¥"""
        loss = super().training_step(model, inputs)

        # TP æ¨¡å¼ï¼šåŒæ­¥ LoRA æƒé‡
        if self.model_args.kt_tp_enabled and hasattr(model, "_kt_wrappers"):
            for wrapper in model._kt_wrappers:
                wrapper.update_lora_pointers()

        return loss
```

---

## 3. æ•°æ®æµè®¾è®¡

### 3.1 å‰å‘ä¼ æ’­æ•°æ®æµ

```
Input: hidden_states [batch, seq_len, hidden_size] (GPU, fp16/bf16)
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                           Transformer Layer                              â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                     Attention (GPU)                              â”‚   â”‚
â”‚  â”‚   - q_proj, k_proj, v_proj, o_proj                              â”‚   â”‚
â”‚  â”‚   - LoRA: peft æ ‡å‡†å®ç°                                          â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                               â”‚                                         â”‚
â”‚                               â–¼                                         â”‚
â”‚  â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”   â”‚
â”‚  â”‚                      MoE Layer (CPU AMX)                         â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â”‚   1. hidden_states â†’ CPU (bf16)                                 â”‚   â”‚
â”‚  â”‚   2. Router è®¡ç®— expert_ids, weights (GPU â†’ CPU)                â”‚   â”‚
â”‚  â”‚   3. AMX forward_sft_task()                                     â”‚   â”‚
â”‚  â”‚      - gate_proj + LoRA                                         â”‚   â”‚
â”‚  â”‚      - up_proj + LoRA                                           â”‚   â”‚
â”‚  â”‚      - SiLU activation                                          â”‚   â”‚
â”‚  â”‚      - down_proj + LoRA                                         â”‚   â”‚
â”‚  â”‚   4. output â†’ GPU                                               â”‚   â”‚
â”‚  â”‚                                                                  â”‚   â”‚
â”‚  â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜   â”‚
â”‚                               â”‚                                         â”‚
â”‚                               â–¼                                         â”‚
â”‚                        Layer Output                                     â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                                â”‚
                                â–¼
                 Next Layer / Final Output
```

### 3.2 åå‘ä¼ æ’­æ•°æ®æµ

```
grad_output: [batch, seq_len, hidden_size] (GPU)
                â”‚
                â–¼
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚                         MOEAMXFunction.backward                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚                                                                         â”‚
â”‚   1. grad_output â†’ CPU (bf16)                                          â”‚
â”‚                                                                         â”‚
â”‚   2. AMX backward_task()                                               â”‚
â”‚      â”œâ”€ backward_down: è®¡ç®— down LoRA æ¢¯åº¦                             â”‚
â”‚      â”œâ”€ backward_activation: è®¡ç®—æ¿€æ´»å‡½æ•°æ¢¯åº¦                          â”‚
â”‚      â””â”€ backward_gate_up: è®¡ç®— gate/up LoRA æ¢¯åº¦                       â”‚
â”‚                                                                         â”‚
â”‚   3. è¾“å‡º:                                                             â”‚
â”‚      â”œâ”€ grad_input â†’ GPU                                               â”‚
â”‚      â””â”€ grad_lora_* â†’ ç´¯ç§¯åˆ° Parameters                                â”‚
â”‚                                                                         â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
                â”‚
                â–¼
         grad_input â†’ Attention backward
```

---

## 4. é…ç½®å‚æ•°è®¾è®¡

### 4.1 æ–°å¢å‚æ•°

| å‚æ•°å | ç±»å‹ | é»˜è®¤å€¼ | è¯´æ˜ |
|--------|------|--------|------|
| `use_kt` | bool | False | æ˜¯å¦ä½¿ç”¨ KT AMX MOE åç«¯ |
| `kt_backend` | str | "AMXBF16" | AMX åç«¯ç±»å‹ï¼šAMXBF16 æˆ– AMXInt8 |
| `kt_num_threads` | int | 60 | CPU çº¿ç¨‹æ•° |
| `kt_tp_enabled` | bool | True | æ˜¯å¦å¯ç”¨ TP è·¨ NUMA èŠ‚ç‚¹ |
| `kt_num_gpu_experts` | int | 0 | GPU ä¸Šè¿è¡Œçš„ä¸“å®¶æ•°é‡ã€‚0=å…¨éƒ¨åœ¨CPUï¼Œ>0=æ··åˆGPU/CPUæ¨¡å¼ (Phase 2) |
| `kt_weight_path` | str | None | é¢„å¤„ç† CPU ä¸“å®¶æƒé‡è·¯å¾„ã€‚ç”¨äºåŠ è½½ INT4/INT8 é‡åŒ–æƒé‡ (Phase 2) |
| `kt_max_cache_depth` | int | 1 | æ¢¯åº¦æ£€æŸ¥ç‚¹ç¼“å­˜æ·±åº¦ |

---

## 5. é”™è¯¯å¤„ç†

### 5.1 åˆå§‹åŒ–é˜¶æ®µ

```python
def validate_kt_amx_setup(model_args, model_config):
    """éªŒè¯ KT AMX è®¾ç½®"""
    # æ£€æŸ¥ kt_kernel æ˜¯å¦å¯ç”¨
    try:
        from kt_kernel import kt_kernel_ext
    except ImportError:
        raise ImportError(
            "kt_kernel not found. Please install kt_kernel to enable KT MoE support."
        )

    # æ£€æŸ¥æ¨¡å‹æ¶æ„æ˜¯å¦æ”¯æŒ
    arch = model_config.architectures[0]
    supported = ["DeepseekV2", "DeepseekV3", "Qwen2Moe", "Qwen3Moe", "Mixtral"]
    if not any(s in arch for s in supported):
        raise ValueError(
            f"Model architecture {arch} not supported for KT AMX. "
            f"Supported: {supported}"
        )

    # æ£€æŸ¥ AMX ç¡¬ä»¶æ”¯æŒ
    if not check_amx_support():
        raise RuntimeError(
            "Intel AMX not supported on this CPU. "
            "Requires Sapphire Rapids or newer."
        )
```

### 5.2 è¿è¡Œæ—¶é”™è¯¯

```python
class KTAMXError(Exception):
    """KT AMX ç›¸å…³é”™è¯¯"""
    pass


class KTAMXForwardError(KTAMXError):
    """å‰å‘ä¼ æ’­é”™è¯¯"""
    pass


class KTAMXBackwardError(KTAMXError):
    """åå‘ä¼ æ’­é”™è¯¯"""
    pass
```

---

## 6. æ€§èƒ½ä¼˜åŒ–

### 6.1 é›¶æ‹·è´è®¾è®¡

- LoRA æƒé‡é€šè¿‡ `tensor.data_ptr()` ä¼ é€’ï¼ŒC++ ç›´æ¥è®¿é—® Python tensor å†…å­˜
- å‡å°‘æ•°æ®æ‹·è´å¼€é”€

### 6.2 å¼‚æ­¥æ‰§è¡Œ

- ä½¿ç”¨ `CPUInfer.submit()` æäº¤ä»»åŠ¡
- ä½¿ç”¨ `CPUInfer.sync()` ç­‰å¾…å®Œæˆ
- å¯ä»¥ä¸ GPU è®¡ç®—é‡å 

### 6.3 NUMA ä¼˜åŒ–

- TP æ¨¡å¼ä¸‹è‡ªåŠ¨è·¨ NUMA èŠ‚ç‚¹åˆ†åŒº
- æƒé‡å’Œè®¡ç®—åˆ†å¸ƒåˆ°ä¸åŒ NUMA èŠ‚ç‚¹

---

## 7. æµ‹è¯•ç­–ç•¥

### 7.1 å•å…ƒæµ‹è¯•

- MOELayerWrapper forward æ­£ç¡®æ€§
- MOEAMXFunction backward æ­£ç¡®æ€§
- è·¯ç”±è®¡ç®—æ­£ç¡®æ€§

### 7.2 é›†æˆæµ‹è¯•

- å®Œæ•´ SFT è®­ç»ƒå¾ªç¯
- å¤šå±‚ MoE æ¨¡å‹è®­ç»ƒ
- checkpoint ä¿å­˜/åŠ è½½

### 7.3 ç²¾åº¦æµ‹è¯•

- ä¸ PyTorch å‚è€ƒå®ç°å¯¹æ¯”
- BF16/INT8 ç²¾åº¦éªŒè¯

### 7.4 æ€§èƒ½æµ‹è¯•

- è®­ç»ƒååé‡
- å†…å­˜ä½¿ç”¨
- GPU/CPU è´Ÿè½½å‡è¡¡

---

## 8. æ¨¡å‹åŠ è½½ç­–ç•¥ ã€2026-01-05 æ–°å¢ã€‘

### 8.1 Device Map ç­–ç•¥

ç”±äº HuggingFace çš„ accelerate åº“åœ¨ä½¿ç”¨ `device_map={"...experts": "cpu"}` æ—¶ä¼šåˆ›å»º meta tensors ç”¨äºå†…å­˜ä¼˜åŒ–ï¼Œ
å¯¼è‡´ Trainer dispatch å¤±è´¥ï¼Œæˆ‘ä»¬é‡‡ç”¨ä»¥ä¸‹åŠ è½½ç­–ç•¥ï¼š

```
åŠ è½½æµç¨‹:
â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚ Step 1: device_map="cpu" åŠ è½½æ•´ä¸ªæ¨¡å‹åˆ° CPU                     â”‚
â”‚         - è®© transformers åˆ›å»ºçœŸå®çš„ CPU tensors               â”‚
â”‚         - æƒé‡ä» HuggingFace åŠ è½½åˆ° CPU (BF16)                 â”‚
â”‚                                                                â”‚
â”‚ Step 2: move_non_experts_to_gpu()                              â”‚
â”‚         - æ‰‹åŠ¨ç§»åŠ¨ Attention/Embedding/Router åˆ° GPU           â”‚
â”‚         - Experts ä¿ç•™åœ¨ CPU ç”¨äº AMX è®¡ç®—                     â”‚
â”‚                                                                â”‚
â”‚ Step 3: wrap_moe_layers_with_amx()                             â”‚
â”‚         - åˆ›å»º MOELayerWrapper æ›¿æ¢åŸ MoE æ¨¡å—                  â”‚
â”‚         - LoRA æƒé‡åœ¨ CPU ä¸Šåˆ›å»º                               â”‚
â”‚                                                                â”‚
â”‚ Step 4: _clear_original_expert_weights()                       â”‚
â”‚         - æ¸…ç† HuggingFace åŠ è½½çš„åŸå§‹ä¸“å®¶æƒé‡                  â”‚
â”‚         - Python GC å›æ”¶å†…å­˜                                   â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
```

### 8.2 kt_loader.py å…³é”®å‡½æ•°

```python
def get_kt_loading_kwargs(config, model_args) -> dict:
    """è·å– KT æ¨¡å‹åŠ è½½å‚æ•°"""
    return {
        "config": config,
        "torch_dtype": torch.bfloat16,
        "device_map": "cpu",  # å…¨éƒ¨åŠ è½½åˆ° CPU
        "trust_remote_code": model_args.trust_remote_code,
        "low_cpu_mem_usage": True,
    }

def move_non_experts_to_gpu(model, moe_config, device="cuda:0"):
    """ç§»åŠ¨éä¸“å®¶éƒ¨åˆ†åˆ° GPU"""
    # ç§»åŠ¨ embedding å’Œ final layers
    model.model.embed_tokens.to(device)
    model.model.norm.to(device)
    model.lm_head.to(device)

    # ç§»åŠ¨æ¯å±‚çš„éä¸“å®¶ç»„ä»¶
    for layer in model.model.layers:
        layer.self_attn.to(device)
        layer.input_layernorm.to(device)
        layer.post_attention_layernorm.to(device)

        # ç§»åŠ¨ router å’Œ shared_experts
        moe_module = getattr(layer, moe_config.moe_layer_attr, None)
        if moe_module is not None:
            router = getattr(moe_module, moe_config.router_attr, None)
            if router is not None:
                router.to(device)
            if hasattr(moe_module, "shared_experts"):
                moe_module.shared_experts.to(device)
        else:
            # Dense å±‚ç›´æ¥ç§»åŠ¨æ•´ä¸ª MLP
            layer.mlp.to(device)
```

---

## 9. Bug è°ƒè¯•è®°å½• ã€2026-01-05 æ–°å¢ã€‘

### 9.1 BUG-001: Meta Device æƒé‡åŠ è½½é”™è¯¯

**çŠ¶æ€**: âœ… **å·²è§£å†³**

**é—®é¢˜**: ä½¿ç”¨ `device_map={"...experts": "cpu"}` æ—¶ï¼Œaccelerate åˆ›å»º meta tensors å¯¼è‡´ Trainer dispatch å¤±è´¥

**è§£å†³æ–¹æ¡ˆ**: æ”¹ç”¨ `device_map="cpu"` åŠ è½½æ•´ä¸ªæ¨¡å‹ï¼Œç„¶åæ‰‹åŠ¨ç§»åŠ¨éä¸“å®¶éƒ¨åˆ†åˆ° GPU

### 9.2 BUG-004: INT8 æ¨¡å¼ Segfault

**çŠ¶æ€**: ğŸ”´ **æœªè§£å†³**

**é—®é¢˜**: ä½¿ç”¨ `kt_backend=AMXInt8` + `kt_weight_path` æ—¶ï¼Œåœ¨åˆ›å»º `AMXInt8_SFT_MOE` å®ä¾‹æ—¶å´©æºƒ

**å¾…è°ƒæŸ¥**: éœ€è¦æ£€æŸ¥ kt-kernel çš„ AMXInt8_SFT_MOE å®ç°

### 9.3 BUG-005: Router è¾“å…¥å½¢çŠ¶ä¸åŒ¹é…

**çŠ¶æ€**: ğŸŸ¡ **å¾…éªŒè¯**

**é—®é¢˜**: DeepSeek router æœŸæœ› 3D è¾“å…¥ä½†æ”¶åˆ° 2Dï¼›ä¸” router è¿”å› `(topk_idx, topk_weight, aux_loss)` è€Œé raw logits

**è§£å†³æ–¹æ¡ˆ**:
1. æ–°å¢ `router_type` å­—æ®µåŒºåˆ†ä¸åŒ router ç±»å‹
2. åœ¨ `MOELayerWrapper.forward()` ä¸­æ ¹æ® router_type åˆ†åˆ«å¤„ç†
3. ä¿®æ”¹ `MOEAMXFunction` æ¥å£æ¥æ”¶ `topk_ids, topk_weights`
